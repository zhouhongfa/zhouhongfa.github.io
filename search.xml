<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Phoenix-一些笔记]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-%E4%B8%80%E4%BA%9B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[sqlline.py使用连接1./sqlline.py localhost 常用命令显示所有表 !tables 查看表结构 !describe 表名 退出 !quit 查看帮助 !help 建表语句1create table userinfo_c(id varchar primary key,name varchar);]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala之旅二-统一类型]]></title>
    <url>%2F2019%2F08%2F31%2FScala%E4%B9%8B%E6%97%85%E4%BA%8C-%E7%BB%9F%E4%B8%80%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在Scala中，所有值都有一个类型，包括数值和函数。下图说明了类型层次结构的子集。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix-安装4.8]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-%E5%AE%89%E8%A3%854-8%2F</url>
    <content type="text"><![CDATA[说明在 Hbase1.2 上安装 phoenix4.8.0 安装手册 下载地址http://archive.apache.org/dist/phoenix/apache-phoenix-4.8.0-HBase-1.2/bin/ 安装解压下载的压缩包 将 phoenix-4.8.0-HBase-1.2-server.jar 拷贝到 hbase安装目录的 lib 目录下 重启 hbase 即可 客户端配置修改 PHOENIX_HOME 下的 conf/env.sh 测试进入 phoenix 目录下的 bin目录中 输入以下命令 1sqlline.py localhost 额，在 windows 下安装不了 1234D:\BIGDATA\phoenix\apache-phoenix-4.8.0-HBase-1.2-bin\binλ .\sqlline.py&apos;C:\Program&apos; 不是内部或外部命令，也不是可运行的程序或批处理文件。]]></content>
      <categories>
        <category>Phoenix</category>
      </categories>
      <tags>
        <tag>Phoenix</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix Spark Plugin 使用]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-phoenix-spark%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[版本说明如果要用 phoenix 的 spark 插件，注意有些版本只支持spark 1.x ，scala也要相应地修改~ 版本问题待解决，官方说2.x是没问题的~ 这里我用 的版本如下 组件 版本 Spark spark-1.6.2-bin-hadoop2.6 phoenix apache-phoenix-4.14.0-cdh5.14.2 ，连的远程phoenix scala 2.10.6 参考资料https://phoenix.apache.org/phoenix_spark.html 创建示例表123CREATE TABLE TABLE1 (ID BIGINT NOT NULL PRIMARY KEY, COL1 VARCHAR);UPSERT INTO TABLE1 (ID, COL1) VALUES (1, 'test_row_1');UPSERT INTO TABLE1 (ID, COL1) VALUES (2, 'test_row_2'); 创建应用用 IDEA 创建 Scala 项目 build.sbt 如下 1234567name := &quot;scala-spark-demo&quot;version := &quot;0.1&quot;scalaVersion := &quot;2.10.6&quot;resolvers += &quot;aliyun&quot; at &quot;http://maven.aliyun.com/nexus/content/groups/public/&quot; Project Structure 配置Libraries 配置 读取Phoenix表src/main/resources/log4j.properties 12345678910111213141516171819202122# Set everything to be logged to the consolelog4j.rootCategory=WARN, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger&apos;s log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=WARN# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=WARNlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFOlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERRORlog4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 如果开启了 phoenix.schema.isNamespaceMappingEnabled ，需要增加以下文件 src/main/resources/hbase-site.xml 1234567&lt;?xml version="1.0"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 使用Data Source API加载为DataFrame12345678910111213141516171819202122package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextobject PhoenixTest3 &#123; def main(args: Array[String]): Unit = &#123; val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) val df = sqlContext.load( "org.apache.phoenix.spark", Map("table" -&gt; "TABLE1", "zkUrl" -&gt; "cdh01:2181") ) df .filter(df("COL1") === "test_row_1" &amp;&amp; df("ID") === 1L) .select(df("ID")) .show &#125;&#125; 输出结果 12345+---+| ID|+---+| 1|+---+ 打包提交测试1打包时按照需求是否将 phoenix 依赖打进去，如果服务器有的话就不用了，打包方法 Ctrl+Alt+Shift+S 打开项目配置界面 配置需要打包的lib 注意要选择 Extract into output root 提交任务 1spark-submit --class com.yibo.examples.PhoenixTest3 --master yarn --deploy-mode client --driver-cores 1 --driver-memory 512M --num-executors 2 --executor-cores 2 --executor-memory 512M D:\zhf\Documents\projects\git\scala-spark-demo\out\artifacts\SparkOperateHBase__jar\scala-spark-demo.jar 使用Configuration对象直接加载为DataFrame这个好像不加载 hbase-site.xml 1234567891011121314151617181920212223242526package com.yibo.examplesimport org.apache.hadoop.conf.Configurationimport org.apache.phoenix.spark._import org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextobject PhoenixTest3 &#123; def main(args: Array[String]): Unit = &#123; val configuration = new Configuration() //配置zookeeper configuration.set("hbase.zookeeper.quorum", "cdh01"); //开启了phoenix.schema.isNamespaceMappingEnabled，则需要配置这个 configuration.set("phoenix.schema.isNamespaceMappingEnabled", "true"); // Can set Phoenix-specific settings, requires 'hbase.zookeeper.quorum' val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) // Load the columns 'ID' and 'COL1' from TABLE1 as a DataFrame val df = sqlContext.phoenixTableAsDataFrame( "TABLE1", Array("ID", "COL1"), conf = configuration ) df.show &#125;&#125; 输出结果 123456+---+----------+| ID| COL1|+---+----------+| 1|test_row_1|| 2|test_row_2|+---+----------+ 写入 Phoenix表创建以下 Phoenix 表进行测试 1CREATE TABLE OUTPUT_TEST_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER); 保存RDDsaveToPhoenix方法是RDD [Product]上的隐式方法，或者是元组的RDD。数据类型必须与Phoenix支持的某种Java类型相对应。 1234567891011121314151617181920212223package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.phoenix.spark._// RDD写入object PhoenixRDDWriteTest &#123; def main(args: Array[String]): Unit = &#123; val sc = new SparkContext("local", "phoenix-test") val dataSet = List((1L, "1", 1), (2L, "2", 2), (3L, "3", 3)) sc .parallelize(dataSet) .saveToPhoenix( "OUTPUT_TEST_TABLE", Seq("ID","COL1","COL2"), zkUrl = Some("cdh01:2181") ) &#125;&#125; 查询 phoenix 表 123456780: jdbc:phoenix:&gt; select * from output_test_table;+-----+-------+-------+| ID | COL1 | COL2 |+-----+-------+-------+| 1 | 1 | 1 || 2 | 2 | 2 || 3 | 3 | 3 |+-----+-------+-------+ 保存DataFramessave是DataFrame上的方法允许传入数据源类型。您可以使用org.apache.phoenix.spark，还必须传入一个表和zkUrl参数来指定要将DataFrame保留到哪个表和服务器。列名称是从DataFrame的架构字段名称派生的，并且必须与Phoenix列名称匹配。 save方法还采用SaveMode选项，仅支持SaveMode.Overwrite。 12CREATE TABLE INPUT_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER);CREATE TABLE OUTPUT_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER); 12upsert into input_table values(1,&apos;11&apos;,111);upsert into input_table values(2,&apos;22&apos;,222); 123456789101112131415161718192021222324252627package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.spark.sql._import org.apache.phoenix.spark._object PhoenixSaveTest &#123; def main(args: Array[String]): Unit = &#123; // Load INPUT_TABLE val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) val hbaseConnectionString = "cdh01:2181" val df = sqlContext.load("org.apache.phoenix.spark", Map("table" -&gt; "INPUT_TABLE", "zkUrl" -&gt; hbaseConnectionString)) // Save to OUTPUT_TABLE df.saveToPhoenix(Map("table" -&gt; "OUTPUT_TABLE", "zkUrl" -&gt; hbaseConnectionString)) //或者// df.write// .format("org.apache.phoenix.spark")// .mode("overwrite")// .option("table", "OUTPUT_TABLE")// .option("zkUrl", hbaseConnectionString)// .save() &#125;&#125; 查询 output_test 表 1234560: jdbc:phoenix:&gt; select * from output_table;+-----+-------+-------+| ID | COL1 | COL2 |+-----+-------+-------+| 1 | 11 | 111 |+-----+-------+-------+ 报错信息1Cannot initiate connection as SYSTEM:CATALOG is found but client does not have phoenix.schema.isNamespaceMappingEnabled enabled 在 src/main/resources 增加 hbase-site.xml 1234567&lt;?xml version="1.0"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/sql/DataFrame 2.x版本问题~(但是官网说2.x是可以用的啊，我试过 2.x下 dataset写是可以的 读会报 Dataset找不到 写会报 上面的错) 降成 1.x 12Error:(16, 25) value phoenixTableAsDataFrame is not a member of org.apache.spark.sql.SQLContext val df = sqlContext.phoenixTableAsDataFrame( 增加 import org.apache.phoenix.spark._即可~]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala之旅一-基础知识]]></title>
    <url>%2F2019%2F08%2F30%2FScala%E4%B9%8B%E6%97%85%E4%B8%80-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[官方文档 表达式表达式是可计算的语句。 11 + 1 可以使用 println 来输出表达式结果 1234println(1) // 1println(1 + 1) // 2println("Hello!") // Hello!println("Hello," + " world!") // Hello, world! 值您可以使用 val 关键字命名表达式的结果。 12val x = 1 + 1println(x) // 2 命名结果（例如x）称为值。引用值不会重新计算它。 无法重新分配值。 1x = 3 // 不能通过编译. 值的类型可以被推断出来，但您也可以显式声明类型，如下所示： 1val x: Int = 1 + 1 注意是如何声明类型的， Int 跟在标识符 x 之后，还需要一个 :。 变量变量就像值，除了你可以重新赋值。您可以使用 var 关键字定义变量。 123var x = 1 + 1x = 3 // 编译通过， 因为 "x" 是用 "var" 关键字声明的。println(x * x) // 9 与值一样，您可以根据需要明确说明类型： 1var x: Int = 1 + 1 块 Blocks 您可以通过用 {} 表达式来组合表达式。我们称之为 块。 块中最后一个表达式的结果也是整个块的结果。 1234println(&#123; val x = 1 + 1 x + 1&#125;) // 3 函数函数是带参数的表达式。 您可以定义一个返回给定整数加一的匿名函数（即无名称）： 1(x: Int) =&gt; x + 1 方法方法的外观和行为与函数非常相似，但它们之间存在一些关键差异。 方法使用 def 关键字定义。 def 之后是名称，参数列表，返回类型和正文。 12def add(x: Int, y: Int): Int = x + yprintln(add(1, 2)) // 3 注意返回值类型在参数列表后有 :Int 方法可以采用多个参数列表。 12def addThenMultiply(x: Int, y: Int)(multiplier: Int): Int = (x + y) * multiplierprintln(addThenMultiply(1, 2)(3)) // 9 或者根本没有参数列表。 12def name: String = System.getProperty("user.name")println("Hello, " + name + "!") 还有一些其他差异，但就目前而言，您可以将它们视为与函数类似的东西。 方法也可以有多行表达式。 1234def getSquareString(input: Double): String = &#123; val square = input * input square.toString&#125; 方法体中的最后一个表达式是方法的返回值。 （Scala确实有一个 return 关键字，但它很少使用。） 类您可以使用 class 关键字定义类，后跟其名称和构造函数参数。 1234class Greeter(prefix: String, suffix: String) &#123; def greet(name: String): Unit = println(prefix + name + suffix)&#125; 方法 greet 的返回类型是 Unit，它表示无返回值。它与 Java和 C 中的void 类似地使用。（不同之处在于，因为每个Scala表达式都必须具有某些值，所以实际上存在Unit类型的单例值，写作()。它不携带任何信息。） 您可以使用 new 关键字创建类的实例。 12val greeter = new Greeter("Hello, ", "!")greeter.greet("Scala developer") // Hello, Scala developer! 我们后面将深入介绍类。 Case 类Scala有一种特殊类型的类，称为“case”类。默认情况下，case类是不可变的，并按值进行比较。您可以使用 case class 关键字来定义。 1case class Point(x: Int, y: Int) 您可以在没有 new 关键字的情况下实例化 Case 类。 123val point = Point(1, 2)val anotherPoint = Point(1, 2)val yetAnotherPoint = Point(2, 2) 并且它们按值进行比较。 1234567891011if (point == anotherPoint) &#123; println(point + " and " + anotherPoint + " are the same.")&#125; else &#123; println(point + " and " + anotherPoint + " are different.")&#125; // Point(1,2) and Point(1,2) are the same.if (point == yetAnotherPoint) &#123; println(point + " and " + yetAnotherPoint + " are the same.")&#125; else &#123; println(point + " and " + yetAnotherPoint + " are different.")&#125; // Point(1,2) and Point(2,2) are different. 我们想介绍的 Case 类 还有很多，我们相信你会爱上它们！我们将在稍后深入介绍它们。 对象对象是它们自己定义的单个实例。你可以把它们想象成是它们自己类中的单例。 您可以使用 object 关键字定义对象。 1234567object IdFactory &#123; private var counter = 0 def create(): Int = &#123; counter += 1 counter &#125;&#125; 您可以通过引用其名称来访问该对象。 1234val newId: Int = IdFactory.create()println(newId) // 1val newerId: Int = IdFactory.create()println(newerId) // 2 我们将在以后深入介绍对象。 Traits 特征？ 特征是包含某些字段和方法的类型。可以组合多种特征。 您可以使用 trait 关键字定义特征。 123trait Greeter &#123; def greet(name: String): Unit&#125; 特征也可以有默认实现。 1234trait Greeter &#123; def greet(name: String): Unit = println("Hello, " + name + "!")&#125; 您可以使用 extends 关键字扩展 traits 并使用 override 关键字覆盖实现。 12345678910111213class DefaultGreeter extends Greeterclass CustomizableGreeter(prefix: String, postfix: String) extends Greeter &#123; override def greet(name: String): Unit = &#123; println(prefix + name + postfix) &#125;&#125;val greeter = new DefaultGreeter()greeter.greet("Scala developer") // Hello, Scala developer!val customGreeter = new CustomizableGreeter("How are you, ", "?")customGreeter.greet("Scala developer") // How are you, Scala developer? 在这里，DefaultGreeter 只扩展了一个特征，但它可以扩展多个特征。 我们稍后将深入介绍特征。 Main 方法Main方法是程序的入口点。 Java虚拟机需要将main方法命名为 main，并接收一个字符串数组参数。 使用对象，您可以按如下方式定义main方法： 1234object Main &#123; def main(args: Array[String]): Unit = println("Hello, Scala developer!")&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala-语法风格之命名规范]]></title>
    <url>%2F2019%2F08%2F30%2FScala-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%E4%B9%8B%E5%91%BD%E5%90%8D%E7%BA%A6%E5%AE%9A%2F</url>
    <content type="text"></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala-语法风格之缩进]]></title>
    <url>%2F2019%2F08%2F30%2FScala-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%E4%B9%8B%E7%BC%A9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[参考Scala官网 Scala style guide-Indentation 简述每个缩进级为 两个空格，而不是 Tab 制表符。 例如： 1234567// right!class Foo &#123; def twospaces = &#123; val x = 2 .. &#125;&#125; Scala语言鼓励使用大量嵌套作用域和逻辑块（函数值等）。 换行如果一个表达式过长（超过80个字符），这样看起来不易于理解，那么你可以换行并用两个空格缩进来表示 如下 123val result = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12 + 13 + 14 + 15 + 16 + 17 + 18 + 19 + 20 如果没有这种结尾方法，Scala会在一行的末尾推断出一个分号，它有时会包装，有时甚至不会发出警告而抛弃编译。 大量参数的方法当调用一个接受大量参数（在五个或更多范围内）的方法时，通常需要将方法调用包装到多行上。在这种情况下，将每个参数单独放在一行上，从当前缩进级别缩进两个空格： 12345foo( someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415) 这样，所有参数都会排成一行，如果稍后需要更改方法的名称，则无需重新对齐它们。 应该非常小心地避免超出行长度这种类型的调用。更具体地说，当每个参数必须缩进超过50个空格以实现对齐时，应该避免这种调用。在这种情况下，调用本身应该移动到下一行并缩进两个空格： 12345678910111213// right!val myLongFieldNameWithNoRealPoint = foo( someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415)// wrong!val myLongFieldNameWithNoRealPoint = foo(someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习-读写Hbase数据]]></title>
    <url>%2F2019%2F08%2F30%2FSpark%E5%AD%A6%E4%B9%A0-%E8%AF%BB%E5%86%99Hbase%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[说明主要记录 Spark中读写Hbase 的操作记录 参考资料Spark入门：读写HBase数据 准备工作创建Hbase表12D:\BIGDATA\hbase-1.2.0\binλ hbase shell 查看所有表 123456hbase(main):001:0&gt; listTABLE0 row(s) in 0.1630 seconds=&gt; [] 创建 student 表 1hbase&gt; create &apos;student&apos;,&apos;info&apos; 可通过 describe 命令查看“ student” 表的基本信息： 1hbase&gt; describe &apos;student&apos; 录入示例数据123456put &apos;student&apos;,&apos;1&apos;,&apos;info:name&apos;,&apos;Xueqian&apos;put &apos;student&apos;,&apos;1&apos;,&apos;info:gender&apos;,&apos;F&apos;put &apos;student&apos;,&apos;1&apos;,&apos;info:age&apos;,&apos;23&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:name&apos;,&apos;Weiliang&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:gender&apos;,&apos;M&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:age&apos;,&apos;24&apos; 查看录入的数据 1234//如果每次只查看一行，就用下面命令hbase&gt; get &apos;student&apos;,&apos;1&apos;//如果每次查看全部数据，就用下面命令hbase&gt; scan &apos;student&apos; 配置Spark拷贝 hbase 的 jar 包到 D:\BIGDATA\spark\spark-1.6.2-bin-hadoop2.6\lib\下 需要拷贝的jar文件包括：所有hbase开头的jar文件、guava-12.0.1.jar、htrace-core-3.1.0-incubating.jar和protobuf-java-2.5.0.jar windows 上不知咋配~（下面我将 这些 jar作为 Libraries 来引用） 参考 博客 里配才是正确的吧~ 编写读取Hbase的应用程序一Libiaries配置如下引用 hbase 的 jar 如下引用 spark 的jar（这里我全引了~） 记得要 Add to modules 代码123456789101112131415161718192021222324252627282930313233package com.yibo.examplesimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.mapreduce.TableInputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark._object HbaseReader &#123; //读取hbase数据 def main(args: Array[String]) &#123; //序列化 System.setProperty("spark.serializer", "org.apache.spark.serializer.KryoSerializer") val conf = HBaseConfiguration.create() val sc = new SparkContext(new SparkConf().setMaster("local").setAppName("SparkOperateHBase")) //设置查询的表名 conf.set(TableInputFormat.INPUT_TABLE, "student") val stuRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = stuRDD.count() println("Students RDD Count:" + count) stuRDD.cache() //遍历输出 stuRDD.foreach(&#123; case (_, result) =&gt; val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes, "name".getBytes)) val gender = Bytes.toString(result.getValue("info".getBytes, "gender".getBytes)) val age = Bytes.toString(result.getValue("info".getBytes, "age".getBytes)) println("Row key:" + key + " Name:" + name + " Gender:" + gender + " Age:" + age) &#125;) &#125;&#125; 运行程序直接启动 com.yibo.examples.HbaseReader 类 输出结果提交程序一jar包到spark打包 Build -&gt; Build Arxx -&gt; Build 1spark-submit --class com.yibo.examples.HbaseReader --master yarn --deploy-mode client --driver-cores 1 --driver-memory 512M --num-executors 2 --executor-cores 2 --executor-memory 512M D:\zhf\Documents\projects\git\scala-spark-demo\out\artifacts\SparkOperateHBase__jar\scala-spark-demo.jar 输出结果有以下 123Students RDD Count:2Row key:1 Name:Xueqian Gender:F Age:23Row key:2 Name:Weiliang Gender:M Age:24 不打印 Row key 问题，增加 以下语句 1System.setProperty("spark.serializer", "org.apache.spark.serializer.KryoSerializer") 编写写入Hbase的应用在程序中增加 SparkWriteHBase 类 代码如下 12345678910111213141516171819202122232425262728293031323334package com.yibo.examplesimport org.apache.hadoop.hbase.client.&#123;Put, Result&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.mapreduce.Jobimport org.apache.spark._object SparkWriteHBase &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName("SparkWriteHBase").setMaster("local") val sc = new SparkContext(sparkConf) val tablename = "student" sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, tablename) val job = new Job(sc.hadoopConfiguration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) val indataRDD = sc.makeRDD(Array("3,Rongcheng,M,26", "4,Guanhua,M,27")) //构建两行记录 val rdd = indataRDD.map(_.split(',')).map &#123; arr =&gt; &#123; val put = new Put(Bytes.toBytes(arr(0))) //行健的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(arr(1))) //info:name列的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("gender"), Bytes.toBytes(arr(2))) //info:gender列的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("age"), Bytes.toBytes(arr(3).toInt)) //info:age列的值 (new ImmutableBytesWritable, put) &#125; &#125; rdd.saveAsNewAPIHadoopDataset(job.getConfiguration()) &#125;&#125; 查询表数据 1hbase&gt; scan &apos;student&apos; 结果如下 1234567891011121314ROW COLUMN+CELL 1 column=info:age, timestamp=1479640712163, value=23 1 column=info:gender, timestamp=1479640704522, value=F 1 column=info:name, timestamp=1479640696132, value=Xueqian 2 column=info:age, timestamp=1479640752474, value=24 2 column=info:gender, timestamp=1479640745276, value=M 2 column=info:name, timestamp=1479640732763, value=Weiliang 3 column=info:age, timestamp=1479643273142, value=\x00\x00\x00\x1A 3 column=info:gender, timestamp=1479643273142, value=M 3 column=info:name, timestamp=1479643273142, value=Rongcheng 4 column=info:age, timestamp=1479643273142, value=\x00\x00\x00\x1B 4 column=info:gender, timestamp=1479643273142, value=M 4 column=info:name, timestamp=1479643273142, value=Guanhua 4 row(s) in 0.3240 seconds 一些问题提交任务 时报错1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration 原因为没有配置Spark 中引用 hbase 的jar，如上图是将 hbase的依赖打进 jar 中 运行任务时出错1java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge 缺少 metrics-core-2.2.0.jar 日志打印问题在 Libiaries 中将 sbt 的 logch 去掉，仅使用 spark 下的 slf4j ，然后将 SPARK_HOME/conf/log4j.properties复制到项目中的 src/main/resources 下 修改为 WARN ，就不会再打印一大堆日志了~ 12# Set everything to be logged to the consolelog4j.rootCategory=WARN, console]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装Hbase]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85Hbase%2F</url>
    <content type="text"><![CDATA[说明记录在 windows下安装 Hbase 参考Hbase与Hadoop版本对应 Hbase下载地址 在windows上安装hadoop2.6.4 安装 下载 Hbase ，注意版本对应，这里我下载 1.2.0 的 http://archive.apache.org/dist/hbase/1.2.0/hbase-1.2.0-bin.tar.gz 解压到目录下，如 D:\BIGDATA 独立模式配置这里windows仅支持 独立模式~ 修改在 D:\BIGDATA\hbase-1.2.0\conf\下 的 hbase-site.xml 文件 123456789101112131415161718192021&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hbase-1.2.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;D:/BIGDATA/hbase-1.2.0/zoo&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 增加统计支持 --&gt; &lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动Hbase进入 D:\BIGDATA\hbase-1.2.0\bin 目录下，执行 start-hbase.cmd，查看控制台输出，无异常则启动成功 检查进程执行 jps 命令，查看反馈中有个 HMaster 表示成功 123456789λ jps10772 Jps21428 NodeManager29288 ResourceManager6632 HMaster1105218828 DataNode22796 NameNode32236 NailgunRunner 访问web端管理界面http://localhost:16010/master-status 关闭hbase执行 stop-hbase.cmd]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>大数据</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala学习-使用IDE开发]]></title>
    <url>%2F2019%2F08%2F30%2FScala%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8IDE%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/lsshlsw/article/details/39668911 安装插件Ctrl+Alt+S 打开 settings 界面 点击 Plugins 切换到 Marketplace ，搜索 scala 安装下载，很慢~ 搜索 sbt 安装下载]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Scala编写Spark应用记]]></title>
    <url>%2F2019%2F08%2F30%2F%E7%94%A8Scala%E7%BC%96%E5%86%99Spark%E5%BA%94%E7%94%A8%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明学习 Scala 写 Spark 应用 IDEA配置需下载 插件 scala 新建项目新建一个 Scala sbt 项目 这里 Spark 版本为 1.6.0 ，所以用 2.10.x的 Scala 打包配置 点击 Ok 即可 打包]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-compose笔记]]></title>
    <url>%2F2019%2F08%2F30%2Fdocker-compose%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明记录 docker-compose 的一些笔记 一些网址官方介绍 compose安装 安装Linux 环境官方手册 下载1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 对二进制文件应用可执行权限1sudo chmod +x /usr/local/bin/docker-compose 如果在安装后 docker-compose 执行失败，可以创建链接到 /usr/bin 到其他路径 例如： 1sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 安装命令行自动完成bash 下面是 bash的 将完成脚本放在 /etc/bash_completion.d/中。 1sudo curl -L https://raw.githubusercontent.com/docker/compose/1.24.0/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose zsh 下面是 zsh 在 ~/.zshrc 下面增加 12plugins=(... docker docker-compose) 卸载如果是使用 crul 安装 1sudo rm /usr/local/bin/docker-compose 如果是使用 pip 安装 1pip uninstall docker-compose 简单说明Compose是一个用于定义和运行多容器Docker应用程序的工具。使用Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，您可以从配置中创建并启动所有服务。 使用Compose基本上是一个三步过程： 使用 Dockerfile 定义应用程序的环境，以便可以在任何地方进行复制。 在 docker-compose.yml 中定义构成应用程序的服务，以便它们可以在隔离环境中一起运行。 运行 docker-compose up 然后Compose会启动并运行整个应用程序。 docker-compose.yml 看起来像这样： 123456789101112131415version: &apos;3&apos;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redisvolumes: logvolume01: &#123;&#125; 详细配置说明]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>docker-compose</tag>
        <tag>curl</tag>
        <tag>zsh</tag>
        <tag>compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-nginx配置记录]]></title>
    <url>%2F2019%2F08%2F30%2FDocker-nginx%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[说明 Docker 的 nginx 镜像使用笔记 Docker Hub 网址https://hub.docker.com/_/nginx 简单的配置12FROM nginxCOPY static-html-directory /usr/share/nginx/html 自定义 nignx.conf如下配置即可 12FROM nginxCOPY nginx.conf /etc/nginx/nginx.conf]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装及配置Spark]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AESpark%2F</url>
    <content type="text"><![CDATA[说明记录在 windows 系统上 Spark 的环境配置 前置条件需安装 jdk1.8 需先在 windows 上搭建 hadoop ，见 在windows上安装hadoop2.6.4 下载地址 组件 下载链接 Scala https://www.scala-lang.org/download/all.html Spark https://archive.apache.org/dist/spark/ 安装Scala到 Scala下载地址 选择对应的版本下载，这里我下的 2.10.6 版本压缩包(下载界面往下拉可以找到) 版本对应的 scala版本可以在 https://spark.apache.org/docs/1.6.0/ 查到，将 1.6.0改成使用的版本即可 解压到某个目录 环境变量 path 增加 scala 的 bin 目录路径 如 D:\scala-2.10.6\bin 安装Spark 到 Spark下载地址 下载对应版本的压缩包，我下载 1.6.2 的 解压到某个目录 添加bin目录路径到 path 环境变量中 配置环境变量 变量名 SPARK_HOME D:\BIGDATA\spark\spark-1.6.2-bin-hadoop2.6 PATH 增加 %SPARK_HOME%\bin 验证在 cmd 中执行 spark-shell 有以下输出即为成功 12Spark context available as sc.SQL context available as sqlContext. 参考资料https://blog.csdn.net/u011513853/article/details/52865076 spark2.2scala 要下载 2.11.x spark下载地址 一些问题119/08/30 16:53:22 ERROR util.ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\zhf\AppData\Local\Temp\spark-35f55385-549a-47c7-adcc-3bc3daaebaf7 在 spark 的 log4j.properties 中配置 (不显示而已~) 在 %SPARK_HOME%\conf 下，如果没有，则复制模板，改名即可 12log4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装hadoop2.6.4]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85hadoop%2F</url>
    <content type="text"><![CDATA[版本 组件 版本 hadoop 2.6.4 jdk 1.8 下载地址hadoop winutils github JDK安装略 hadoop安装配置下载，解压到 D:\BIGDATA下 配置环境变量 变量名 值 HADOOP_HOME D:\BIGDATA\hadoop-2.6.4 HADOOP_CONF_DIR %HADOOP_HOME%/etc/hadoop PATH 增加 %HADOOP_HOME%\bin 配置 hadoop-env.cmd在 hadoop 目录下的 \etc\hadoop\hadoop-env.cmd 文件 修改为以下，注意路径中有空格要写成如下 Progra~1 set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_191 验证 1234C:\software\cmder_miniλ hadoopUsage: hadoop [--config confdir] COMMANDwhere COMMAND is one of: 配置 core-site.xml路径%HADOOP_HOME%/etc/hadoop/core-site.xml 内容如下 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 hdfs-site.xml 首先要创建 data 目录 在 HADOOP_HOME下创建data 目录，再创建 data/datanode data/namenode两个目录 在 hadoop 目录下的 /etc/hadoop/hdfs-site.xml 修改为以下内容 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hadoop-2.6.4/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hadoop-2.6.4/data/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 yarn-site.xml在 hadoop 目录下的 /etc/hadoop/yarn-site.xml下 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 替换bin目录 下载 winutils github 到本地，找到对应的版本 将 HADOOP_HOME 的 bin 目录删除 将 winutils-master 里对应版本的 bin 目录复制到 HADOOP_HOME下 格式化执行以下命令(cmder 不行，要用cmd执行) 1hdfs namenode –format 有以下输出 1219/08/29 12:49:17 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1502620146-192.168.168.1-156705415741719/08/29 12:49:17 INFO common.Storage: Storage directory D:\BIGDATA\hadoop-2.6.4\data\namenode has been successfully formatted. 在 HADOOP_HOME下的 data/namenode 会生成一个目录 hadoop启动与关闭启动 进入 HADOOP_HOME\sbin 目录 双击 start-all.cmd 即可(或在 cmd 中调用) 关闭 进入 HADOOP_HOME\sbin 目录 双击 stop-all.cmd 即可(或在 cmd 中调用) 一些网址 说明 网址 NameNode http://localhost:50070 ResourceManage http://localhost:8088]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>windows</tag>
        <tag>hadoop</tag>
        <tag>yarn</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次pdf提取文本]]></title>
    <url>%2F2019%2F08%2F29%2F%E8%AE%B0%E4%B8%80%E6%AC%A1pdf%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[说明需要从 十几个pdf文件中提取 xml，本来想用 pdfbox 框架来直接转的，可是测试后发现转换出来的是乱码~，于是换个思路…… 思路 首页将需要提取的文件截图（可以写个脚本来做？有空看看）使用 ocr 来转换，我用的是 腾讯AI上的，然后写个程序去调 ocr 接口，将获取的结果写入xml中，然后再用 正则替换 来修正其中的一些格式问题。 优图OCR服务注意在 腾讯AI开放平台注册，然后创建应用，需要接入 通用OCR 能力，才能调api，不然会报 no auth 错误 点击 接入能力 选择你的应用即可 批量调用接口并生成xml项目github 正则替换记录我用的 vscode 软件来替换的 正则 替换文本 说明 ^\(|〈|&lt;\s+ &lt; &lt; 去空格 ( 改为 &lt; (&lt;[a-zA-Z]+)\) $1&gt; &lt;houseNumber) 改成 &lt;houseNumber&gt; 〉|\)$ &gt; 修正 &gt; \b’|’\s?|‘|”|’ “ &quot;修正 (=)\s(&quot;)\s $1$2 Code = &quot; IN 改成 Code = &quot;IN \s?(=)\s? $1 去掉 = 两边的空格 xsi:\s|xsi\s: xsi: xsi:type 修正 执行两次~ &lt;/\s|&lt;\s/ &lt;/ 修正 &lt;/ 去掉多余的空格 \s/&gt;|/\s&gt; /&gt; 修正 /&gt; 去掉多余的空格 ^&lt;!\s-+ &lt;!-- 修正 &lt;!-- (&lt;!-)(?!-) $1- &lt;!- 改成 &lt;!-- (?&lt;!-)(-&gt;)$ -$1 -&gt; 改成 --&gt; (\d)\s(\.) $1$2 2.16 .8改成 2.16.8 (\d\.)\s $1 2.16. 1 改成 2.16.1 \s(&quot;) $1 code=&quot;30954-2 &quot; 改成 code=&quot;30954-2&quot; (=&quot;)\s $1 typeCode=&quot; RCT&quot; 改成 typeCode=&quot;RCT&quot; (&lt;[a-zA-z]+)\) $1&gt; &lt;addr) 改成 &lt;addr&gt; (?&lt;=&quot;)(?=[a-zA-Z]+=) 空格 &quot;2&quot;unit= 改成 &quot;2&quot; unit= (=&quot;)\s $1 root=&quot; 2 改成 root=&quot;2 (?&lt;=&lt;/)([a-zA-Z]+)\s $1 &lt;/entryRelat ionship&gt;改成 &lt;/entryRelationship&gt; 应多次执行 (?&lt;=&lt;/?)([a-zA-Z]+$) $1&gt; &lt;/observation 改成 &lt;/observation&gt; (&lt;[a-zA-Z]+) (?!xsi:type)(?!/&gt;)(?![a-zA-Z:]+=) $1 &lt;entryRelat ionship 改成 &lt;entryRelationship 需要多次调用以防 出现 &lt;entry Relat ionship这种情况 (&lt;[a-zA-Z]+)\.\s* $1 &lt;ass. igned 改成 &lt;assigned (\s[a-zA-Z]+)\s([a-zA-Z]+=) $1$2 extens ion=&quot;PO改成 extension=&quot;PO (\.)\n $1 将以 . 结尾的换行符去掉 \n^(=) $1 将以 =开头的前一个换行符去掉 &quot;$\n &quot;空格 将以&quot; 结尾的换行符替换来空格]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>ocr</tag>
        <tag>pdf转换</tag>
        <tag>regex</tag>
        <tag>cda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[element-ui表格行内编辑使用]]></title>
    <url>%2F2019%2F08%2F25%2Felement-ui%E8%A1%A8%E6%A0%BC%E8%A1%8C%E5%86%85%E7%BC%96%E8%BE%91%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[说明通过使用 vue-element-extends 来拓展 element-ui 中的表格功能 参考vue-element-extends csdn博文 配置1npm install xe-utils vue-element-extends 在 main.js 里增加以下 12345import Vue from 'vue'import VueElementExtends from 'vue-element-extends'import 'vue-element-extends/lib/index.css'Vue.use(VueElementExtends) 官方示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;template&gt; &lt;div&gt; &lt;el-button @click=&quot;$refs.editable.insert()&quot;&gt;新增&lt;/el-button&gt; &lt;el-button @click=&quot;$refs.editable.removeSelecteds()&quot;&gt;删除选中&lt;/el-button&gt; &lt;elx-editable ref=&quot;editable&quot; :data.sync=&quot;tableData&quot;&gt; &lt;elx-editable-column type=&quot;selection&quot; width=&quot;55&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column type=&quot;index&quot; width=&quot;55&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;name&quot; label=&quot;只读&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;sex&quot; label=&quot;下拉&quot; :edit-render=&quot;&#123;name: &apos;ElSelect&apos;, options: sexList&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;num&quot; label=&quot;数值&quot; :edit-render=&quot;&#123;name: &apos;ElInputNumber&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;date&quot; label=&quot;日期&quot; :edit-render=&quot;&#123;name: &apos;ElDatePicker&apos;, props: &#123;type: &apos;date&apos;, format: &apos;yyyy-MM-dd&apos;&#125;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;flag&quot; label=&quot;开关&quot; :edit-render=&quot;&#123;name: &apos;ElSwitch&apos;, type: &apos;visible&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;remark&quot; label=&quot;文本&quot; :edit-render=&quot;&#123;name: &apos;ElInput&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;/elx-editable&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data () &#123; return &#123; tableData: [&#123; date: 1551322088449, name: &apos;小徐&apos;, sex: &apos;1&apos;, num: &apos;26&apos;, flag: false, remark: &apos;备注&apos; &#125;], sexList: [ &#123; &apos;label&apos;: &apos;男&apos;, &apos;value&apos;: &apos;1&apos; &#125;, &#123; &apos;label&apos;: &apos;女&apos;, &apos;value&apos;: &apos;0&apos; &#125; ] &#125; &#125;&#125;&lt;/script&gt;]]></content>
      <categories>
        <category>element-ui</category>
      </categories>
      <tags>
        <tag>vue</tag>
        <tag>element-ui</tag>
        <tag>vue-element-extends</tag>
        <tag>elx-editable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用笔记]]></title>
    <url>%2F2019%2F08%2F24%2FHexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明记录一下搭建 hexo 博客中的一些笔记 Hexo 安装网上的教程很详细了，这里简单记一下 在 D:\Files 打开 git bash 12npm install -g hexo-clihexo init hexo-blog 12345zhf@DESKTOP-2EL8QVU MINGW64 /d/Files$ cd hexo-blog/zhf@DESKTOP-2EL8QVU MINGW64 /d/Files/hexo-blog$ npm install hexo s 启动 配置 git 12345678git initgit remote add origin https://github.com/zhouhongfa/hexo-blog.gitgit config user.email &apos;zhouhongfa1996@gmail.com&apos;git pull origin mastergit add .git commit -m &apos;init&apos;git push --set-upstream origin master next 主题配置先 fork next主题github仓库 ，(这样就能保存你的配置了，需要进入 next 目录来推，如果有更新，则用 pull request拉更新的内容，可参考 github上fork了别人的项目后，再同步更新别人的提交) 12cd hexo-bloggit clone https://github.com/zhouhongfa/hexo-theme-next.git themes/next 主题wiki github Pages部署到 github 创建一个 zhouhongfa.github.io 公开项目 _config.yml 配置如下 123deploy: type: git repo: https://github.com/zhouhongfa/zhouhongfa.github.io.git 同步到 github 上 12hexo cleanhexo deploy 头像配置主要配置 avatar 属性 123456# 配置头像路径url: /images/avatar.jpg# If true, the avatar would be dispalyed in circle.为true时会将头像显示为圆形的rounded: true# If true, the avatar would be rotated with the cursor.为true时，鼠标放在头像上会旋转~rotated: true 侧边栏配置社交栏邮箱正确配置如下 123social: GitHub: https://github.com/zhouhongfa || github E-Mail: mailto:zhouhongfa1996@gmail.com || envelope 侧边栏位置在 _config.yml 中配置 sidebar 属性 1position: right 默认展开所有目录在 _config.yml 中配置 toc 属性 1expand_all: true 搜索配置参考博客 记录一下 1npm install hexo-generator-searchdb --save 编辑博客配置文件：_config.yml 增加以下 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件：_config.yml 修改以下 123# Local searchlocal_search: enable: true 重新部署 点击爱心效果这里我用的 next 主题比较新，可能会不一样请参考 记录一下 在 themes/next/source/js 目录下创建 clicklove.js 文件 内容如下 1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 修改 themes\next\layout\_layout.swig 文件 在最后的&lt;/body&gt;下加上以下 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/clicklove.js&quot;&gt;&lt;/script&gt; 分类和标签参考博客 记得要在 主题配置文件中解开注释 大概在 158行 menu 属性下 如何设置read more?用以下语法即可 1&lt;!--more--&gt; 图片引用问题开启 _config.yml 下的 1post_asset_folder: true 然后将需要用到的图片放置到 文章同名目录下 引用语法如下 1&#123;% asset_img 1566638486203.png test%&#125; 然后首页和文章内都能正常访问啦 语言问题需要根据主题里的文件名进行配置 如 next 主题里是 zh-CN ，就在 _config.yml 配置为 zh-CN 是配置 hexo 的配置 大概在 11 行 我是如下配置的 123language: - zh-CN- en 评论功能这里用 Gitalk 来实现、 参考 额，注意配置里的是 repo name ，也就是配置仓库名字即可，而不是克隆的地址 看板娘设置参考 1git clone https://github.com/galnetwen/Live2D.git 复制 live2d 目录到 themes/next/source下 修改 themes/next/layout/_layout.swig 文件 在 head 标签增加以下 1&lt;link rel="stylesheet" href="/live2d/css/live2d.css" /&gt; 在 body 标签增加以下 12345&lt;div id="landlord"&gt; &lt;div class="message" style="opacity:0"&gt;&lt;/div&gt; &lt;canvas id="live2d" width="280" height="250" class="live2d"&gt;&lt;/canvas&gt; &lt;div class="hide-button"&gt;隐藏&lt;/div&gt;&lt;/div&gt; 增加以下脚本 12345678910&lt;!-- 看板娘 --&gt;&lt;script type="text/javascript"&gt; var message_Path = '/live2d/' var home_Path = 'https://zhouhongfa.github.io/' //此处修改为你的域名，必须带斜杠&lt;/script&gt;&lt;script type="text/javascript" src="/live2d/js/live2d.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="/live2d/js/message.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; loadlive2d("live2d", "/live2d/model/tia/model.json");&lt;/script&gt; 效果如下 引用其他文章举例 引用 Hello.md 1&#123;% post_link Hello %&#125;]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>next</tag>
        <tag>gitalk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的博客]]></title>
    <url>%2F2019%2F08%2F24%2Fintroduce%2F</url>
    <content type="text"><![CDATA[hexo 搭建 https://zhouhongfa.github.io]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
