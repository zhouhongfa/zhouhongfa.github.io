<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Manjaro-pacman命令说明]]></title>
    <url>%2F2019%2F10%2F11%2FManjaro-pacman%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[pacman - 软件包管理器，这里记录一些常用的命令 查询查看已安装软件的可选依赖1pacman -Si 查看可用的包组里的包1# pacman -Sg gnome 安装软件包安装指定的包安装或者升级单个软件包，或者一列软件包（包含依赖包），使用如下命令： 1# pacman -S package_name1 package_name2 ... 用正则表达式安装多个软件包 1# pacman -S $(pacman -Ssq package_regex) 安装多个含有相似名称的软件包，而并非整个包组或全部匹配的软件包； 1# pacman -S plasma-&#123;desktop,mediacenter,nm&#125; 安装包组有的包组包含大量的软件包，有时用户只需其中几个。除了逐一键入序号外，pacman 还支持选择或排除某个区间内的的软件包： 1Enter a selection (default=all): 1-10 15 这将选中序号 1 至 10 和 15 的软件包。而 1Enter a selection (default=all): ^5-8 ^2 将会选中除了序号 5 至 8 和 2 之外的所有软件包。 想要查看哪些包属于 gnome 组，运行： 1# pacman -Sg gnome 也可以访问 https://www.archlinux.org/groups/ 查看可用的包组。 注意: 如果列表中的包已经安装在系统中，它会被重新安装，即使它已经是最新的。可以用 --needed 选项覆盖这种行为。 删除软件包删除单个软件包，保留其全部已经安装的依赖关系 1pacman -R package_name 删除指定软件包，及其所有没有被其他已安装软件包使用的依赖关系： 1pacman -Rs package_name 要删除软件包和所有依赖这个软件包的程序: 警告: 此操作是递归的，请小心检查，可能会一次删除大量的软件包。 1# pacman -Rsc package_name 要删除软件包，但是不删除依赖这个软件包的其他程序： 1# pacman -Rdd package_name pacman 删除某些程序时会备份重要配置文件，在其后面加上*.pacsave扩展名。-n 选项可以避免备份这些文件： 1pacman -Rn package_name 注意: pacman 不会删除软件自己创建的文件(例如主目录中的 .dot 文件不会被删除。 升级软件包一个 pacman 命令就可以升级整个系统。花费的时间取决于系统有多老。这个命令会同步非本地(local)软件仓库并升级系统的软件包： 1# pacman -Syu 查询包数据库pacman 使用 -Q 参数查询本地软件包数据库。参见： 1$ pacman -Q --help 使用 -S 参数来查询远程同步的数据库。参见： 1$ pacman -S --help pacman 可以在包数据库中查询软件包，查询位置包含了软件包的名字和描述： 1$ pacman -Ss string1 string2 ... 有时，-s的内置正则会匹配很多不需要的结果，所以应当指定仅搜索包名，而非描述或其他子段，下面的命令就会返回很多不必要结果: 1$ pacman -Ss &apos;^vim-&apos; 要查询已安装的软件包： 1$ pacman -Qs string1 string2 ... 按文件名查找软件库： 1$ pacman -Fs string1 string2 ... 显示软件包的详尽的信息： 1$ pacman -Si package_name 查询本地安装包的详细信息： 1$ pacman -Qi package_name 使用两个 -i 将同时显示备份文件和修改状态： 1$ pacman -Qii package_name 要获取已安装软件包所包含文件的列表： 1$ pacman -Ql package_name 查询远程库中软件包包含的文件： 1$ pacman -Fl package_name 检查软件包安装的文件是否都存在： 1$ pacman -Qk package_name 两个参数k将会执行一次更彻底的检查。 查询数据库获取某个文件属于哪个软件包： 1$ pacman -Qo /path/to/file_name 查询文件属于远程数据库中的哪个软件包： 1$ pacman -Fo /path/to/file_name 要罗列所有不再作为依赖的软件包(孤立orphans)： 1$ pacman -Qdt 要罗列所有明确安装而且不被其它包依赖的软件包： 1$ pacman -Qet 要显示软件包的依赖树： 1$ pactree package_name 检查一个安装的软件包被那些包依赖，可以使用 pkgtoolsAUR中的whoneeds: 1$ whoneeds package_name 或者pactree中使用-r: 1$ pactree -r package_name 更多信息查看pacman tips。 清理软件包缓存pacman 将下载的软件包保存在 /var/cache/pacman/pkg/ 并且不会自动移除旧的和未安装版本的软件包，因此需要手动清理，以免该文件夹过于庞大。 使用内建选项即可清除未安装软件包的缓存： 1# pacman -Sc pacman-contrib 提供的 paccache 命令默认会删除近3个版本前的软件包 1# paccache -r Tip: 可以使用 pacman hooks 自动执行清理，这里是参考示例。 也可以自己设置保留最近几个版本： 1# paccache -rk 1 清理所有未安装包的缓存文件，再此运行paccache: 1# paccache -ruk0 更多功能参见paccache -h。 paccache，还可以使用 Arch User Repository 中的 pkgcachecleanAUR： 1# pkgcacheclean ，以及pacleanerAUR，这两个是未来的替代工具. 其他命令升级系统时安装其他软件包： 1# pacman -Syu package_name1 package_name2 ... 下载包而不安装它： 1# pacman -Sw package_name 安装一个本地包(不从源里下载）： 1# pacman -U /path/to/package/package_name-version.pkg.tar.xz 要将本地包保存至缓存，可执行： 1# pacman -U file://path/to/package/package_name-version.pkg.tar.xz 安装一个远程包（不在 pacman 配置的源里面）： 1# pacman -U http://www.example.com/repo/example.pkg.tar.xz 要禁用 -S, -U 和 -R 动作，可以使用 -p 选项. pacman 会列出需要安装和删除的软件，并在执行动作前要求需要的权限。 参考资源pacman (简体中文)]]></content>
      <categories>
        <category>Manjaro</category>
      </categories>
      <tags>
        <tag>Manjaro</tag>
        <tag>pacman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-创建服务的方法]]></title>
    <url>%2F2019%2F10%2F11%2FLinux-%E5%88%9B%E5%BB%BA%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在linux 系统中，将自定义命令创建成系统服务的方法 创建服务这里以 创建 ssr 服务为例 1sudo vim /etc/systemd/system/ssr.service 内容如下 12345678910111213[Unit]Description=ssr ServiceRequires=network.targetAfter=network.target[Service]Type=forkingUser=zhouhongfaExecStart=/usr/local/bin/ssr start[Install]WantedBy=multi-user.target 加载服务1sudo systemctl daemon-reload 服务启动与关闭12sudo systemctl start ssrsudo systemctl stop ssr 查询服务状态1sudo systemctl status ssr 配置开机自启1sudo systemctl enable ssr]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>systemctl</tag>
        <tag>service</tag>
        <tag>linux</tag>
        <tag>systemd</tag>
        <tag>linux服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Switchyomega插件的安装与使用]]></title>
    <url>%2F2019%2F10%2F11%2FSwitchyomega%E6%8F%92%E4%BB%B6%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[翻墙工具 Chrome &amp; Firefox 插件-Switchyomega 安装与使用，需配合 ssr 使用（直连模式） 下载分情况进行下载 如果能访问chrome应用，直接点击直接安装 添加即可 如果不能访问的话，到 github下载 ，chrome 下载 crx , Firefox 下载 xpi 的包 这里我提供 2.5.20 的压缩包(这个解压出来就可以用了) 链接：https://pan.baidu.com/s/1LIFERFzTDdpwRDtSPtnf8Q提取码：l8vk 安装这里需要注意，新版本的Chrome不支持拖动 crx进行安装了，需要如下操作 将下载的 SwitchyOmega_Chromium.crx 改名为 SwitchyOmega_Chromium.rar ，然后解压(用winrar)出来。 在 Chrome 地址栏输入 chrome://extensions 打开扩展程序，右上角切换到 开发者模式 选择 加载已解压的插件 ，选择解压出来的目录即可成功安装~ linux 下不能如上面windows这样操作 需要使用 windows 系统，下载后改成 .rar 结尾的文件，解压到一个新目录 a，然后将 a 目录 压缩成一个文件 a.rar ，将压缩文件传到 linux 上，这样 linux 解压就不会报错了。 配置proxy配置 auto proxy 配置 规则地址，要点立即更新 1https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 点击 应用选项 即可 开启插件新安装的是没启用的，需要手动开启 参考文档switchyomega官网 switchyomega github Chrome 解决 CRX HEADER INVALID 问题]]></content>
      <categories>
        <category>ssr</category>
      </categories>
      <tags>
        <tag>Switchyomega</tag>
        <tag>ssr</tag>
        <tag>Chrome</tag>
        <tag>Firefox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-添加用户并且给用户超级管理员权限]]></title>
    <url>%2F2019%2F10%2F10%2FLinux-%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%E5%B9%B6%E4%B8%94%E7%BB%99%E7%94%A8%E6%88%B7%E8%B6%85%E7%BA%A7%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[Linux-添加用户并且给用户超级管理员权限 添加用户1234567#adduser tommy//添加一个名为tommy的用户#passwd tommy //修改密码Changing password for user tommy.New UNIX password: //在这里输入新密码Retype new UNIX password: //再次输入新密码passwd: all authentication tokens updated successfully. 赋予root权限修改 /etc/sudoers 文件 提示只读的话用 sudo visudo 命令 参考资源Linux 添加用户 并且给用户超级管理员权限 设置su和sudo为不需要密码]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>useradd</tag>
        <tag>visudo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Manjaro-Manjaro使用笔记]]></title>
    <url>%2F2019%2F10%2F10%2FManjaro-Manjaro%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录 Manjaro 系统使用笔记，主要是安装完系统后的一些配置和软件安装 设置sudo不需要输入密码参考Archwiki-Sudo (简体中文) 1sudo visudo 在最后增加以下内容 1Defaults:zhouhongfa !authenticate 网络配置操作说明 在右下角网络图标左击然后单击连接名可重连，右击可进行编辑信息 更新系统设置中国源1sudo pacman-mirrors --country China 更新1pacman -Syyu 安装软件1[zhouhongfa@zhouhongfa-pc ~]$ sudo pacman -S vim git 卸载软件1sudo pacman -Rns chromium 开启 ssh服务安装 1sudo pacman -S openssh 如果自带 ssh 则不需要安装 验证 1sudo systemctl status sshd 开启服务 1sudo systemctl start sshd 开机自启 1sudo systemctl enable sshd 安装中文字体12sudo pacman -S adobe-source-han-sans-cn-fontssudo pacman -S wqy-microhei wqy-zenhei 时间设置在 Manjaro Settings Manager 中有 时间和日期设置，勾选自动设置时间即可更正时间。 安装oh-my-zsh1sudo pacman -S zsh 1sh -c &quot;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot; 手动切换用户的shell chsh -s /usr/bin/zsh 配置 powerline 主题1git clone git://github.com/jeremyFreeAgent/oh-my-zsh-powerline-theme 安装 12cd oh-my-zsh-powerline-theme ./install_in_omz.sh 下载字体 12345678# clonegit clone https://github.com/powerline/fonts.git --depth=1# installcd fonts./install.sh# clean-up a bitcd ..rm -rf fonts 然后将终端字体改为 “ Meslo LG M for powerLine“ 字体 ( L M S 为尺寸大小吧) 在终端模拟器 编辑-&gt;首选项-&gt;外观 更改 主题配置参考 vim powerline插件1sudo pacman -S powerline-vim 编辑 .vimrc 文件 1vim ~/.vimrc 1234let g:powerline_pycmd=&quot;py3&quot;set laststatus=2set t_Co=256syntax on 使用 sudo vim 时会报错问题，使用 su - 切换到 root 用户，然后同上编辑 .vimrc 文件即可 安装 chrome参考资料 打包，构建 123456git clone https://aur.archlinux.org/google-chrome.gitcd google-chrome/makepkg -s 安装 1sudo pacman -U google-chrome-77.0.3865.90-1-x86_64.pkg.tar.xz 输入法安装这里我装 fcitx ，配置为新世纪五笔输入法 12sudo pacman -S fcitx-im fcitx-configtool# 我装了全部,不知道有什么不同 fcitx配置12vim ~/.xinitrcvim ~/.xprofile # xfce用这个 注意 KDM、GDM、LightDM 等显示管理器的用户，向 ~/.xprofile添加以上命令。(可能是.Xprofile，根据 .Xauthority 的大小写来判断) startx 与 slim 的用户，向 ~/.xinitrc，在 exec 语句前添加以下命令 添加以下内容 123export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=@im=fcitx 注销生效 打开设置界面：$ fcitx-configtool Ctrl + 空格 启动fcitxCtrl + Shift切换输入法 rime配置1sudo pacman -S fcitx-rime 下载词库 链接: https://pan.baidu.com/s/1bpdarlt 密码: qiiv 解压 1unzip 新世纪五笔rime词库.zip 移动文件 12sudo mv xinshijiwubi.dict.yaml /usr/share/rime-datasudo mv xinshijiwubi.schema.yaml /usr/share/rime-data 添加方案 1vim /usr/share/rime-data/default.yaml schema_list 下面增加 1- schema: xinshijiwubi 保存文件 重新部署 然后 可在方案列表切换为 新世纪五笔 输入法切换Ctrl+Space 切换输入法 rime中 Ctrl + ~ 切换方案 (~这个键) ssr客户端安装 注，这个要搭配 switchyomegy 使用吧？好像是 直连模式？ 下载脚本 12345wget https://raw.githubusercontent.com/the0demiurge/CharlesScripts/master/charles/bin/ssrsudo mv ssr /usr/local/bin/cd /usr/local/binsudo chmod a+x ssr 安装 1ssr install 帮助信息 ssr help 配置 1vim ~/.local/share/shadowsocksr/config.json 示例 12345678910111213141516171819202122232425&#123; &quot;server&quot;: &quot;&quot;, &quot;server_ipv6&quot;: &quot;::&quot;, &quot;server_port&quot;:&quot;18403&quot;, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;: 1080, &quot;password&quot;: &quot;&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;, &quot;protocol&quot;: &quot;auth_sha1_v4&quot;, &quot;protocol_param&quot;: &quot;&quot;, &quot;obfs&quot;: &quot;tls1.2_ticket_auth&quot;, &quot;obfs_param&quot;: &quot;&quot;, &quot;speed_limit_per_con&quot;: 0, &quot;speed_limit_per_user&quot;: 0, &quot;additional_ports&quot; : &#123;&#125;, &quot;additional_ports_only&quot; : false, &quot;timeout&quot;: 120, &quot;udp_timeout&quot;: 60, &quot;dns_ipv6&quot;: false, &quot;connect_verbose_info&quot;: 0, &quot;redirect&quot;: &quot;&quot;, &quot;fast_open&quot;: false&#125; 安装一些软件 1sudo pacman -S jq 启动 1ssr start 关闭 1ssr stop switchyomega 插件安装与使用见 Switchyomega插件的安装与使用]]></content>
      <categories>
        <category>Manjaro</category>
      </categories>
      <tags>
        <tag>Manjaro</tag>
        <tag>pacman</tag>
        <tag>powerline</tag>
        <tag>zsh</tag>
        <tag>oh-my-zsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Manjaro-VmWare安装Manjaro笔记]]></title>
    <url>%2F2019%2F10%2F09%2FManjaro-VmWare%E5%AE%89%E8%A3%85Manjaro%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这里安装 18.1.0 Manjaro 说明这里在vmware 中安装 Manjaro 系统，尝试在 Manjaro下编程，熟练后在物理机上使用！ 下载直接 官网 下载 这里下的是 manjaro-xfce-18.1.0-stable-x86_64.iso 创建虚拟机 先镜像，下一步，下一步即可~ 配置时区 切换到 Boot xxxx xfce进入 图型化系统 界面 双击图标进行安装 这里选手动分区 卡退了一下~，重新打开即可 分区新建分区表 选中新建的分区，点击创建 这里分区如下 挂载点 文件系统 大小 标记 / ext4 51200M lvm /boot fat32 512M boot swap linuxswap 4096M swap /home ext4 剩余的 lvm 安装配置一些信息 下一步，立刻安装 网络配置使用 NAT 模式 在 vmware 查询出主机网关 在manjaro图形化中配置以下即可 操作说明 在网络图标左击然后单击连接名可重连，右击可进行编辑信息]]></content>
      <categories>
        <category>Manjaro</category>
      </categories>
      <tags>
        <tag>Manjaro</tag>
        <tag>VmWare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-jdk8 安装及配置]]></title>
    <url>%2F2019%2F10%2F09%2Flinux-jdk8-%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[记录在linux上安装和配置 jdk8 下载 1.8压缩包 注意用 Tab 来补全文件或目录名（版本不一样的话，不要直接复制以下命令） 1234sudo mkdir -p /usr/javasudo tar -zxf jdk-8u144-linux-x64.tar.gz -C /usr/javasudo ln -s /usr/java/jdk1.8.0_144 /usr/java/latestsudo vim /etc/profile shift+g 移动到最后，单击 o 加上以下内容 1234export JAVA_HOME=/usr/java/latestexport JRE_HOME=/usr/java/latest/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 单击 Esc输入 :wq 保存并退出 使环境变量生效 1source /etc/profile 一键安装脚本如下 123touch jdk-install.shchmod +x jdk-install.shvim ./jdk-install.sh 1234567891011121314151617181920212223242526272829303132333435363738394041#!/bin/sh# 用于一键安装配置JDK，放到jdk安装包同级目录即可# 使用source来运行该脚本即可# source jdk-install.sh# 获取jdk路径# 如果不是参数，则用默认的路径../JDK_TAR=$1if [ -n $JDK_TAR ];then# 找出当前目录下的jdk-*.tar.gz文件 JDK_TAR=`find . -name "jdk-*.tar.gz"`fiecho $JDK_TAR# 配置文件安装路径INS_PATH_JDK="/usr/java/latest"echo "开始进行jdk安装"# 当串的长度大于0时为真(串非空)if [ -n $JAVA_HOME ]; then # JDK安装 mkdir -p /usr/java tar -zxf $JDK_TAR -C /usr/java ln -s /usr/java/jdk* /usr/java/latest # 设置环境变量 export JAVA_HOME=/usr/java/latest export JRE_HOME=/usr/java/latest/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin # 写入到文件中 echo "export JAVA_HOME=/usr/java/latestexport JRE_HOME=/usr/java/latest/jreexport CLASSPATH=.:\$JAVA_HOME/lib/dt.jar:\$JAVA_HOME/lib/tools.jar:\$JRE_HOME/libexport PATH=\$PATH:\$JAVA_HOME/bin:\$JRE_HOME/bin" &gt;&gt; /etc/profile source /etc/profile echo "jdk安装完成"else echo "检测到已安装JDK，JAVA_HOME为 $JAVA_HOME ，跳过jdk安装"fiecho "jdk版本信息如下"echo `java -version`]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-表空间相关笔记]]></title>
    <url>%2F2019%2F10%2F09%2Foracle-%E8%A1%A8%E7%A9%BA%E9%97%B4%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[oracle 表空间相关的笔记 官方例子 参考https://docs.oracle.com/html/E25494_01/omf003.htm#sthref1838 Example 1以下示例将数据文件创建的缺省位置设置为 /u01/oradata，然后在该位置创建一个包含数据文件的表空间tbs_1。数据文件为100 MB，可自动扩展，最大大小不受限制。 12SQL&gt; ALTER SYSTEM SET DB_CREATE_FILE_DEST = '/u01/oradata';SQL&gt; CREATE TABLESPACE tbs_1; Example 2此示例在目录 / u01/oradata 中创建一个名为 tbs_2 的表空间，其中包含数据文件。数据文件初始大小为400 MB，并且由于指定了SIZE子句，因此数据文件不可自动扩展。 The following parameter setting is included in the initialization parameter file: DB_CREATE_FILE_DEST = &#39;/u01/oradata&#39; The following statement is issued at the SQL prompt: 1SQL&gt; CREATE TABLESPACE tbs_2 DATAFILE SIZE 400M; 表空间的一些笔记查询用户默认表空间1select username,default_tablespace,TEMPORARY_TABLESPACE from user_users; 查询默认表空间路径该路径是在创建表空间时未指定 datafile 时默认保存的路径 1show parameter DB_CREATE_FILE_DEST 修改默认表空间路径1ALTER SYSTEM SET DB_CREATE_FILE_DEST = '/u01/oradata'; 建表空间语句这里文件路径可用 ./GZ_FUYI.dbf 这里创建一个初始大小为100m，自动扩展50m，无限制扩展的表空间 gz_fuyi 1234create tablespace gz_fuyi datafile 'D:\oradata\orcl\yibo\gz_fuyi.dbf' size 100mautoextend on next 50m maxsize unlimitedextent management local autoallocatesegment space management auto; 查询当前数据库默认的表空间类型1select * from database_properties where property_name = 'DEFAULT_TBS_TYPE'; v$datafile查当前所有表空间1select name from v$datafile 查询当前所有表空间名及路径1select t1.name,t2.name from v$tablespace t1, v$datafile t2 where t1.ts# = t2.ts#; dba_data_files查询某个表空间的信息1select file_name,tablespace_name,online_status from dba_data_files where tablespace_name = 'FUYI_ZHUANKEPINGTAI'; 查询所有表空间容量情况123456789101112SELECT Total.name "Tablespace Name",Free_space,(total_space - Free_space) Used_space,total_spaceFROM (select tablespace_name, sum(bytes / 1024 / 1024) Free_Spacefrom sys.dba_free_spacegroup by tablespace_name) Free,(select b.name, sum(bytes / 1024 / 1024) TOTAL_SPACEfrom sys.v_$datafile a, sys.v_$tablespace Bwhere a.ts# = b.ts#group by b.name) TotalWHERE Free.Tablespace_name = Total.name ORDER BY Free.Tablespace_name; 修改数据库默认使用bigfile表空间1alter database set default bigfile tablespace; 建bigfile表空间可以将表空间默认类型改为bigfile，这样用上面的语句创建出来的就是bigfile表空间了 官方文档 1234create bigfile tablespace bigtbs_testdatafile 'D:\oradata\orcl\bigfile_test\bigtbs_test.dbf'size 10gautoextend on; 建临时表空间123456--temporary tablespacecreate temporary tablespace GZ_FUYI_TEMP tempfile 'D:\oradata\orcl\yibo\GZ_FUYI_TEMP.dbf' size 100mautoextend onnext 50m maxsize 20Gextent management local; 分配表空间给指定用户给用户user_test 分配表空间TS_TEST,配额无限制 1alter user user_test quota unlimited on TS_TEST; 表空间修改为自动扩展1alter tablespace BIGTBS_TEST autoextend on; 查询表空间信息的语句1234567select file_name, tablespace_name, blocks, user_bytes / 1024 / 1024 as userful_m, autoextensible from dba_data_files where tablespace_name = upper('bigtbs_test'); 示例 一些说明 USER_BYTES is ‘Size of the useful portion of file in bytes’;USER_BLOCKS is ‘Size of the useful portion of file in ORACLE blocks’; 修改用户默认表空间1alter user yibo default tablespace gz_yibo_bigtbs; SQL&gt; alter user yibo default tablespace gz_yibo_bigtbs;User altered 123alter user kcgl default tablespace kcgl temporary tablespace temp;--针对某个用户的:alter user user_name default tablespace tbs_name; 修改用户临时表空间1alter user yibo temporary tablespace YIBO_TEMP1; 修改数据库的默认临时表空间1alter database default temporary tablespace temp_tbs_name; 删除表空间及其数据文件123drop tablespace TBS_01including contentscascade constraints; 表空间数量超出上限解决1234567show parameter db_files;alter system set db_files=更大的值 scope=spfile;shutdown immediate;startup; 修改表空间最大值UNDO表空间查看当前undo表空间1show parameter undo_tablespace 创建语法 123456--使用create undo tablespaceCREATE [BIGFILE | SMALLFILE] UNDO TABLESPACE tbs_nameDATAFILE 'path/filename' SIZE integer [K | M] [REUSE][AUTOEXTEND] [OFF | ON] NEXT integer [K | M] MAXSIZE [UNLIMITED | integer [K | M] ][EXTENT MANAGEMENT LOCAL] [AUTOALLOCATE][RETENTION GUARANTEE | NOGUARANTEE] 实例 1create bigfile undo tablespace yibo_undo_bigtbs datafile 'D:\YIBODB\yibo_undo_bigtbs.dbf' size 100m autoextend on next 50m maxsize 50g; 切换undo表空间12ALTER SYSTEM SET UNDO_TABLESPACE = yibo_undo_bigtbs 不用重启实例 查询undo表空间 数据字典 解释 v$undostat 包含所有undo表空间的统计信息，用于对undo表空间进行监控和调整。通过该视图，可以估计当前undo表空间的大小，Oracle利用该视图完成对回退信息的自动管理，该视图数据是有最近4天内，每10分钟产生一条统计记录构成的。 v$rollstat 包含undo表空间中回退段的性能统计信息 v$transaction 包含事务所使用的回退段信息 dba_undo_extents 包含undo表空间中区的大小与状态信息 dba_hist_undostat 包含v$undostat的快照，主要是4天前的统计信息 查询undo表空间中回退信息的当前状态 1select tablespace_name,segment_name,extent_id,status from dba_undo_extents; undo表空间中区的状态一共有3种：EXPIRED、UNEXPIRED、ACTIVE。 –EXPIRED：表示该回退信息对应的事务已经提交，保存时间超过保留区； –UNEXPIRED：表示该回退信息对应的事务已经提交，保存时间没有超过保留区； –ACTIVE：表示回退信息对应的事务还没有提交，该区还在使用；]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>tablespace</tag>
        <tag>temporary tablespace</tag>
        <tag>undo tablespace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-用户相关笔记]]></title>
    <url>%2F2019%2F10%2F09%2Foracle-%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录与oracle 用户相关的笔记，用户信息查询、修改、解锁、过期等。 查询所有用户1select * from all_users 1select * from dba_users; 解锁用户1SQL&gt; alter user scott account unlock; 修改密码12SQL&gt; alter user system identified by manager;User altered. 查询当前用户1234SQL&gt; select user from dual;USER------------------------------SCOTT 查看用户过期时间注意用户名为大写 12345SQL&gt; select username, account_status, EXPIRY_DATE from dba_users where username=&apos;HIOP&apos;;USERNAME ACCOUNT_STATUS EXPIRY_DATE------------------------------ -------------------------------- -----------HIOP OPEN 2020/3/15 1 用户过期解决使用 sysdba 角色进行操作 1234567SQL&gt; connect sys/secret_password as sysdba;解锁用户SQL&gt; alter user sysman account unlock ;修改原密码，改成旧的也可以SQL&gt; alter user DBUSER identified by newpa$$word; 将用户密码设置为永不过期12345678910111213SQL&gt; alter profile DEFAULT limit PASSWORD_REUSE_TIME unlimited;Profile alteredSQL&gt; alter profile DEFAULT limit PASSWORD_LIFE_TIME unlimited;Profile alteredSQL&gt; select username, account_status, EXPIRY_DATE from dba_users where username=&apos;HIOP&apos;;USERNAME ACCOUNT_STATUS EXPIRY_DATE------------------------------ -------------------------------- -----------HIOP OPEN]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>oracle用户</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-spfile笔记]]></title>
    <url>%2F2019%2F10%2F09%2Foracle-spfile%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[该文件用于指定 oracle 实例启动参数，这里记录一下spfile的使用 导出当前配置到文件中123SQL&gt; create pfile=&apos;D:\init.ora&apos; from spfile;File created. 这样就将 当前的 spfile 导出到 D:\init.ora 文件中 查看 spfile 文件路径这个是数据库实例启动时默认使用的配置文件路径 123456789SQL&gt; show parameter pfileNAME TYPE------------------------------------ ---------------------------------VALUE------------------------------spfile stringD:\APP\ZHF\PRODUCT\11.2.0\DBHOME_1\DATABASE\SPFILEORCL.ORA 启动时指定 pfile12345678910SQL&gt; startup pfile=&apos;E:\init0220.ora&apos;ORACLE 例程已经启动。Total System Global Area 8017100800 bytesFixed Size 2181944 bytesVariable Size 2483029192 bytesDatabase Buffers 5502926848 bytesRedo Buffers 28962816 bytes数据库装载完毕。数据库已经打开。 这里指定了不好维护，建议去 spfile 文件路径 中备份原文件，然后再修改里面的信息，这样 startup 启动数据库实例时就用的新的配置]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>spfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-sqlplus相关笔记]]></title>
    <url>%2F2019%2F10%2F09%2Foracle-sqlplus%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[oracle 中的 sqlplus 使用笔记 SQLPLUS登录123456789C:\Users\zhf&gt;sqlplus /nologSQL*Plus: Release 11.2.0.1.0 Production on 星期六 12月 15 15:28:00 2018Copyright (c) 1982, 2010, Oracle. All rights reserved.SQL&gt; conn /as sysdba已连接。SQL&gt; 其他用户 1conn scott/tiger@orcl 查询当前使用的数据库12345678910111213SQL&gt; conn sys/yb123123@gzfy as sysdba已连接。SQL&gt; select name from v$database;NAME---------GZFYSQL&gt; select instance from v$thread;INSTANCE--------------------------------------------------------------------------------gzfy 切换数据库123SQL&gt; connect system@orcl输入口令:已连接。 使用dba角色登录 123SQL&gt; conn sys/yb123123@gzfy as sysdba已连接。SQL&gt; 连远程数据库第一种 直接用ip 12sqlplus /nologconn sys/yibo123@10.54.2.194/orcl as sysdba 第二种 用 tnsname 需先在 tnsname.ora文件中配置 12345678yibo130 = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.130)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl) ) ) 重启监听服务 12SQL&gt; conn yibo_test/yibo123@yibo130Connected. 显示时间12345SQL&gt; set time on22:22:19 SQL&gt; 22:29:50 SQL&gt; set time offSQL&gt; 记录执行时间12345622:29:27 SQL&gt; set timing on22:29:37 SQL&gt; select count(*) from tb_cis_patient_info;count(*)---------- 2324938Executed in 0.323 seconds 1SQL&gt; set timing off 数据库启动与关闭123456789101112131415SQL&gt; shutdown immediate数据库已经关闭。已经卸载数据库。ORACLE 例程已经关闭。SQL&gt; startupORACLE 例程已经启动。Total System Global Area 1.2827E+10 bytesFixed Size 2187728 bytesVariable Size 3456110128 bytesDatabase Buffers 9361686528 bytesRedo Buffers 7385088 bytes数据库装载完毕。数据库已经打开。]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>sqlplus</tag>
        <tag>tnsname.ora</tag>
        <tag>startup</tag>
        <tag>shutdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-解决ORA-12514 监听程序当前无法识别连接描述符中请求的服务]]></title>
    <url>%2F2019%2F10%2F09%2Foracle-%E8%A7%A3%E5%86%B3ORA-12514-%E7%9B%91%E5%90%AC%E7%A8%8B%E5%BA%8F%E5%BD%93%E5%89%8D%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E8%BF%9E%E6%8E%A5%E6%8F%8F%E8%BF%B0%E7%AC%A6%E4%B8%AD%E8%AF%B7%E6%B1%82%E7%9A%84%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[解决 oracle 中 tns listener报错问题记录 listener.ora 文件修改 listener.ora 文件 该文件在oracle安装目录下，示例C:\app\Administrator\product\11.2.0\dbhome_1\NETWORK\ADMIN\listener.ora 这里主要在 SID_LIST_LISTENER 增加 一个 SID_DESC 示例，增加 gzfy sid 服务 12345(SID_DESC = (GLOBAL_DBNAME = GZFY) (ORACLE_HOME = C:\app\Administrator\product\11.2.0\dbhome_1) (SID_NAME = GZFY)) 完整文件示例 12345678910111213141516171819202122232425262728293031# listener.ora Network Configuration File: C:\app\Administrator\product\11.2.0\dbhome_1\network\admin\listener.ora# Generated by Oracle configuration tools.SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = CLRExtProc) (ORACLE_HOME = C:\app\Administrator\product\11.2.0\dbhome_1) (PROGRAM = extproc) (ENVS = &quot;EXTPROC_DLLS=ONLY:C:\app\Administrator\product\11.2.0\dbhome_1\bin\oraclr11.dll&quot;) ) (SID_DESC = (GLOBAL_DBNAME = ORCL) (ORACLE_HOME = C:\app\Administrator\product\11.2.0\dbhome_1) (SID_NAME = ORCL) ) (SID_DESC = (GLOBAL_DBNAME = GZFY) (ORACLE_HOME = C:\app\Administrator\product\11.2.0\dbhome_1) (SID_NAME = GZFY) ) )LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = pc130)(PORT = 1521)) ) )ADR_BASE_LISTENER = C:\app\Administrator tnsname.ora对应上面配置的 sid 描述，在 tnsname.ora 文件中配置 12345678GZFY = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = pc130)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = gzfy) ) ) 完整文件示例 1234567891011121314151617181920212223242526272829303132333435# tnsnames.ora Network Configuration File: C:\app\Administrator\product\11.2.0\dbhome_1\network\admin\tnsnames.ora# Generated by Oracle configuration tools.ORACLR_CONNECTION_DATA = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) (CONNECT_DATA = (SID = CLRExtProc) (PRESENTATION = RO) ) )LISTENER_ORCL = (ADDRESS = (PROTOCOL = TCP)(HOST = pc130)(PORT = 1521))ORCL = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = pc130)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl) ) )GZFY = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = pc130)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = gzfy) ) ) 重启服务重启监听服务即可解决]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>tnsname.ora</tag>
        <tag>ora-12514</tag>
        <tag>tns listener</tag>
        <tag>listener.ora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在vps上搭建ssr服务]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%9C%A8vps%E4%B8%8A%E6%90%AD%E5%BB%BAssr%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[这里是centos7为示例，介绍如何在 vps 上搭建 ssr 服务端。 服务器相关建议购买香港服务器，配置不需要高，512m 都行，腾讯云活动还是挺优惠的~(不过要新用户) 安装最新内核并开启 BBR 脚本参考资料 1wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh &amp;&amp; chmod +x bbr.sh &amp;&amp; ./bbr.sh 会重启vps 验证 1sysctl net.ipv4.tcp_congestion_control 更新系统epel-release 安装 1yum install epel-release -y 更新系统： 1yum update 安装一些软件1yum -y install wget vim SSR安装12345wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.shchmod +x shadowsocksR.sh./shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log 按提示选择对应的 配置示例 1234567Congratulations, ShadowsocksR server install completed!Your Server IP : 47.56.170.229 Your Server Port : 18406 Your Password : teddysun.com Your Protocol : auth_sha1_v4 Your obfs : tls1.2_ticket_authYour Encryption Method: aes-256-cfb 配置SSR1[root@host ~]# vim /etc/shadowsocks.json 协议和混淆插件说明 123如不考虑原版的情况下，推荐使用的协议，只有auth_sha1_v4和auth_aes128_md5和auth_aes128_sha1和auth_chain_a，推荐使用的混淆只有plain,http_simple,http_post,tls1.2_ticket_auth不要奇怪为什么推荐plain，因为混淆不总是有效果，要看各地区的策略的，有时候不混淆让其看起来像随机数据更好 单用户示例 123456789101112131415161718&#123; "server":"0.0.0.0", "server_ipv6":"[::]", "server_port":18406, "local_address":"127.0.0.1", "password": "teddysun.com", "local_port":1080, "timeout":120, "method":"aes-256-cfb", "protocol":"auth_sha1_v4", "protocol_param":"", "obfs":"tls1.2_ticket_auth", "obfs_param":"", "redirect":"", "dns_ipv6":false, "fast_open":false, "workers":1&#125; 多用户配置示例 12345678910111213141516171819&#123; "server":"0.0.0.0", "server_ipv6":"[::]", "local_address":"127.0.0.1", "port_password": &#123; "18406": "ssr18406ssr!@#" &#125;, "local_port":1080, "timeout":120, "method":"aes-256-cfb", "protocol":"auth_sha1_v4", "protocol_param":"", "obfs":"tls1.2_ticket_auth", "obfs_param":"", "redirect":"", "dns_ipv6":false, "fast_open":false, "workers":1&#125; 使用命令12345678启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status配置文件路径：/etc/shadowsocks.json日志文件路径：/var/log/shadowsocks.log代码安装目录：/usr/local/shadowsocks 卸载1./shadowsocksR.sh uninstall 参考资料https://github.com/shadowsocksr-backup/shadowsocks-rss/wiki/Server-Setuphttps://github.com/shadowsocksr-backup/shadowsocks-rss/blob/master/ssr.mdhttps://github.com/shadowsocksr-backup/shadowsocks-rss/wiki/config.json ssr客户端下载各个版本shadowsocks github 一些错误配置了多用户后重启报以下错 12345[root@zhouhongfa ~]# /etc/init.d/shadowsocks restartIPv6 not supportERROR: found an error in config.json: Expecting property name: line 7 column 1 (char 133)Stopping ShadowsocksR failedShadowsocksR (pid 26765) is already running... 解决：json格式问题~]]></content>
      <categories>
        <category>ssr</category>
      </categories>
      <tags>
        <tag>ssr</tag>
        <tag>shadowrocket</tag>
        <tag>bbr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8-Optional使用]]></title>
    <url>%2F2019%2F10%2F08%2Fjava8-Optional%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在本教程中，我们将展示Java 8中引入的Optional类。 该类的目的是提供一种用于表示可选值而非空引用的类型级别的解决方案。 创建 Optional 对象创建空对象 12345@Testpublic void whenCreatesEmptyOptional_thenCorrect() &#123; Optional&lt;String&gt; empty = Optional.empty(); assertFalse(empty.isPresent());&#125; 请注意，我们使用了 isPresent() 方法来检查 Optional 对象中是否有一个值。仅当我们使用非空值创建Optional时，该值才存在。我们将在下一节中讨论isPresent方法。 我们还可以使用以下静态方法创建Optional对象 123456@Testpublic void givenNonNull_whenCreatesNonNullable_thenCorrect() &#123; String name = "baeldung"; Optional&lt;String&gt; opt = Optional.of(name); assertTrue(opt.isPresent());&#125; 但是，传递给of()方法的参数不能为null。否则，我们将获得NullPointerException： 12345@Test(expected = NullPointerException.class)public void givenNull_whenThrowsErrorOnCreate_thenCorrect() &#123; String name = null; Optional.of(name);&#125; 但是，如果我们期望某些空值，则可以使用 ofNullable() 方法： 123456@Testpublic void givenNonNull_whenCreatesNullable_thenCorrect() &#123; String name = "baeldung"; Optional&lt;String&gt; opt = Optional.ofNullable(name); assertTrue(optionalName.isPresent());&#125; 这样，如果我们传入一个空引用，它不会抛出异常，而是返回一个空的Optional对象： 123456@Testpublic void givenNull_whenCreatesNullable_thenCorrect() &#123; String name = null; Optional&lt;String&gt; opt = Optional.ofNullable(name); assertFalse(optionalName.isPresent());&#125; 检查值是否存在 isPresent() 和 isEmpty()当我们有一个从方法返回或由我们创建的Optional对象时，可以使用isPresent()方法检查其中是否有值： 12345678@Testpublic void givenOptional_whenIsPresentWorks_thenCorrect() &#123; Optional&lt;String&gt; opt = Optional.of("Baeldung"); assertTrue(opt.isPresent()); opt = Optional.ofNullable(null); assertFalse(opt.isPresent());&#125; 如果包装的值不为null，则此方法返回true。 此外，从Java 11开始，我们可以使用isEmpty方法进行相反的操作： 12345678@Testpublic void givenAnEmptyOptional_thenIsEmptyBehavesAsExpected() &#123; Optional&lt;String&gt; opt = Optional.of("Baeldung"); assertFalse(opt.isEmpty()); opt = Optional.ofNullable(null); assertTrue(opt.isEmpty());&#125; 使用 ifPresent() 的条件操作如果发现包装值非空，则ifPresent()方法使我们能够对包装值运行一些代码。在可选之前，我们将执行以下操作： 123if(name != null) &#123; System.out.println(name.length());&#125; 此代码在继续执行一些代码之前检查name变量是否为null。这种方法很冗长，这不是唯一的问题，也容易出错。 如果忘记检查，则会导致空指针异常。 当程序由于输入问题而失败时，通常是由于不良的编程习惯造成的。 使用 Optional 使我们可以显式处理可空值，作为执行良好编程习惯的一种方式。现在让我们看一下如何在Java 8中重构以上代码。 在典型的函数式编程风格中，我们可以对实际存在的对象执行操作： 12345@Testpublic void givenOptional_whenIfPresentWorks_thenCorrect() &#123; Optional&lt;String&gt; opt = Optional.of("baeldung"); opt.ifPresent(name -&gt; System.out.println(name.length()));&#125; 默认值 orElse()orElse() 方法用于检索包装在Optional实例内的值。它采用一个参数作为默认值。 orElse() 方法返回包装的值（如果存在），否则返回其参数： 123456@Testpublic void whenOrElseWorks_thenCorrect() &#123; String nullName = null; String name = Optional.ofNullable(nullName).orElse("john"); assertEquals("john", name);&#125; 默认值 orElseGet()orElseGet() 方法类似于orElse()。但是，如果没有提供Optional值，则不采用返回值，而是采用供应商功能接口，该接口将被调用并返回调用的值： 123456@Testpublic void whenOrElseGetWorks_thenCorrect() &#123; String nullName = null; String name = Optional.ofNullable(nullName).orElseGet(() -&gt; "john"); assertEquals("john", name);&#125; orElse() 和 orElseGet() 的区别让我们在测试类中创建一个名为getMyDefault() 的方法，该方法不带任何参数并返回默认值： 1234public String getMyDefault() &#123; System.out.println("Getting Default Value"); return "Default Value";&#125; 12345678910@Testpublic void whenOrElseGetAndOrElseOverlap_thenCorrect() &#123; String text = null; String defaultText = Optional.ofNullable(text).orElseGet(this::getMyDefault); assertEquals("Default Value", defaultText); defaultText = Optional.ofNullable(text).orElse(getMyDefault()); assertEquals("Default Value", defaultText);&#125; 在上面的示例中，我们在Optional对象中包装了一个空文本，然后尝试使用两种方法中的每一种来获取包装后的值。副作用如下： 12Getting default value...Getting default value... 在每种情况下都会调用getMyDefault()方法。碰巧的是，当不存在包装的值时，orElse() 和 orElseGet() 的工作方式完全相同。 现在让我们运行另一个测试，其中存在该值，理想情况下，甚至不应创建默认值： 12345678910111213@Testpublic void whenOrElseGetAndOrElseDiffer_thenCorrect() &#123; String text = "Text present"; System.out.println("Using orElseGet:"); String defaultText = Optional.ofNullable(text).orElseGet(this::getMyDefault); assertEquals("Text present", defaultText); System.out.println("Using orElse:"); defaultText = Optional.ofNullable(text).orElse(getMyDefault()); assertEquals("Text present", defaultText);&#125; 在上面的示例中，我们不再包装空值，其余代码保持不变。现在，让我们看一下运行此代码的副作用： 123Using orElseGet:Using orElse:Getting default value... 请注意，使用orElseGet() 检索包装的值时，由于存在包含的值，因此甚至不会调用getMyDefault() 方法。 但是，使用orElse() 时，无论是否存在包装的值，都会创建默认对象。因此，在这种情况下，我们仅创建了一个从未使用过的冗余对象。 在这个简单的示例中，创建默认对象不会花费很多成本，因为JVM知道如何处理此类对象。但是，当诸如getMyDefault() 之类的方法必须进行Web服务调用或查询数据库时，则成本变得非常明显。 异常 orElseThrow()orElseThrow() 方法紧随 orElse() 和 orElseGet() 之后，并添加了一种新方法来处理缺少的值。当包装值不存在时，它不会返回默认值，而是会引发异常： 123456@Test(expected = IllegalArgumentException.class)public void whenOrElseThrowWorks_thenCorrect() &#123; String nullName = null; String name = Optional.ofNullable(nullName).orElseThrow( IllegalArgumentException::new);&#125; 使用 get() 返回值检索包装值的最终方法是 get() 方法： 123456@Testpublic void givenOptional_whenGetsValue_thenCorrect() &#123; Optional&lt;String&gt; opt = Optional.of("baeldung"); String name = opt.get(); assertEquals("baeldung", name);&#125; 但是，与上述三种方法不同，get（）仅在包装的对象不为null时才能返回值，否则，将引发 no such element 异常： 12345@Test(expected = NoSuchElementException.class)public void givenOptionalWithNull_whenGetThrowsException_thenCorrect() &#123; Optional&lt;String&gt; opt = Optional.ofNullable(null); String name = opt.get();&#125; 这是 get() 方法的主要缺陷。理想情况下，Optional应该可以帮助我们避免此类不可预见的异常。因此，此方法违背Optional的目标，并且可能在以后的版本中弃用。 带条件返回 filter()我们可以使用filter方法对包装的值进行内联测试。它以断言作为参数，并返回Optional对象。如果包装的值通过断言的测试，则按原样返回Optional。 但是，如果断言返回false，则它将返回空的Optional： 123456789@Testpublic void whenOptionalFilterWorks_thenCorrect() &#123; Integer year = 2016; Optional&lt;Integer&gt; yearOptional = Optional.of(year); boolean is2016 = yearOptional.filter(y -&gt; y == 2016).isPresent(); assertTrue(is2016); boolean is2017 = yearOptional.filter(y -&gt; y == 2017).isPresent(); assertFalse(is2017);&#125; 通常以这种方式使用filter方法来基于预定义规则拒绝包装的值。我们可以使用它来拒绝错误的电子邮件格式或强度不够的密码。 让我们看另一个有意义的例子。假设我们要购买调制解调器，而我们只关心它的价格。我们从某个站点接收有关调制解调器价格的推送通知，并将其存储在对象中： 12345678public class Modem &#123; private Double price; public Modem(Double price) &#123; this.price = price; &#125; // standard getters and setters&#125; 然后，我们将这些对象提供给某些代码，其唯一目的是检查调制解调器的价格是否在我们预算的范围内。 现在让我们看一下没用Optional的代码： 1234567891011public boolean priceIsInRange1(Modem modem) &#123; boolean isInRange = false; if (modem != null &amp;&amp; modem.getPrice() != null &amp;&amp; (modem.getPrice() &gt;= 10 &amp;&amp; modem.getPrice() &lt;= 15)) &#123; isInRange = true; &#125; return isInRange;&#125; 请注意要实现此目的必须编写多少代码，尤其是在if条件下。如果条件对应用程序至关重要，则唯一的部分是最后的价格范围检查；其余的检查是防御性的： 12345678@Testpublic void whenFiltersWithoutOptional_thenCorrect() &#123; assertTrue(priceIsInRange1(new Modem(10.0))); assertFalse(priceIsInRange1(new Modem(9.9))); assertFalse(priceIsInRange1(new Modem(null))); assertFalse(priceIsInRange1(new Modem(15.5))); assertFalse(priceIsInRange1(null));&#125; 除此之外，很可能会忘记一整天的空检查而不会出现任何编译时错误。 现在让我们看一下带有Optional＃filter的变体： 1234567public boolean priceIsInRange2(Modem modem2) &#123; return Optional.ofNullable(modem2) .map(Modem::getPrice) .filter(p -&gt; p &gt;= 10) .filter(p -&gt; p &lt;= 15) .isPresent(); &#125; 映射调用仅用于将一个值转换为其他值。请记住，此操作不会修改原始值。 在我们的例子中，我们从 Mode l类中获取一个价格对象。在下一节中，我们将详细介绍 map() 方法。 首先，如果将null对象传递给此方法，则不会有任何问题。 其次，我们在其主体内编写的唯一逻辑就是方法名称所描述的，即价格范围检查。可选的照顾其余的： 12345678@Testpublic void whenFiltersWithOptional_thenCorrect() &#123; assertTrue(priceIsInRange2(new Modem(10.0))); assertFalse(priceIsInRange2(new Modem(9.9))); assertFalse(priceIsInRange2(new Modem(null))); assertFalse(priceIsInRange2(new Modem(15.5))); assertFalse(priceIsInRange2(null));&#125; 先前的方法有望检查价格范围，但除了防御其固有的脆弱性外，还必须做更多的事情。因此，我们可以使用filter方法替换不必要的if语句并拒绝不需要的值。 使用 map() 转换值在上一节中，我们研究了如何基于过滤器拒绝或接受值。我们可以使用类似的语法通过 map() 方法转换Optional值： 1234567891011@Testpublic void givenOptional_whenMapWorks_thenCorrect() &#123; List&lt;String&gt; companyNames = Arrays.asList( "paypal", "oracle", "", "microsoft", "", "apple"); Optional&lt;List&lt;String&gt;&gt; listOptional = Optional.of(companyNames); int size = listOptional .map(List::size) .orElse(0); assertEquals(6, size);&#125; 在此示例中，我们将字符串列表包装在Optional对象中，并使用其map方法对包含的列表执行操作。我们执行的操作是检索列表的大小。 map方法返回包装在Optional中的计算结果。然后，我们必须在返回的Optional上调用适当的方法以获取其值。 请注意，filter方法仅对值进行检查并返回布尔值。另一方面，map方法采用现有值，使用该值执行计算，然后返回包装在Optional对象中的计算结果： 12345678910@Testpublic void givenOptional_whenMapWorks_thenCorrect2() &#123; String name = "baeldung"; Optional&lt;String&gt; nameOptional = Optional.of(name); int len = nameOptional .map(String::length) .orElse(0); assertEquals(8, len);&#125; 我们可以将 map 和 filter 链接在一起，以执行更强大的操作。 假设我们要检查用户输入的密码是否正确；我们可以使用 map 转换清除密码，并使用过滤器检查密码的正确性： 1234567891011121314@Testpublic void givenOptional_whenMapWorksWithFilter_thenCorrect() &#123; String password = " password "; Optional&lt;String&gt; passOpt = Optional.of(password); boolean correctPassword = passOpt.filter( pass -&gt; pass.equals("password")).isPresent(); assertFalse(correctPassword); correctPassword = passOpt .map(String::trim) .filter(pass -&gt; pass.equals("password")) .isPresent(); assertTrue(correctPassword);&#125; 如我们所见，如果不先清除输入内容，就会将其过滤掉-但是用户可能会认为前导空格和尾随空格都构成了输入。因此，在过滤掉不正确的密码之前，我们先使用 map 将脏密码转换为干净的密码。 使用 flatMap() 转换值就像 map() 方法一样，我们也有 flatMap() 方法作为转换值的替代方法。区别在于，map 仅在展开值时才转换值，而flatMap会在转换值之前采用已包装的值并对其进行解包。 以前，我们创建了简单的String和Integer对象，用于包装在Optional实例中。但是，通常，我们将从复杂对象的访问者那里接收这些对象。 为了更清楚地了解两者之间的区别，让我们看一下一个Person对象，该对象带有一个人的详细信息，例如姓名，年龄和密码： 12345678910111213141516171819public class Person &#123; private String name; private int age; private String password; public Optional&lt;String&gt; getName() &#123; return Optional.ofNullable(name); &#125; public Optional&lt;Integer&gt; getAge() &#123; return Optional.ofNullable(age); &#125; public Optional&lt;String&gt; getPassword() &#123; return Optional.ofNullable(password); &#125; // normal constructors and setters&#125; 我们通常会创建一个这样的对象并将其包装在Optional对象中，就像处理String一样。或者，可以通过另一个方法调用将其返回给我们： 12Person person = new Person("john", 26);Optional&lt;Person&gt; personOptional = Optional.of(person); 现在注意，当包装一个Person对象时，它将包含嵌套的Optional实例： 1234567891011121314151617@Testpublic void givenOptional_whenFlatMapWorks_thenCorrect2() &#123; Person person = new Person("john", 26); Optional&lt;Person&gt; personOptional = Optional.of(person); Optional&lt;Optional&lt;String&gt;&gt; nameOptionalWrapper = personOptional.map(Person::getName); Optional&lt;String&gt; nameOptional = nameOptionalWrapper.orElseThrow(IllegalArgumentException::new); String name1 = nameOptional.orElse(""); assertEquals("john", name1); String name = personOptional .flatMap(Person::getName) .orElse(""); assertEquals("john", name);&#125; 在这里，我们试图检索Person对象的name属性以执行断言。 注意在第三条语句中如何使用map() 方法实现此目的，然后注意之后如何使用flatMap()方法来实现此目的。 Person::getName方法的引用类似于上一节中用于清理密码的String::trim调用。 唯一的区别是 getName() 返回的是Optional而不是String，与 trim() 操作一样。这加上map转换将结果包装在Optional对象中的事实导致嵌套的Optional。 因此，在使用 map() 方法时，我们需要在使用转换后的值之前添加一个额外的调用来检索值。这样，可选包装将被删除。使用 flatMap 时，将隐式执行此操作。 Java8 链式 Optional有时，我们可能需要从多个Optional中获取第一个非空的Optional对象。在这种情况下，使用类似orElseOptional()的方法将非常方便。不幸的是，Java 8不直接支持这种操作。 让我们首先介绍在本节中将要使用的一些方法： 123456789101112131415161718private Optional&lt;String&gt; getEmpty() &#123; return Optional.empty();&#125; private Optional&lt;String&gt; getHello() &#123; return Optional.of("hello");&#125; private Optional&lt;String&gt; getBye() &#123; return Optional.of("bye");&#125; private Optional&lt;String&gt; createOptional(String input) &#123; if (input == null || "".equals(input) || "empty".equals(input)) &#123; return Optional.empty(); &#125; return Optional.of(input);&#125; 为了链接多个Optional对象并获得Java 8中的第一个非空对象，我们可以使用Stream API： 123456789@Testpublic void givenThreeOptionals_whenChaining_thenFirstNonEmptyIsReturned() &#123; Optional&lt;String&gt; found = Stream.of(getEmpty(), getHello(), getBye()) .filter(Optional::isPresent) .map(Optional::get) .findFirst(); assertEquals(getHello(), found);&#125; 这种方法的缺点是，无论Stream中非空Optional出现在何处，我们所有的get方法都始终执行。 如果我们想懒惰地评估传递给Stream.of()的方法，则需要使用方法参考和Supplier接口： 1234567891011@Testpublic void givenThreeOptionals_whenChaining_thenFirstNonEmptyIsReturnedAndRestNotEvaluated() &#123; Optional&lt;String&gt; found = Stream.&lt;Supplier&lt;Optional&lt;String&gt;&gt;&gt;of(this::getEmpty, this::getHello, this::getBye) .map(Supplier::get) .filter(Optional::isPresent) .map(Optional::get) .findFirst(); assertEquals(getHello(), found);&#125; 如果需要使用带有参数的方法，则必须求助于lambda表达式： 12345678910111213@Testpublic void givenTwoOptionalsReturnedByOneArgMethod_whenChaining_thenFirstNonEmptyIsReturned() &#123; Optional&lt;String&gt; found = Stream.&lt;Supplier&lt;Optional&lt;String&gt;&gt;&gt;of( () -&gt; createOptional("empty"), () -&gt; createOptional("hello") ) .map(Supplier::get) .filter(Optional::isPresent) .map(Optional::get) .findFirst(); assertEquals(createOptional("hello"), found);&#125; 通常，在所有链接的Optionals为空的情况下，我们通常希望返回一个默认值。我们可以通过添加对orElse()或orElseGet()的调用来做到这一点，如以下示例所示： 1234567891011121314@Testpublic void givenTwoEmptyOptionals_whenChaining_thenDefaultIsReturned() &#123; String found = Stream.&lt;Supplier&lt;Optional&lt;String&gt;&gt;&gt;of( () -&gt; createOptional("empty"), () -&gt; createOptional("empty") ) .map(Supplier::get) .filter(Optional::isPresent) .map(Optional::get) .findFirst() .orElseGet(() -&gt; "default"); assertEquals("default", found);&#125; jdk9 Optional ApiJava 9 Optional API Additions Optional的滥用最后，让我们看一下使用Optionals的诱人但危险的方法：将Optional参数传递给方法。 想象我们有一个人员列表，我们想要一种方法在该列表中搜索具有给定名称的人。另外，如果指定了年龄，我们希望该方法匹配至少具有一定年龄的条目。由于此参数是可选的，因此我们提供了以下方法： 1234567public List&lt;Person&gt; search(List&lt;Person&gt; people, String name, Optional&lt;Integer&gt; age) &#123; // Null checks for people and name people.stream() .filter(p -&gt; p.getName().equals(name)) .filter(p -&gt; p.getAge() &gt;= age.orElse(0)) .collect(Collectors.toList());&#125; 然后，我们发布我们的方法，另一个开发人员尝试使用它： 现在，开发人员执行其代码并获取一个NullPointerException。在这里，我们必须对我们的可选参数进行空检查，这在想要避免这种情况时违背了我们的初衷。 我们可能会做一些更好的处理方法： 123456789public List&lt;Person&gt; search(List&lt;Person&gt; people, String name, Integer age) &#123; // Null checks for people and name age = age != null ? age : 0; people.stream() .filter(p -&gt; p.getName().equals(name)) .filter(p -&gt; p.getAge() &gt;= age) .collect(Collectors.toList());&#125; 在那里，该参数仍然是可选的，但是我们仅需检查一下即可处理它。另一种可能性是创建两个重载方法： 123456789101112131415public List&lt;Person&gt; search(List&lt;Person&gt; people, String name) &#123; return doSearch(people, name, 0);&#125; public List&lt;Person&gt; search(List&lt;Person&gt; people, String name, int age) &#123; return doSearch(people, name, age);&#125; private List&lt;Person&gt; doSearch(List&lt;Person&gt; people, String name, int age) &#123; // Null checks for people and name return people.stream() .filter(p -&gt; p.getName().equals(name)) .filter(p -&gt; p.getAge() &gt;= age) .collect(Collectors.toList());&#125; 这样，我们提供了一个清晰的API，其中包含两种可以完成不同任务的方法（尽管它们共享实现）。 因此，有一些解决方案可以避免使用Optionals作为方法参数。发行Optional时Java的意图是将其用作返回类型，从而表明方法可以返回空值。实际上，某些代码检查人员甚至不建议使用Optional作为方法参数。 参考资源Oracle Optional文档 原文 Guide To Java 8 Optional]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
        <tag>Optional</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA-运行程序时指定环境变量]]></title>
    <url>%2F2019%2F10%2F08%2FIDEA-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%E6%97%B6%E6%8C%87%E5%AE%9A%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[记一下在idea中修改环境变量值的操作，免得忘了~ 在 Edit Configuration 中配置 Environment variables 直接配置即可，可以覆盖系统中的值]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-sql函数笔记]]></title>
    <url>%2F2019%2F10%2F08%2Foracle-sql%E5%87%BD%E6%95%B0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明记录一些oracle常用的函数 nvl从两个表达式返回一个非 null 值。 语法 NVL(eExpression1, eExpression2) 参数eExpression1, eExpression2 如果 eExpression1 的计算结果为 null 值，则 NVL( ) 返回 eExpression2。如果 eExpression1 的计算结果不是 null 值，则返回 eExpression1。eExpression1 和 eExpression2 可以是任意一种数据类型。如果 eExpression1 与 eExpression2 的结果皆为 null 值，则 NVL( ) 返回 .NULL.。 to_datemm 和 mi注意 1to_date(t.mcyjbh, &apos;yyyy-mm-dd HH:mi:ss&apos;)]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>nvl</tag>
        <tag>to_date</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-DBMS_JOB使用笔记]]></title>
    <url>%2F2019%2F10%2F08%2Foracle-DBMS-JOB%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[用于提交定时任务 基础使用123456789declare job number;begin dbms_job.submit( job =&gt;job, what=&gt;&apos;PROC1;PROC2;&apos;, next_date =&gt; sysdate, -- 初次执行时间 interval =&gt;&apos;TRUNC(SYSDATE + 1))&apos;;--每天凌晨同步一次 commit;end; 一些实例 1234DBMS_JOB.SUBMIT(JOB =&gt; job_id, WHAT =&gt; 'pkg_platform_data_upload_yibo.tb_cis_consult(trunc(sysdate - 1));', NEXT_DATE =&gt; TRUNC(sysdate) + 1 +1/(24), INTERVAL =&gt; 'TRUNC(sysdate) + 1 +1/(24)'); 查询已创建的任务1select * form user_jobs 删除任务1EXEC DBMS_JOB.REMOVE(123); 使用存储过程执行示例12345678910111213141516171819202122create or replace procedure proc_auto_exec_job asbegin declare job number; BEGIN DBMS_JOB.SUBMIT( JOB =&gt; job, /*自动生成JOB_ID*/ WHAT =&gt; 'proc_test_job;', /*需要执行的过程或SQL语句*/ /*NEXT_DATE =&gt; sysdate, */ /*初次执行时间，立刻执行*/ /*INTERVAL =&gt; 'sysdate+3/(24*60*60)' */ /*执行周期 -每3秒钟*/ NEXT_DATE =&gt; TRUNC(SYSDATE+1)+(0*60+30)/(24*60), /*初次执行时间，12点30分*/ INTERVAL =&gt; 'TRUNC(SYSDATE+1)+(0*60+30)/(24*60)' /*每天12点30分*/ ); COMMIT; /*dbms_job.submit(job, 'proc_test_job;', sysdate, 'trunc(sysdate,''mi'')+1/(24*60)'); /*执行周期 -每1分钟*/ commit;*/ DBMS_JOB.RUN(job); end;end proc_auto_exec_job;begin proc_auto_exec_job; end;]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>dbms_job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-分析函数over使用]]></title>
    <url>%2F2019%2F10%2F04%2Foracle-%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0over%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[开窗函数Oracle从8.1.6开始提供分析函数，分析函数用于计算基于组的某种聚合值，它和聚合函数的不同之处是：对于每个组返回多行，而聚合函数对于每个组只返回一行。 开窗函数指定了分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化 语法over（order by salary） 按照salary排序进行累计，order by是个默认的开窗函数over（partition by deptno）按照部门分区over（partition by deptno order by salary） 开窗的窗口范围over(order by salary range between 5 preceding and 5 following)：窗口范围为当前行数据幅度减5加5后的范围内的。 123456789101112--sum(s)over(order by s range between 2 preceding and 2 following) 表示加2或2的范围内的求和select name,class,s,sum(s)over(order by s range between 2 preceding and 2 following) mm from t2adf 3 45 45 --45加2减2即43到47，但是s在这个范围内只有45asdf 3 55 55cfe 2 74 743dd 3 78 158 --78在76到80范围内有78，80，求和得158fda 1 80 158gds 2 92 92ffd 1 95 190dss 1 95 190ddd 3 99 198gf 3 99 198 over(order by salary rows between 5 preceding and 5 following) ：窗口范围为当前行前后各移动5行。 123456789101112--sum(s)over(order by s rows between 2 preceding and 2 following)表示在上下两行之间的范围内select name,class,s, sum(s)over(order by s rows between 2 preceding and 2 following) mm from t2adf 3 45 174 （45+55+74=174）asdf 3 55 252 （45+55+74+78=252）cfe 2 74 332 （74+55+45+78+80=332）3dd 3 78 379 （78+74+55+80+92=379）fda 1 80 419gds 2 92 440ffd 1 95 461dss 1 95 480ddd 3 99 388gf 3 99 293 over（order by salary range between unbounded preceding and unbounded following）或者over（order by salary rows between unbounded preceding and unbounded following）：窗口不做限制 与over函数结合的几个函数介绍row_number()over()、rank()over()和dense_rank()over()函数的使用下面以班级成绩表t2来说明其应用 t2表信息如下： 12345678910cfe 2 74dss 1 95ffd 1 95fda 1 80gds 2 92gf 3 99ddd 3 99adf 3 45asdf 3 553dd 3 78 12345select * from ( select name,class,s,rank()over(partition by class order by s desc) mm from t2 ) where mm=1； 得到的结果是: 12345dss 1 95 1ffd 1 95 1gds 2 92 1gf 3 99 1ddd 3 99 1 注意： 1.在求第一名成绩的时候，不能用row_number()，因为如果同班有两个并列第一，row_number()只返回一个结果; 12345select * from ( select name,class,s,row_number()over(partition by class order by s desc) mm from t2 ) where mm=1； 1231 95 1 --95有两名但是只显示一个2 92 13 99 1 --99有两名但也只显示一个 2.rank()和dense_rank()可以将所有的都查找出来： 如上可以看到采用rank可以将并列第一名的都查找出来；rank()和dense_rank()区别：–rank()是跳跃排序，有两个第二名时接下来就是第四名； 1select name,class,s,rank()over(partition by class order by s desc) mm from t2 12345678910dss 1 95 1ffd 1 95 1fda 1 80 3 --直接就跳到了第三gds 2 92 1cfe 2 74 2gf 3 99 1ddd 3 99 13dd 3 78 3asdf 3 55 4adf 3 45 5 –dense_rank()l是连续排序，有两个第二名时仍然跟着第三名 1select name,class,s,dense_rank()over(partition by class order by s desc) mm from t2 12345678910dss 1 95 1ffd 1 95 1fda 1 80 2 --连续排序（仍为2）gds 2 92 1cfe 2 74 2gf 3 99 1ddd 3 99 13dd 3 78 2asdf 3 55 3adf 3 45 4 sum() over()的使用1select name,class,s, sum(s)over(partition by class order by s desc) mm from t2 --根据班级进行分数求和 12345678910dss 1 95 190 --由于两个95都是第一名，所以累加时是两个第一名的相加ffd 1 95 190fda 1 80 270 --第一名加上第二名的gds 2 92 92cfe 2 74 166gf 3 99 198ddd 3 99 1983dd 3 78 276asdf 3 55 331adf 3 45 376 first_value() over()和last_value() over()的使用找出这三条电路每条电路的第一条记录类型和最后一条记录类型 123456SELECT opr_id,res_type, first_value(res_type) over(PARTITION BY opr_id ORDER BY res_type) low, last_value(res_type) over(PARTITION BY opr_id ORDER BY res_type rows BETWEEN unbounded preceding AND unbounded following) high FROM rm_circuit_routeWHERE opr_id IN ('000100190000000000021311','000100190000000000021355','000100190000000000021339') ORDER BY opr_id; 注：rows BETWEEN unbounded preceding AND unbounded following 的使用 –取last_value时不使用 rows BETWEEN unbounded preceding AND unbounded following 的结果 123456SELECT opr_id,res_type, first_value(res_type) over(PARTITION BY opr_id ORDER BY res_type) low, last_value(res_type) over(PARTITION BY opr_id ORDER BY res_type) high FROM rm_circuit_route WHERE opr_id IN ('000100190000000000021311','000100190000000000021355','000100190000000000021339') ORDER BY opr_id; 如果不使用rows BETWEEN unbounded preceding AND unbounded following，取出的last_value由于与res_type进行进行排列，因此取出的电路的最后一行记录的类型就不是按照电路的范围提取了，而是以res_type为范围进行提取了。 在first_value和last_value中ignore nulls的使用取出该电路的第一条记录，加上ignore nulls后，如果第一条是判断的那个字段是空的，则默认取下一条，结果如下所示： 123select opr_id, frist_value(route_name ignore nulls) over(order by opr_id)from rm_circuit_routewhere opr_id = '0000123' lag() over()函数用法（取出前n行数据）lag(expresstion,&lt;offset&gt;,&lt;default&gt;) 123456789101112with a as(select 1 id,'a' name from dual union select 2 id,'b' name from dual union select 3 id,'c' name from dual union select 4 id,'d' name from dual union select 5 id,'e' name from dual)select id,name,lag(id,1,'')over(order by name) from a; lead() over()函数用法（取出后N行数据）lead(expresstion,&lt;offset&gt;,&lt;default&gt;) 123456789101112with a as(select 1 id,'a' name from dual union select 2 id,'b' name from dual union select 3 id,'c' name from dual union select 4 id,'d' name from dual union select 5 id,'e' name from dual)select id,name,lead(id,1,'')over(order by name) from a; ratio_to_report(a)函数用法Ratio_to_report() 括号中就是分子，over() 括号中就是分母 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556with a as (select 1 a from dual union allselect 1 a from dual union allselect 1 a from dual union allselect 2 a from dual union allselect 3 a from dual union allselect 4 a from dual union allselect 4 a from dual union allselect 5 a from dual )select a, ratio_to_report(a)over(partition by a) b from aorder by a;with a as (select 1 a from dual union allselect 1 a from dual union allselect 1 a from dual union allselect 2 a from dual union allselect 3 a from dual union allselect 4 a from dual union allselect 4 a from dual union allselect 5 a from dual )select a, ratio_to_report(a)over() b from a --分母缺省就是整个占比order by a;with a as (select 1 a from dual union allselect 1 a from dual union allselect 1 a from dual union allselect 2 a from dual union allselect 3 a from dual union allselect 4 a from dual union allselect 4 a from dual union allselect 5 a from dual )select a, ratio_to_report(a)over() b from agroup by a order by a;--分组后的占比 percent_rank用法计算方法：所在组排名序号-1除以该组所有的行数-1，如下所示自己计算的pr1与通过percent_rank函数得到的值是一样的： 123456789101112131415SELECT a.deptno, a.ename, a.sal, a.r, b.n, (a.r-1)/(n-1) pr1, percent_rank() over(PARTITION BY a.deptno ORDER BY a.sal) pr2 FROM (SELECT deptno, ename, sal, rank() over(PARTITION BY deptno ORDER BY sal) r --计算出在组中的排名序号 FROM emp ORDER BY deptno, sal) a, (SELECT deptno, COUNT(1) n FROM emp GROUP BY deptno) b --按部门计算每个部门的所有成员数 WHERE a.deptno = b.deptno; cume_dist函数计算方法：所在组排名序号除以该组所有的行数，但是如果存在并列情况，则需加上并列的个数-1，如下所示自己计算的pr1与通过percent_rank函数得到的值是一样的： 123456789101112131415161718192021222324SELECT a.deptno, a.ename, a.sal, a.r, b.n, c.rn, (a.r + c.rn - 1) / n pr1, cume_dist() over(PARTITION BY a.deptno ORDER BY a.sal) pr2 FROM (SELECT deptno, ename, sal, rank() over(PARTITION BY deptno ORDER BY sal) r FROM emp ORDER BY deptno, sal) a, (SELECT deptno, COUNT(1) n FROM emp GROUP BY deptno) b, (SELECT deptno, r, COUNT(1) rn,sal FROM (SELECT deptno,sal, rank() over(PARTITION BY deptno ORDER BY sal) r FROM emp) GROUP BY deptno, r,sal ORDER BY deptno) c --c表就是为了得到每个部门员工工资的一样的个数 WHERE a.deptno = b.deptno AND a.deptno = c.deptno(+) AND a.sal = c.sal; percentile_cont函数含义：输入一个百分比（该百分比就是按照percent_rank函数计算的值），返回该百分比位置的平均值如下，输入百分比为0.7，因为0.7介于0.6和0.8之间，因此返回的结果就是0.6对应的sal的1500加上0.8对应的sal的1600平均 1234567SELECT ename, sal, deptno, percentile_cont(0.7) within GROUP(ORDER BY sal) over(PARTITION BY deptno) "Percentile_Cont", percent_rank() over(PARTITION BY deptno ORDER BY sal) "Percent_Rank" FROM emp WHERE deptno IN (30, 60); PERCENTILE_DISC函数功能描述：返回一个与输入的分布百分比值相对应的数据值，分布百分比的计算方法见函数CUME_DIST，如果没有正好对应的数据值，就取大于该分布值的下一个值。注意：本函数与PERCENTILE_CONT的区别在找不到对应的分布值时返回的替代值的计算方法不同 SAMPLE：下例中0.7的分布值在部门30中没有对应的Cume_Dist值，所以就取下一个分布值0.83333333所对应的SALARY来替代 1234567SELECT ename, sal, deptno, percentile_disc(0.7) within GROUP(ORDER BY sal) over(PARTITION BY deptno) "Percentile_Disc", cume_dist() over(PARTITION BY deptno ORDER BY sal) "Cume_Dist" FROM emp WHERE deptno IN (30, 60); 示例按 zjhm,xm 分组, ywscsj 降序，取第一条记录 123456select aa.* from (select t.* ROW_NUMBER() over(partition by t.zjhm, t.xm order by t.ywscsj desc) as g_id from hiop.tb_yl_patient_information t) aa where aa.g_id = 1 参考资料OVER(PARTITION BY)函数用法 ROW_NUMBER(), RANK(), DENSE_RANK() WITH EXAMPLE]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>over(partition by)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle-字符串处理相关笔记]]></title>
    <url>%2F2019%2F10%2F04%2Foracle-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录一些在 oracle 中的字符串处理相关的函数用法 判断是否存在字符串返回的是所在的Index 1234567891011SQL&gt; select INSTR(&apos;广东省&apos;, &apos;省&apos;) from dual;INSTR(&apos;广东省&apos;,&apos;省&apos;)---------------- 3SQL&gt; select INSTR(&apos;广东省&apos;, &apos;北&apos;) from dual;INSTR(&apos;广东省&apos;,&apos;北&apos;)---------------- 0 替换字符串replace12345SQL&gt; select replace(&apos;广东省 广州市&apos;, &apos; &apos;, &apos;&apos;) c1 from dual;C1------------------广东省广州市 正则 REGEXP_REPLACE123456789101112SQL&gt; select regexp_replace(&apos;23456中国3-00=.,45&apos;,&apos;[^0-9]&apos;) from dual;REGEXP_REPLACE(&apos;23456中国3-0--------------------------2345630045 SQL&gt; select regexp_replace(&apos;23456中国3-00=.,45&apos;,&apos;[^0-9]&apos;, &apos;@&apos;) c from dual;C----------------23456@@3@00@@@45]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>INSTR</tag>
        <tag>replace</tag>
        <tag>regexp_replace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-mkdir]]></title>
    <url>%2F2019%2F09%2F21%2FLinux%E5%91%BD%E4%BB%A4-mkdir%2F</url>
    <content type="text"><![CDATA[linux mkdir 命令用来创建指定的名称的目录，要求创建目录的用户在当前目录中具有写权限，并且指定的目录名不能是当前目录中已有的目录。 命令格式1mkdir [选项] 目录... 命令功能通过 mkdir 命令可以实现在指定位置创建以 DirName(指定的文件名)命名的文件夹或目录。要创建文件夹或目录的用户必须对所创建的文件夹的父文件夹具有写权限。并且，所创建的文件夹(目录)不能与其父目录(即父文件夹)中的文件名重名，即同一个目录下不能有同名的(区分大小写)。 命令参数 参数 说明 -m, –mode=模式 设定权限&lt;模式&gt; (类似 chmod)，而不是 rwxrwxrwx 减 umask -p, –parents 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立好那些尚不存在的目录,即一次可以建立多个目录; -v, –verbose 每次创建新目录都显示信息 –help 显示此帮助信息并退出 –version 输出版本信息并退出 命令实例创建一个空目录命令： mkdir test1 输出： 12345678[root@localhost soft]# cd test[root@localhost test]# mkdir test1[root@localhost test]# ll总计 4drwxr-xr-x 2 root root 4096 10-25 17:42 test1[root@localhost test]# 递归创建多个目录命令： mkdir -p test2/test22 输出： 123456789101112[root@localhost test]# mkdir -p test2/test22[root@localhost test]# ll总计 8drwxr-xr-x 2 root root 4096 10-25 17:42 test1drwxr-xr-x 3 root root 4096 10-25 17:44 test2[root@localhost test]# cd test2/[root@localhost test2]# ll总计 4drwxr-xr-x 2 root root 4096 10-25 17:44 test22[root@localhost test2]# 创建权限为777的目录命令： mkdir -m 777 test3 输出： 12345678[root@localhost test]# mkdir -m 777 test3[root@localhost test]# ll总计 12drwxr-xr-x 2 root root 4096 10-25 17:42 test1drwxr-xr-x 3 root root 4096 10-25 17:44 test2drwxrwxrwx 2 root root 4096 10-25 17:46 test3[root@localhost test]# 说明： test3 的权限为 rwxrwxrwx 创建新目录都显示信息命令： mkdir -v test4 输出： 12345678[root@localhost test]# mkdir -v test4mkdir: 已创建目录 “test4”[root@localhost test]# mkdir -vp test5/test5-1mkdir: 已创建目录 “test5”mkdir: 已创建目录 “test5/test5-1”[root@localhost test]# 一个命令创建项目的目录结构参考：http://www.ibm.com/developerworks/cn/aix/library/au-badunixhabits.html 命令： 1mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125; 输出： 1234567891011121314151617181920212223242526272829303132[root@localhost test]# mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;mkdir: 已创建目录 “scf”mkdir: 已创建目录 “scf/lib”mkdir: 已创建目录 “scf/bin”mkdir: 已创建目录 “scf/doc”mkdir: 已创建目录 “scf/doc/info”mkdir: 已创建目录 “scf/doc/product”mkdir: 已创建目录 “scf/logs”mkdir: 已创建目录 “scf/logs/info”mkdir: 已创建目录 “scf/logs/product”mkdir: 已创建目录 “scf/service”mkdir: 已创建目录 “scf/service/deploy”mkdir: 已创建目录 “scf/service/deploy/info”mkdir: 已创建目录 “scf/service/deploy/product”[root@localhost test]# tree scf/scf/|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files[root@localhost test]# 原文每天一个linux命令（4）：mkdir命令]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
        <tag>mkdir</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-pwd]]></title>
    <url>%2F2019%2F09%2F21%2FLinux%E5%91%BD%E4%BB%A4-pwd%2F</url>
    <content type="text"><![CDATA[Linux中用 pwd 命令来查看”当前工作目录“的完整路径。 简单得说，每当你在终端进行操作时，你都会有一个当前工作目录。 在不太确定当前位置时，就会使用pwd来判定当前目录在文件系统内的确切位置。 命令格式pwd [选项] 命令功能查看”当前工作目录“的完整路径 常用参数一般情况下不带任何参数 如果目录是链接时： 格式：pwd -P 显示出实际路径，而非使用连接（link）路径。 常用实例用 pwd 命令查看默认工作目录的完整路径命令： pwd 输出： 12[root@localhost ~]# pwd/root 使用 pwd 命令查看指定文件夹命令： pwd 输出： 123[root@localhost ~]# cd /opt/soft/[root@localhost soft]# pwd /opt/soft 目录连接链接时，显示出实际路径，而非使用连接路径命令： pwd -P 输出： 123456[root@localhost soft]# cd /etc/init.d [root@localhost init.d]# pwd/etc/init.d[root@localhost init.d]# pwd -P/etc/rc.d/init.d[root@localhost init.d]# /bin/pwd命令： /bin/pwd [选项] 选项： -L 目录连接链接时，输出连接路径 -P 输出物理路径 输出： 12345678[root@localhost init.d]# /bin/pwd /etc/rc.d/init.d[root@localhost init.d]# /bin/pwd --help[root@localhost init.d]# /bin/pwd -P/etc/rc.d/init.d[root@localhost init.d]# /bin/pwd -L/etc/init.d[root@localhost init.d]# 当前目录被删除了，而pwd命令仍然显示那个目录输出： 1234567891011121314151617181920212223[root@localhost init.d]# cd /opt/soft[root@localhost soft]# mkdir removed[root@localhost soft]# cd removed/[root@localhost removed]# pwd/opt/soft/removed[root@localhost removed]# rm ../removed -rf[root@localhost removed]# pwd/opt/soft/removed[root@localhost removed]# /bin/pwd/bin/pwd: couldn&apos;t find directory entry in “..” with matching i-node[root@localhost removed]# cd [root@localhost ~]# pwd/root[root@localhost ~]# 原文每天一个linux命令（3）：pwd命令]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
        <tag>pwd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法笔记]]></title>
    <url>%2F2019%2F09%2F21%2FMarkdown%E8%AF%AD%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录常用的 Markdown 语法，以及在使用 Markdown语法中遇到的问题 在代码块中显示 `参考 如果要在 `` 中显示 ` ，则要在外层加多 ``` hi` `` 在 markdown 中显示 为 hi` 一些参考Markdown 语法]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ls]]></title>
    <url>%2F2019%2F09%2F10%2FLinux%E5%91%BD%E4%BB%A4-ls%2F</url>
    <content type="text"><![CDATA[ls命令是linux下最常用的命令。 ls命令是linux下最常用的命令。ls命令就是list的缩写，缺省下ls用来打印出当前目录的清单。如果ls指定其他目录，那么就会显示指定目录里的文件及文件夹清单。 通过ls 命令不仅可以查看linux文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)，查看目录信息等等。ls 命令在日常的linux操作中用的很多! 命令格式ls [选项] [目录名] 命令功能列出目标目录中所有的子目录和文件 常用参数 参数 说明 -a, –all 列出目录下的所有文件，包括以 . 开头的隐含文件 -A 同-a，但不列出“.”(表示当前目录)和“..”(表示当前目录的父目录)。 -c 配合 -lt：根据 ctime 排序及显示 ctime (文件状态最后更改的时间)配合 -l：显示 ctime 但根据名称排序否则：根据 ctime 排序 -C 每栏由上至下列出项目 –color[=WHEN] 控制是否使用色彩分辨文件。WHEN 可以是’never’、’always’或’auto’其中之一 -d, –-directory 将目录象文件一样显示，而不是显示其下的文件。 -D, -–dired 产生适合 Emacs 的 dired 模式使用的结果 -f 对输出的文件不进行排序，-aU 选项生效，-lst 选项失效 -g 类似 -l,但不列出所有者 -G, –-no-group 不列出任何有关组的信息 -h，-–human-readable 以容易理解的格式列出文件大小 (例如 1K 234M 2G) –si 类似 -h,但文件大小取 1000 的次方而不是 1024 -H, -–dereference-command-line 使用命令列中的符号链接指示的真正目的地 –indicator-style=方式 指定在每个项目名称后加上指示符号&lt;方式&gt;：none (默认)，classify (-F)，file-type (-p) -i, -–inode 印出每个文件的 inode 号 -I, –-ignore=样式 不印出任何符合 shell 万用字符&lt;样式&gt;的项目 -k 即 –block-size=1K,以 k 字节的形式表示文件的大小。 -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来。 -L, -–dereference 当显示符号链接的文件信息时，显示符号链接所指示的对象而并非符号链接本身的信息 -m 所有项目以逗号分隔，并填满整行行宽 -o 类似 -l,显示文件的除组信息外的详细信息。 -r, –-reverse 依相反次序排列 -R, –-recursive 同时列出所有子目录层 -s, –-size 以块大小为单位列出所有文件的大小 -S 根据文件大小排序 –-sort=WORD 以下是可选用的 WORD 和它们代表的相应选项：extension -X status -c none -U time -t size -S atime -u time -t access -u version -v use -u -t 以文件修改时间排序 -u 配合 -lt:显示访问时间而且依访问时间排序;配合 -l:显示访问时间但根据名称排序;否则：根据访问时间排序 -U 不进行排序;依文件系统原有的次序列出项目 -v 根据版本进行排序 -w, –-width=COLS 自行指定屏幕宽度而不使用目前的数值 -x 逐行列出项目而不是逐栏列出 -X 根据扩展名排序 -1 每行只列出一个文件 -–help 显示此帮助信息并离开 -–version 显示版本信息并离开 常用范例列出文件夹下的所有文件和目录的详细资料命令：ls -l -R /home/peidachang 在使用 ls 命令时要注意命令的格式：在命令提示符后，首先是命令的关键字，接下来是命令参数，在命令参数之前要有一短横线 -，所有的命令参数都有特定的作用，自己可以根据需要选用一个或者多个参数，在命令参数的后面是命令的操作对象。在以上这条命令 ls -l -R /home/peidachang 中，ls 是命令关键字，-l -R是参数，/home/peidachang 是命令的操作对象。在这条命令中，使用到了两个参数，分别为 l 和 R，当然，你也可以把他们放在一起使用，如下所示： 命令：ls -lR /home/peidachang 这种形式和上面的命令形式执行的结果是完全一样的。另外，如果命令的操作对象位于当前目录中，可以直接对操作对象进行操作;如果不在当前目录则需要给出操作对象的完整路径，例如上面的例子中，我的当前文件夹是peidachang文件夹，我想对 home 文件夹下的 peidachang 文件进行操作，我可以直接输入 ls -lR peidachang，也可以用 ls -lR /home/peidachang。 列出当前目录中所有以“t”开头的目录的详细内容命令：ls -l t* 可以查看当前目录下文件名以 t 开头的所有文件的信息。其实，在命令格式中，方括号内的内容都是可以省略的，对于命令 ls 而言，如果省略命令参数和操作对象，直接输入ls，则将会列出当前工作目录的内容清单。 只列出文件下的子目录命令：ls -F /opt/soft |grep /$ 列出 /opt/soft 文件下面的子目录 输出： 1234[root@localhost opt]# ls -F /opt/soft |grep /$jdk1.6.0_16/subversion-1.6.1/tomcat6.0.32/ 命令：ls -l /opt/soft | grep ^d 列出 /opt/soft 文件下面的子目录详细情况 输出： 1234[root@localhost opt]# ls -l /opt/soft | grep ^ddrwxr-xr-x 10 root root 4096 09-17 18:17 jdk1.6.0_16drwxr-xr-x 16 1016 1016 4096 10-11 03:25 subversion-1.6.1drwxr-xr-x 9 root root 4096 2011-11-01 tomcat6.0.32 列出目前工作目录下所有名称是s 开头的档案，愈新的排愈后面命令：ls -ltr s* 输出： 123456789101112131415[root@localhost opt]# ls -ltr s*src:总计 0script:总计 0soft:总计 350644drwxr-xr-x 9 root root 4096 2011-11-01 tomcat6.0.32-rwxr-xr-x 1 root root 81871260 09-17 18:15 jdk-6u16-linux-x64.bindrwxr-xr-x 10 root root 4096 09-17 18:17 jdk1.6.0_16-rw-r--r-- 1 root root 205831281 09-17 18:33 apache-tomcat-6.0.32.tar.gz-rw-r--r-- 1 root root 5457684 09-21 00:23 tomcat6.0.32.tar.gz-rw-r--r-- 1 root root 4726179 10-10 11:08 subversion-deps-1.6.1.tar.gz-rw-r--r-- 1 root root 7501026 10-10 11:08 subversion-1.6.1.tar.gzdrwxr-xr-x 16 1016 1016 4096 10-11 03:25 subversion-1.6.1 列出目前工作目录下所有档案及目录;目录于名称后加”/“, 可执行档于名称后加”*“命令：ls -AF 输出： 12[root@localhost opt]# ls -AFlog/ script/ soft/ src/ svndata/ web/ 计算当前目录下的文件数和目录数命令： 123ls -l * |grep &quot;^-&quot;|wc -l ---文件个数 ls -l * |grep &quot;^d&quot;|wc -l ---目录个数 在ls中列出文件的绝对路径命令：ls | sed &quot;s:^:`pwd`/:&quot; 输出： 1234567[root@localhost opt]# ls | sed &quot;s:^:`pwd`/:&quot; /opt/log/opt/script/opt/soft/opt/src/opt/svndata/opt/web 列出当前目录下的所有文件（包括隐藏文件）的绝对路径， 对目录不做递归命令：find $PWD -maxdepth 1 | xargs ls -ld 递归列出当前目录下的所有文件（包括隐藏文件）的绝对路径命令： find $PWD | xargs ls -ld 指定文件时间输出格式命令： ls -tl --time-style=full-iso 输出： 12345[root@localhost soft]# ls -tl --time-style=full-iso 总计 350644drwxr-xr-x 16 1016 1016 4096 2012-10-11 03:25:58.000000000 +0800 subversion-1..1 ls -ctl --time-style=long-iso 输出： 12345[root@localhost soft]# ls -ctl --time-style=long-iso总计 350644drwxr-xr-x 16 1016 1016 4096 2012-10-11 03:25 subversion-1.6.1 扩展显示彩色目录列表​ 打开/etc/bashrc, 加入如下一行: ​ alias ls=”ls –color” ​ 下次启动bash时就可以像在Slackware里那样显示彩色的目录列表了, 其中颜色的含义如下: ​ \1. 蓝色–&gt;目录 ​ \2. 绿色–&gt;可执行文件 ​ \3. 红色–&gt;压缩文件 ​ \4. 浅蓝色–&gt;链接文件 ​ \5. 灰色–&gt;其他文件 原文每天一个linux命令(1)：ls命令]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
        <tag>ls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cd]]></title>
    <url>%2F2019%2F09%2F10%2FLinux%E5%91%BD%E4%BB%A4-cd%2F</url>
    <content type="text"><![CDATA[Linux cd 命令可以说是Linux中最基本的命令语句，其他的命令语句要进行操作，都是建立在使用 cd 命令上的。 所以，学习Linux 常用命令，首先就要学好 cd 命令的使用方法技巧。 命令格式cd [目录名] 命令功能切换当前目录至dirName 常用范例例一：进入系统根目录命令： 1cd / 输出： 1[root@localhost ~]# cd / 说明：进入系统根目录,上面命令执行完后拿ls命令看一下，当前目录已经到系统根目录了 命令： cd .. 或者 cd .. // 输出: 123456[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd ..[root@localhost opt]# cd ..//[root@localhost /]# pwd/ 说明： 进入系统根目录可以使用“ cd .. ”一直退，就可以到达根目录 命令： cd ../.. // 输出： 123456[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd ../.. //[root@localhost /]# pwd/[root@localhost /]# 说明：使用cd 命令实现进入当前目录的父目录的父目录。 例2：使用 cd 命令进入当前用户主目录“当前用户主目录”和“系统根目录”是两个不同的概念。进入当前用户主目录有两个方法。 命令1： cd 输出： 12345[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd[root@localhost ~]# pwd/root 命令2： cd ~ 输出： 123456[root@localhost ~]# cd /opt/soft/[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd ~[root@localhost ~]# pwd/root 例3：跳转到指定目录命令： cd /opt/soft 输出： 1234567[root@localhost ~]# cd /opt/soft[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd jdk1.6.0_16/[root@localhost jdk1.6.0_16]# pwd/opt/soft/jdk1.6.0_16[root@localhost jdk1.6.0_16]# 说明： 跳转到指定目录，从根目录开始，目录名称前加 / ,当前目录内的子目录直接写名称即可 例四：返回进入此目录之前所在的目录命令： cd - 输出： 123456789[root@localhost soft]# pwd/opt/soft[root@localhost soft]# cd -/root[root@localhost ~]# pwd/root[root@localhost ~]# cd -/opt/soft[root@localhost soft]# 例五：把上个命令的参数作为cd参数使用命令： cd !$ 输出： 1234567[root@localhost soft]# cd !$cd -/root[root@localhost ~]# cd !$cd -/opt/soft[root@localhost soft]# 原文原文]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
        <tag>cd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala-将Java的Map转为Scala的Map]]></title>
    <url>%2F2019%2F09%2F09%2FScala-%E5%B0%86Java%E7%9A%84Map%E8%BD%AC%E4%B8%BAScala%E7%9A%84Map%2F</url>
    <content type="text"><![CDATA[将java中的Map转为Scala中可用的 123456def main(args: Array[String]): Unit = &#123; val maps = new util.HashMap[String,Int]() maps.put("aaa", 1) import scala.collection.JavaConverters._ println(maps.asScala)&#125; 输出 1Map(aaa -&gt; 1) value 注意不能用 Object 类型 可以为如下 1val maps = new util.HashMap[Object,Int]() 但是不能这样 1val maps = new util.HashMap[Object,Object]() 会报以下错误 12Error:(17, 21) the result type of an implicit conversion must be more specific than Object maps.put(&quot;aaa&quot;, 1)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-synchronized方法控制执行次数示例]]></title>
    <url>%2F2019%2F09%2F09%2FJava-synchronized%E6%96%B9%E6%B3%95%E6%8E%A7%E5%88%B6%E6%89%A7%E8%A1%8C%E6%AC%A1%E6%95%B0%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[synchronized使用示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.yibo.modules.empi;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * TODO * * @author zhouhongfa@gz-yibo.com * @ClassName Syn * @Version 1.0 * @since 2019/9/9 15:35 */public class SynchronizedTest &#123; private static SynchronizedTest synchronizedTest = new SynchronizedTest(); private Boolean flag = true; private SynchronizedTest() &#123; &#125; public static SynchronizedTest getInstance() &#123; return synchronizedTest; &#125; public void test() &#123; synchronized (flag) &#123; if (flag) &#123; flag = false; ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); singleThreadExecutor.execute(() -&gt; &#123; try &#123; for (int i = 0; i &lt; 3; i++) &#123; final int index = i; System.out.println(index); Thread.sleep(1000); &#125; &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; flag = true; &#125; &#125;); &#125; else &#123; System.out.println("正在执行"); &#125; &#125; &#125; public static void main(String[] args) &#123; getInstance().test(); try &#123; Thread.sleep(1000); getInstance().test(); Thread.sleep(2000); getInstance().test(); Thread.sleep(4000); getInstance().test(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 123456780正在执行12正在执行012]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8-将List转成Map]]></title>
    <url>%2F2019%2F09%2F09%2FJava8-%E5%B0%86List%E8%BD%AC%E6%88%90Map%2F</url>
    <content type="text"><![CDATA[参考博文 几个Java 8的例子展示怎样将一个 对象的集合（List）放入一个Map中，并且展示怎样处理多个重复keys的问题。 12345678910111213141516package com.mkyong.java8 public class Hosting &#123; private int Id; private String name; private long websites; public Hosting(int id, String name, long websites) &#123; Id = id; this.name = name; this.websites = websites; &#125; //getters, setters and toString()&#125; 1. List to Map – Collectors.toMap()创建一个 Hosting 对象集合, 并且用 Collectors.toMap 去将它转换放入一个 Map. 1234567891011121314151617181920212223242526272829303132333435363738package com.mkyong.java8 import java.util.ArrayList;import java.util.List;import java.util.Map;import java.util.stream.Collectors; public class TestListMap &#123; public static void main(String[] args) &#123; List&lt;Hosting&gt; list = new ArrayList&lt;&gt;(); list.add(new Hosting(1, "liquidweb.com", 80000)); list.add(new Hosting(2, "linode.com", 90000)); list.add(new Hosting(3, "digitalocean.com", 120000)); list.add(new Hosting(4, "aws.amazon.com", 200000)); list.add(new Hosting(5, "mkyong.com", 1)); // key = id, value - websites Map&lt;Integer, String&gt; result1 = list.stream().collect( Collectors.toMap(Hosting::getId, Hosting::getName)); System.out.println("Result 1 : " + result1); // key = name, value - websites Map&lt;String, Long&gt; result2 = list.stream().collect( Collectors.toMap(Hosting::getName, Hosting::getWebsites)); System.out.println("Result 2 : " + result2); // Same with result1, just different syntax // key = id, value = name Map&lt;Integer, String&gt; result3 = list.stream().collect( Collectors.toMap(x -&gt; x.getId(), x -&gt; x.getName())); System.out.println("Result 3 : " + result3); &#125;&#125; 输出结果 123Result 1 : &#123;1=liquidweb.com, 2=linode.com, 3=digitalocean.com, 4=aws.amazon.com, 5=mkyong.com&#125;Result 2 : &#123;liquidweb.com=80000, mkyong.com=1, digitalocean.com=120000, aws.amazon.com=200000, linode.com=90000&#125;Result 3 : &#123;1=liquidweb.com, 2=linode.com, 3=digitalocean.com, 4=aws.amazon.com, 5=mkyong.com&#125; 2. List转Map 重复key问题以下代码会抛出异常 12345678910111213141516171819202122232425262728package com.mkyong.java8; import java.util.ArrayList;import java.util.List;import java.util.Map;import java.util.stream.Collectors; public class TestDuplicatedKey &#123; public static void main(String[] args) &#123; List&lt;Hosting&gt; list = new ArrayList&lt;&gt;(); list.add(new Hosting(1, "liquidweb.com", 80000)); list.add(new Hosting(2, "linode.com", 90000)); list.add(new Hosting(3, "digitalocean.com", 120000)); list.add(new Hosting(4, "aws.amazon.com", 200000)); list.add(new Hosting(5, "mkyong.com", 1)); list.add(new Hosting(6, "linode.com", 100000)); // new line // key = name, value - websites , but the key 'linode' is duplicated!? Map&lt;String, Long&gt; result1 = list.stream().collect( Collectors.toMap(Hosting::getName, Hosting::getWebsites)); System.out.println("Result 1 : " + result1); &#125;&#125; 1234Exception in thread &quot;main&quot; java.lang.IllegalStateException: Duplicate key 90000 at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133) at java.util.HashMap.merge(HashMap.java:1245) //... 为了解决上面重复key的问题，通过增加第三个参数解决: 12345Map&lt;String, Long&gt; result1 = list.stream().collect( Collectors.toMap(Hosting::getName, Hosting::getWebsites, (oldValue, newValue) -&gt; oldValue ) ); 输出 1Result 1 : &#123;..., aws.amazon.com=200000, linode.com=90000&#125; (oldValue, newValue) -&gt; oldValue ==&gt; 如果key是重复的，你选择oldKey or newKey? 如果是用新的值 12345Map&lt;String, Long&gt; result1 = list.stream().collect( Collectors.toMap(Hosting::getName, Hosting::getWebsites, (oldValue, newValue) -&gt; newvalue ) ); 输出如下 1Result 1 : &#123;..., aws.amazon.com=200000, linode.com=100000&#125; 3. 排序后转换12345678910111213141516171819202122232425262728293031package com.mkyong.java8; import java.util.*;import java.util.stream.Collectors; public class TestSortCollect &#123; public static void main(String[] args) &#123; List&lt;Hosting&gt; list = new ArrayList&lt;&gt;(); list.add(new Hosting(1, "liquidweb.com", 80000)); list.add(new Hosting(2, "linode.com", 90000)); list.add(new Hosting(3, "digitalocean.com", 120000)); list.add(new Hosting(4, "aws.amazon.com", 200000)); list.add(new Hosting(5, "mkyong.com", 1)); list.add(new Hosting(6, "linode.com", 100000)); //example 1 Map result1 = list.stream() .sorted(Comparator.comparingLong(Hosting::getWebsites).reversed()) .collect( Collectors.toMap( Hosting::getName, Hosting::getWebsites, // key = name, value = websites (oldValue, newValue) -&gt; oldValue, // if same key, take the old key LinkedHashMap::new // returns a LinkedHashMap, keep order )); System.out.println("Result 1 : " + result1); &#125;&#125; 1Result 1 : &#123;aws.amazon.com=200000, digitalocean.com=120000, linode.com=100000, liquidweb.com=80000, mkyong.com=1&#125; P.S 在上面的例子中， stream 在collect 之前已经被排序, 所以 “linode.com=100000”变为 ‘oldValue’. References Java 8 Collectors JavaDoc Java 8 – How to sort a Map Java 8 Lambda : Comparator example]]></content>
      <categories>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java8</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase-Hbase Shell常用命令]]></title>
    <url>%2F2019%2F09%2F07%2FHbase-Hbase-Shell%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[参考 1. 进入hbase shell console$HBASE_HOME/bin/hbase shell 如果有kerberos认证，需要事先使用相应的keytab进行一下认证（使用kinit命令），认证成功之后再使用hbase shell进入可以使用whoami命令可查看当前用户. 1hbase(main)&gt; whoami 2. 表的管理1）查看有哪些表hbase(main)&gt; list 2）创建表语法： create &lt;table&gt;, {NAME =&gt; &lt;family&gt;, VERSIONS =&gt; &lt;VERSIONS&gt;} 例如：创建表t1，有两个family name：f1，f2，且版本数均为2 1hbase(main)&gt; create &apos;t1&apos;,&#123;NAME =&gt; &apos;f1&apos;, VERSIONS =&gt; 2&#125;,&#123;NAME =&gt; &apos;f2&apos;, VERSIONS =&gt; 2&#125; 3）删除表分两步：1. disable，2. drop例如：删除表t1 12hbase(main)&gt; disable &apos;t1&apos;hbase(main)&gt; drop &apos;t1&apos; 4）查看表的结构语法：describe &lt;table&gt; 例如：查看表t1的结构hbase(main)&gt; describe &#39;t1&#39; 5）修改表结构修改表结构必须先disable 语法： alter &#39;t1&#39;, {NAME =&gt; &#39;f1&#39;}, {NAME =&gt; &#39;f2&#39;, METHOD =&gt; &#39;delete&#39;} 例如：修改表test1的cf的TTL为180天 123hbase(main)&gt; disable &apos;test1&apos;hbase(main)&gt; alter &apos;test1&apos;,&#123;NAME=&gt;&apos;body&apos;,TTL=&gt;&apos;15552000&apos;&#125;,&#123;NAME=&gt;&apos;meta&apos;, TTL=&gt;&apos;15552000&apos;&#125;hbase(main)&gt; enable &apos;test1&apos; 3. 权限管理1）分配权限语法: grant &lt;user&gt; &lt;permissions&gt; &lt;table&gt; &lt;column family&gt; &lt;column qualifier&gt;, 参数后面用逗号分隔.权限用五个字母表示： “RWXCA”.**READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’) 例如:给用户‘test’分配对表t1有读写的权限.hbase(main)&gt; grant &#39;test&#39;,&#39;RW&#39;,&#39;t1&#39; 2）查看权限语法: user_permission &lt;table&gt; 例如: 查看表t1的权限列表.hbase(main)&gt; user_permission &#39;t1&#39; 3）收回权限语法: revoke &lt;user&gt; &lt;table&gt; &lt;column family&gt; &lt;column qualifier&gt; 例如: 收回test用户在表t1上的权限hbase(main)&gt; revoke &#39;test&#39;,&#39;t1&#39; 4. 表数据的增删改查1）添加数据语法: put &lt;table&gt;,&lt;rowkey&gt;,&lt;family:column&gt;,&lt;value&gt;,&lt;timestamp&gt; 例如: 给表t1的添加一行记录：rowkey是rowkey001，family** name：f1，column name：col1，value：value01，timestamp：系统默认hbase(main)&gt; put &#39;t1&#39;,&#39;rowkey001&#39;,&#39;f1:col1&#39;,&#39;value01&#39; 2）查询数据a）查询某行记录语法: get &lt;table&gt;,&lt;rowkey&gt;,[&lt;family:column&gt;,....] 例如: 查询表t1，rowkey001中的f1下的col1的值**hbase(main)&gt; get &#39;t1&#39;,&#39;rowkey001&#39;, &#39;f1:col1&#39;orhbase(main)&gt; get &#39;t1&#39;,&#39;rowkey001&#39;, {COLUMN=&gt;&#39;f1:col1&#39;} 查询表t1，rowke002中的f1下的所有列值hbase(main)&gt; get &#39;t1&#39;,&#39;rowkey001&#39; b）扫描表语法: scan &lt;table&gt;, {COLUMNS =&gt; [ &lt;family:column&gt;,.... ], LIMIT =&gt; num}另外，还可以添加STARTROW、TIMERANGE和FITLER等高级功能 例如: 扫描表t1的前5条数据hbase(main)&gt; scan &#39;t1&#39;,{LIMIT=&gt;5} c）查询表中的数据行数语法: count &lt;table&gt;, {INTERVAL =&gt; intervalNum, CACHE =&gt; cacheNum}INTERVAL设置多少行显示一次及对应的rowkey，默认1000；CACHE每次去取的缓存区大小，默认是10，调整该参数可提高查询速度 例如: 查询表t1中的行数，每100条显示一次，缓存区为500**hbase(main)&gt; count &#39;t1&#39;, {INTERVAL =&gt; 100, CACHE =&gt; 500} 3）删除数据a )删除行中的某个列值语法: delete &lt;table&gt;, &lt;rowkey&gt;, &lt;family:column&gt; , &lt;timestamp&gt;,必须指定列名. 例如: 删除表t1，rowkey001中的f1:col1的数据hbase(main)&gt; delete &#39;t1&#39;,&#39;rowkey001&#39;,&#39;f1:col1&#39;注：将删除改行f1:col1列所有版本的数据 b )删除行语法: deleteall &lt;table&gt;, &lt;rowkey&gt;, &lt;family:column&gt; , &lt;timestamp&gt;，可以不指定列名，删除整行数据. 例如: 删除表t1，rowk001的数据hbase(main)&gt; deleteall &#39;t1&#39;,&#39;rowkey001&#39; c）删除表中的所有数据语法: truncate &lt;table&gt; 删除流程: disable table -&gt; drop table -&gt; create table 例如: 删除表t1的所有数据hbase(main)&gt; truncate &#39;t1&#39; 5. Region管理1）移动region语法: move &#39;encodeRegionName&#39;, &#39;ServerName&#39;,encodeRegionName指的regioName后面的编码，ServerName指的是master-status的Region Servers列表 例如：hbase(main)&gt;move &#39;4343995a58be8e5bbc739af1e91cd72d&#39;, &#39;db-41.xxx.xxx.org,60020,1390274516739&#39; 2）开启/关闭region语法: balance_switch true|false 例如：hbase(main)&gt; balance_switch 3）手动split语法: split &#39;regionName&#39;, &#39;splitKey&#39; 4）手动触发major compaction语法：Compact all regions in a table:hbase&gt; major_compact &#39;t1&#39;Compact an entire region:hbase&gt; major_compact &#39;r1&#39;Compact a single column family within a region:hbase&gt; major_compact &#39;r1&#39;, &#39;c1&#39;Compact a single column family within a table:hbase&gt; major_compact &#39;t1&#39;, &#39;c1&#39; 6. 配置管理及节点重启1）修改hdfs配置hdfs配置位置：/etc/hadoop/conf同步hdfs配置 1cat /home/hadoop/slaves|xargs -i -t scp /etc/hadoop/conf/hdfs-site.xml hadoop@&#123;&#125;:/etc/hadoop/conf/hdfs-site.xml 关闭： 1cat /home/hadoop/slaves|xargs -i -t ssh hadoop@&#123;&#125; "sudo /home/hadoop/cdh4/hadoop-2.0.0-cdh4.2.1/sbin/hadoop-daemon.sh --config /etc/hadoop/conf stop datanode"` 启动： 1cat /home/hadoop/slaves|xargs -i -t ssh hadoop@&#123;&#125; "sudo /home/hadoop/cdh4/hadoop-2.0.0-cdh4.2.1/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode" 2）修改hbase配置hbase配置位置：/etc/hbase/conf同步hbase配置 1cat /home/hadoop/hbase/conf/regionservers|xargs -i -t scp /home/hadoop/hbase/conf/hbase-site.xml hadoop@&#123;&#125;:/home/hadoop/hbase/conf/hbase-site.xml graceful重启 12cd ~/hbasebin/graceful_stop.sh --restart --reload --debug inspurXXX.xxx.xxx.org]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>hbase shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven-shade化解决依赖冲突]]></title>
    <url>%2F2019%2F09%2F06%2FMaven-shade%E5%8C%96%E8%A7%A3%E5%86%B3%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81%2F</url>
    <content type="text"><![CDATA[解决依赖中的jar包冲突问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.yibo.bigdata&lt;/groupId&gt; &lt;artifactId&gt;phoenix-client&lt;/artifactId&gt; &lt;version&gt;4.14.1-Hbase-1.2&lt;/version&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;name&gt;thirdparty&lt;/name&gt; &lt;url&gt;http://183.6.50.10:13181/nexus/content/repositories/thirdparty/&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt; &lt;dependencies&gt; &lt;!-- phoenix-core 版本号要和本地安装的phoenix版本号保持一致 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-client&lt;/artifactId&gt; &lt;version&gt;4.14.1-HBase-1.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-HBase-1.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;--&gt; &lt;!-- https://mvnrepository.com/artifact/joda-time/joda-time --&gt; &lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;com.google.common&lt;/pattern&gt; &lt;shadedPattern&gt;my_guava.common&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.google.gson&lt;/pattern&gt; &lt;shadedPattern&gt;my_gson.common&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/maven/**&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>shade</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2-笔记二(Spark Sql 基础)]]></title>
    <url>%2F2019%2F09%2F04%2FSpark2-%E7%AC%94%E8%AE%B0%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[按照官方文档 sql-getting-started，学习Spark2的使用 https://spark.apache.org/docs/latest/sql-getting-started.html 创建 SprinSession创建 SprinSession 12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.some.config.option", "some-value") .getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ 创建 DataFrames以以下json文件示例 123&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; 1234567891011121314151617package org.apache.spark.examples.learnimport org.apache.spark.sql.SparkSessionobject Demo1 &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .appName("spark- demo") .config("spark.master", "local") .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ val df = spark.read.json("src/main/resources/people.json") df.show() &#125;&#125; 报错 1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX commongs-lang3版本问题 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/codehaus/janino/InternalCompilerException 增加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;janino&lt;/artifactId&gt; &lt;version&gt;3.0.8&lt;/version&gt;&lt;/dependency&gt; 额，ok了，输出如下 1234567+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ 详细使用在 Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo. 无类型数据集操作（又名DataFrame操作）如上所述，在Spark 2.0中，DataFrames只是Scala和Java API中Rows的数据集。与“类型转换”相比，这些操作也称为“无类型转换”，带有强类型Scala / Java数据集。 这里我们包括使用数据集进行结构化数据处理的一些基本示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// This import is needed to use the $-notationimport spark.implicits._// Print the schema in a tree formatdf.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the "name" columndf.select("name").show()// +-------+// | name|// +-------+// |Michael|// | Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select($"name", $"age" + 1).show()// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy| 31|// | Justin| 20|// +-------+---------+// Select people older than 21df.filter($"age" &gt; 21).show()// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy("age").count().show()// +----+-----+// | age|count|// +----+-----+// | 19| 1|// |null| 1|// | 30| 1|// +----+-----+ Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo. 以编程方式运行SQL查询SparkSession上的sql函数使应用程序能够以编程方式运行SQL查询并将结果作为DataFrame返回。 123456789101112// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView("people")val sqlDF = spark.sql("SELECT * FROM people")sqlDF.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 全局临时视图Spark SQL中的临时视图是会话范围的，如果创建它的会话终止，它将消失。如果您希望拥有一个在所有会话之间共享的临时视图并保持活动状态，直到Spark应用程序终止，您可以创建一个全局临时视图。全局临时视图与系统保留的数据库 global_temp 绑定，我们必须使用限定名称来引用它，例如 SELECT * FROM global_temp.view1。 12345678910111213141516171819202122// Register the DataFrame as a global temporary viewdf.createGlobalTempView("people")// Global temporary view is tied to a system preserved database `global_temp`spark.sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// Global temporary view is cross-sessionspark.newSession().sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo. 创建数据集Dataset数据集与RDD类似，但是，它们不使用Java序列化或Kryo，而是使用专用的编码器来序列化对象以便通过网络进行处理或传输。编码器和标准序列化都负责将对象转换为字节，编码器是动态生成的代码，它使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而无需将字节反序列化回对象。 1234567891011121314151617181920212223242526case class Person(name: String, age: Long)// Encoders are created for case classesval caseClassDS = Seq(Person("Andy", 32)).toDS()caseClassDS.show()// +----+---+// |name|age|// +----+---+// |Andy| 32|// +----+---+// Encoders for most common types are automatically provided by importing spark.implicits._val primitiveDS = Seq(1, 2, 3).toDS()primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = "examples/src/main/resources/people.json"val peopleDS = spark.read.json(path).as[Person]peopleDS.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 与RDD进行转换Spark SQL支持两种不同的方法将现有RDD转换为数据集。第一种方法使用反射来推断包含特定类型对象的RDD的模式。这种基于反射的方法可以提供更简洁的代码，并且在您编写Spark应用程序时已经了解模式时可以很好地工作。 创建数据集的第二种方法是通过编程接口，允许您构建模式，然后将其应用于现有RDD。虽然此方法更详细，但它允许您在直到运行时才知道列及其类型时构造数据集。 使用反射推断模式(schema)Spark SQL的Scala接口支持自动将包含RDD的案例类转换为DataFrame。case类定义了表的模式。使用反射读取case类的参数名称，并成为列的名称。案例类也可以嵌套或包含复杂类型，如Seqs或Arrays。此RDD可以隐式转换为DataFrame，然后注册为表。表可以在后续SQL语句中使用。 people.txt内容如下 123Michael, 29Andy, 30Justin, 19 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.yibo.examples.sparksqlimport org.apache.spark.sql.SparkSessioncase class Person(name: String, age: Long)object SparkSqlTest &#123; val spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.master", "local") .getOrCreate() val sc = spark.sparkContext def main(args: Array[String]): Unit = &#123; // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ // Create an RDD of Person objects from a text file, convert it to a Dataframe val peopleDF = sc.textFile("src/main/resources/people.txt") .map(_.split(",")) .map(attributes=&gt;Person(attributes(0), attributes(1).trim.toInt)) .toDF() //注册成临时视图 peopleDF.createOrReplaceTempView("people") // SQL statements can be run by using the sql methods provided by Spark val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19") // 结果中的行的列可以通过字段索引访问 teenagersDF.map(teenager =&gt; "Name: " + teenager(0)).show() // +------------+ // | value| // +------------+ // |Name: Justin| // +------------+ // 或者通过字段名 teenagersDF.map(teenager =&gt; "Name: " + teenager.getAs[String]("name")).show() // +------------+ // | value| // +------------+ // |Name: Justin| // +------------+ // 对 Dataset[Map[K,V]]，无预定义的编码器, 需要显式声明 implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]] // Primitive types and case classes can be also defined as // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder() // row.getValuesMap[T] 一次检索多个列到 Map[String, T] teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List("name", "age"))).collect() // Array(Map("name" -&gt; "Justin", "age" -&gt; 19)) &#125;&#125; 以编程方式指定 schema如果无法提前定义案例类（例如，记录结构以字符串形式编码，或者文本数据集将被解析，字段将针对不同用户进行不同的投影），可以通过三个步骤以编程方式创建 DataFrame。 从原始RDD创建行的RDD; 创建由与步骤1中创建的RDD中的行结构匹配的 StructType 表示的模式。 通过SparkSession提供的createDataFrame方法将模式应用于行的RDD。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.yibo.examples.sparksqlimport org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.types._case class Person(name: String, age: Long)object SparkSqlTest &#123; val spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.master", "local") .getOrCreate() val sc = spark.sparkContext def main(args: Array[String]): Unit = &#123; // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ //创建 RDD val peopleRDD = sc.textFile("src/main/resources/people.txt") //用字符串编码 val schemaString = "name age" //根据schema字符串创建schema val fields = schemaString.split(" ") .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) //转换 RDD(people) 成 Rows val rowRDD = peopleRDD .map(_.split(",")) .map(attributes=&gt;Row(attributes(0), attributes(1).trim)) //将schema应用到 RDD val peopleDF = spark.createDataFrame(rowRDD, schema) //使用 DataFrame 创建临时视图 peopleDF.createOrReplaceTempView("people") // SQL can be run over a temporary view created using DataFrames val results = spark.sql("SELECT name FROM people") // The results of SQL queries are DataFrames and support all the normal RDD operations // The columns of a row in the result can be accessed by field index or by field name results.map(attributes =&gt; "Name: " + attributes(0)).show() // +-------------+ // | value| // +-------------+ // |Name: Michael| // | Name: Andy| // | Name: Justin| // +-------------+ &#125;&#125; 聚合内置的DataFrames函数提供常见的聚合，例如count(), countDistinct(), avg(), max(), min(), etc. 虽然这些函数是为DataFrames设计的，但Spark SQL还为Scala和Java中的某些函数提供了类型安全版本，以便使用强类型数据集。此外，用户不限于预定义的聚合函数，并且可以创建自己的聚合函数。 用户定义的无类型聚合函数用户必须扩展UserDefinedAggregateFunction抽象类以实现自定义无类型聚合函数。例如，用户定义的平均值可能如下所示：]]></content>
      <categories>
        <category>Spark2</category>
      </categories>
      <tags>
        <tag>Spark2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2-笔记一]]></title>
    <url>%2F2019%2F09%2F04%2FSpark2-%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[记录Spark2使用的一些笔记 开始上手Spark中所有功能的入口点是SparkSession类。要创建基本的SparkSession，只需使用 SparkSession.builder()： 12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.some.config.option", "some-value") .getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ 一些配置 1234spark.master spark://5.6.7.8:7077spark.executor.memory 4gspark.eventLog.enabled truespark.serializer org.apache.spark.serializer.KryoSerializer 所有配置可在 https://spark.apache.org/docs/latest/configuration.html 找到 一些问题netty版本问题 1Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.defaultNumHeapArena()I 配置以下，然后解决jar包冲突（maven helper），排除掉老版本的 netty 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.18.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 1234&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt;&lt;/dependency&gt; 1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/codehaus/janino/InternalCompilerException 增加依赖，把老版本排除 12345 &lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;janino&lt;/artifactId&gt; &lt;version&gt;3.0.8&lt;/version&gt;&lt;/dependency&gt; RDD转成 dataframe直接转的话 12import sc.implicits._stuRDD.toDF().show() 1Exception in thread &quot;main&quot; scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving object InterfaceAudience 原因未知~ 用craeteDataframe 1sc.createDataFrame(stuRDD,classOf[PatientTest]).show() 1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/codehaus/janino/InternalCompilerException]]></content>
      <categories>
        <category>Spark2</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-Dataframe笔记一]]></title>
    <url>%2F2019%2F09%2F02%2FSpark-Dataframe%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明这里以连接 phoenix 为示例，见 Phoenix Spark Plugin 使用 这里用的Spark版本为 1.6.2 测试表1create table patient_test(id varchar primary key,name varchar, id_number varchar, phone varchar, sex bigint, file_number varchar,empi varchar); 1234567upsert into patient_test values( &apos;1000&apos; ,&apos;王五&apos;,&apos;1234&apos;,&apos;12345&apos;,1,&apos;0001&apos;);upsert into patient_test values( &apos;1001&apos; ,&apos;李四&apos;,&apos;1235&apos;,&apos;12346&apos;,1,&apos;0002&apos;);upsert into patient_test values( &apos;1002&apos; ,&apos;张三&apos;,&apos;1236&apos;,&apos;12347&apos;,1,&apos;0003&apos;);upsert into patient_test values( &apos;1003&apos; ,&apos;赵四&apos;,&apos;1237&apos;,&apos;12348&apos;,1,&apos;0004&apos;);upsert into patient_test values( &apos;1004&apos; ,&apos;武松&apos;,&apos;1238&apos;,&apos;12349&apos;,1,&apos;0005&apos;);upsert into patient_test values( &apos;1005&apos; ,&apos;王五&apos;,&apos;1234&apos;,&apos;12345&apos;,1,&apos;0006&apos;);upsert into patient_test values( &apos;1006&apos; ,&apos;小明&apos;,&apos;1239&apos;,&apos;12350&apos;,1,&apos;0007&apos;); SparkSql使用12345678910val sparkConf = new SparkConf().setMaster("local").setAppName("empi-test")val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)val df = sqlContext.read .format("org.apache.phoenix.spark") .option("table", "patient_test") .option("zkUrl", "cdh01:2181") .load()df.show() 输出结果 123456789+----+----+---------+-----+---+-----------+| ID|NAME|ID_NUMBER|PHONE|SEX|FILE_NUMBER|+----+----+---------+-----+---+-----------+|1000| 王五| 1234|12345| 1| 0001||1001| 李四| 1235|12346| 1| 0002||1002| 张三| 1236|12347| 1| 0003||1003| 赵四| 1237|12348| 1| 0004||1004| 王五| 1238|12345| 1| 0006|+----+----+---------+-----+---+-----------+ $说明1val updatedDf = df.withColumn("EMPI", empi(df("ID"))) 也可以写成 1val updatedDf = df.withColumn(&quot;EMPI&quot;, empi($&quot;ID&quot;)) 不过要用$ ，需要在使用之前引用以下语句 1import sqlContext.implicits._ show1df.show(1) 123456+----+----+---------+-----+---+-----------+| ID|NAME|ID_NUMBER|PHONE|SEX|FILE_NUMBER|+----+----+---------+-----+---+-----------+|1000| 王五| 1234|12345| 1| 0001|+----+----+---------+-----+---+-----------+only showing top 1 row select1df.select("FILE_NUMBER").show() 输出结果 123456789+-----------+|FILE_NUMBER|+-----------+| 0001|| 0002|| 0003|| 0004|| 0006|+-----------+ map1df.map(t =&gt; "NAME: " + t.getAs[String]("NAME")).collect().foreach(println) 12345NAME: 王五NAME: 李四NAME: 张三NAME: 赵四NAME: 王五 1df.map(_.getValuesMap[Any](List("ID", "NAME"))).collect().foreach(println) 12345Map(ID -&gt; 1000, NAME -&gt; 王五)Map(ID -&gt; 1001, NAME -&gt; 李四)Map(ID -&gt; 1002, NAME -&gt; 张三)Map(ID -&gt; 1003, NAME -&gt; 赵四)Map(ID -&gt; 1004, NAME -&gt; 王五) filter排除为空的 ID_NUMBER 用 !== === 1val idNumberNotNullList = df.filter($&quot;EMPI&quot;.isNotNull.and(df(columnName).isNotNull)) groupBy1columnsDf.groupBy(columnNoIdListBuffer:_*).count().show() 12 1234//排除为空的身份证的人，找出所有身份证和ID，并按身份证分组val idIdNumbers = df.filter(df("ID_NUMBER") &gt; 0).map(_.getValuesMap[Any](List("ID", "ID_NUMBER"))).groupBy(_.get("ID_NUMBER")).collect()//按 ID_NUMBER分组idIdNumbers.foreach(println) 12345(Some(1236),CompactBuffer(Map(ID -&gt; 1002, ID_NUMBER -&gt; 1236)))(Some(1238),CompactBuffer(Map(ID -&gt; 1004, ID_NUMBER -&gt; 1238)))(Some(1234),CompactBuffer(Map(ID -&gt; 1000, ID_NUMBER -&gt; 1234), Map(ID -&gt; 1005, ID_NUMBER -&gt; 1234)))(Some(1235),CompactBuffer(Map(ID -&gt; 1001, ID_NUMBER -&gt; 1235)))(Some(1237),CompactBuffer(Map(ID -&gt; 1003, ID_NUMBER -&gt; 1237))) 1234567//排除为空的身份证的人，找出所有身份证和ID，并按身份证分组val idIdNumbers = df.filter(df("ID_NUMBER") &gt; 0).map(_.getValuesMap[Any](List("ID", "ID_NUMBER"))).groupBy(_.get("ID_NUMBER")).collect()//按 ID_NUMBER分组idIdNumbers.foreach(e=&gt;&#123; println(e._1) e._2.foreach(println)&#125;) 1234567891011Some(1236)Map(ID -&gt; 1002, ID_NUMBER -&gt; 1236)Some(1238)Map(ID -&gt; 1004, ID_NUMBER -&gt; 1238)Some(1234)Map(ID -&gt; 1000, ID_NUMBER -&gt; 1234)Map(ID -&gt; 1005, ID_NUMBER -&gt; 1234)Some(1235)Map(ID -&gt; 1001, ID_NUMBER -&gt; 1235)Some(1237)Map(ID -&gt; 1003, ID_NUMBER -&gt; 1237) 合并字段1columnsNotNullDf.select($"ID",concat($"ID_NUMBER",$"SEX")).show() null值替换1df.na.fill("") 修改值说明需要用新的 Dataframe 一些说明要用 $ 要引用 12// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Dataframe</tag>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix-一些笔记]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-%E4%B8%80%E4%BA%9B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[sqlline.py使用连接1./sqlline.py localhost 常用命令显示所有表 !tables 查看表结构 !describe 表名 退出 !quit 查看帮助 !help 建表语句1create table userinfo_c(id varchar primary key,name varchar); 与Hbase关联先到 Hbase建表 1create &apos;patient_test&apos;,&apos;info&apos; 再创建phoenix表 1create table &quot;patient_test&quot;(&quot;id&quot; varchar primary key,&quot;info&quot;.&quot;name&quot; varchar, &quot;info&quot;.&quot;id_number&quot; varchar, &quot;info&quot;.&quot;phone&quot; varchar, &quot;info&quot;.&quot;sex&quot; varchar, &quot;info&quot;.&quot;file_number&quot; varchar, &quot;info&quot;.&quot;empi&quot; varchar); 注意要用 双引号括起来所有的字段和表名，不然关联不了 在 phoenix 上插入一条数据 120: jdbc:phoenix:&gt; upsert into &quot;patient_test&quot; values( &apos;1000&apos; ,&apos;王五&apos;,&apos;1234&apos;,&apos;12345&apos;,&apos;1&apos;,&apos;0001&apos;,&apos;&apos;);1 row affected (0.097 seconds) 到 hbase上查询 12345678hbase(main):009:0&gt; scan &apos;patient_test&apos;ROW COLUMN+CELL 1000 column=info:_0, timestamp=1567564026903, value=x 1000 column=info:file_number, timestamp=1567564026903, value=0001 1000 column=info:id_number, timestamp=1567564026903, value=1234 1000 column=info:name, timestamp=1567564026903, value=\xE7\x8E\x8B\xE4\xBA\x94 1000 column=info:phone, timestamp=1567564026903, value=12345 1000 column=info:sex, timestamp=1567564026903, value=\x80\x00\x00\x00\x00\x00\x00\x01 Hbase 1put &apos;patient_test&apos;, &apos;1000&apos;,&apos;info:sex&apos;,1 bigint 类型字段会存在 hbase 插入的数据 phoneix读取不了，phoenix插入的数据 hbase读取不正常 1230: jdbc:phoenix:&gt; select * from &quot;patient_test&quot;;Error: ERROR 201 (22000): Illegal data. Expected length of at least 8 bytes, but had 6 (state=22000,code=201)java.sql.SQLException: ERROR 201 (22000): Illegal data. Expected length of at least 8 bytes, but had 6 所以最好所有都用 varchar 存 修改表字段12345ALTER TABLE my_schema.my_table ADD d.dept_id char(10) VERSIONS=10ALTER TABLE my_table ADD dept_name char(50), parent_id char(15) null primary keyALTER TABLE my_table DROP COLUMN d.dept_id, parent_id;ALTER VIEW my_view DROP COLUMN new_col;ALTER TABLE my_table SET IMMUTABLE_ROWS=true,DISABLE_WAL=true; 12ALTER TABLE "gzhonghui"."patient_basic_information" ADD "empi" varchar;ALTER TABLE "gzhonghui"."patient_basic_information" DROP "empi";]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala之旅二-统一类型]]></title>
    <url>%2F2019%2F08%2F31%2FScala%E4%B9%8B%E6%97%85%E4%BA%8C-%E7%BB%9F%E4%B8%80%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在Scala中，所有值都有一个类型，包括数值和函数。下图说明了类型层次结构的子集。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix-安装4.8]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-%E5%AE%89%E8%A3%854-8%2F</url>
    <content type="text"><![CDATA[说明在 Hbase1.2 上安装 phoenix4.8.0 安装手册 下载地址http://archive.apache.org/dist/phoenix/apache-phoenix-4.8.0-HBase-1.2/bin/ 安装解压下载的压缩包 将 phoenix-4.8.0-HBase-1.2-server.jar 拷贝到 hbase安装目录的 lib 目录下 重启 hbase 即可 客户端配置修改 PHOENIX_HOME 下的 conf/env.sh 测试进入 phoenix 目录下的 bin目录中 输入以下命令 1sqlline.py localhost 额，在 windows 下安装不了 1234D:\BIGDATA\phoenix\apache-phoenix-4.8.0-HBase-1.2-bin\binλ .\sqlline.py&apos;C:\Program&apos; 不是内部或外部命令，也不是可运行的程序或批处理文件。]]></content>
      <categories>
        <category>Phoenix</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix Spark Plugin 使用]]></title>
    <url>%2F2019%2F08%2F31%2FPhoenix-phoenix-spark%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[版本说明如果要用 phoenix 的 spark 插件，注意有些版本只支持spark 1.x ，scala也要相应地修改~ 版本问题待解决，官方说2.x是没问题的~ 版本问题可升级成 4.14.1-Hbase1.2解决,但是读取有问题，写入没问题，读取可以用jdbc方式 这里我用 的版本如下 组件 版本 Spark spark-1.6.2-bin-hadoop2.6 phoenix apache-phoenix-4.14.0-cdh5.14.2 ，连的远程phoenix scala 2.10.6 参考资料https://phoenix.apache.org/phoenix_spark.html 创建示例表123CREATE TABLE TABLE1 (ID BIGINT NOT NULL PRIMARY KEY, COL1 VARCHAR);UPSERT INTO TABLE1 (ID, COL1) VALUES (1, 'test_row_1');UPSERT INTO TABLE1 (ID, COL1) VALUES (2, 'test_row_2'); 创建应用用 IDEA 创建 Scala 项目 build.sbt 如下 1234567name := &quot;scala-spark-demo&quot;version := &quot;0.1&quot;scalaVersion := &quot;2.10.6&quot;resolvers += &quot;aliyun&quot; at &quot;http://maven.aliyun.com/nexus/content/groups/public/&quot; Project Structure 配置Libraries 配置 读取Phoenix表src/main/resources/log4j.properties 12345678910111213141516171819202122# Set everything to be logged to the consolelog4j.rootCategory=WARN, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger&apos;s log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=WARN# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=WARNlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFOlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERRORlog4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 如果开启了 phoenix.schema.isNamespaceMappingEnabled ，需要增加以下文件 src/main/resources/hbase-site.xml 1234567&lt;?xml version="1.0"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 使用Data Source API加载为DataFrame12345678910111213141516171819202122package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextobject PhoenixTest3 &#123; def main(args: Array[String]): Unit = &#123; val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) val df = sqlContext.load( "org.apache.phoenix.spark", Map("table" -&gt; "TABLE1", "zkUrl" -&gt; "cdh01:2181") ) df .filter(df("COL1") === "test_row_1" &amp;&amp; df("ID") === 1L) .select(df("ID")) .show &#125;&#125; 输出结果 12345+---+| ID|+---+| 1|+---+ 打包提交测试1打包时按照需求是否将 phoenix 依赖打进去，如果服务器有的话就不用了，打包方法 Ctrl+Alt+Shift+S 打开项目配置界面 配置需要打包的lib 注意要选择 Extract into output root 提交任务 1spark-submit --class com.yibo.examples.PhoenixTest3 --master yarn --deploy-mode client --driver-cores 1 --driver-memory 512M --num-executors 2 --executor-cores 2 --executor-memory 512M D:\zhf\Documents\projects\git\scala-spark-demo\out\artifacts\SparkOperateHBase__jar\scala-spark-demo.jar 使用Configuration对象直接加载为DataFrame这个好像不加载 hbase-site.xml 1234567891011121314151617181920212223242526package com.yibo.examplesimport org.apache.hadoop.conf.Configurationimport org.apache.phoenix.spark._import org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextobject PhoenixTest3 &#123; def main(args: Array[String]): Unit = &#123; val configuration = new Configuration() //配置zookeeper configuration.set("hbase.zookeeper.quorum", "cdh01"); //开启了phoenix.schema.isNamespaceMappingEnabled，则需要配置这个 configuration.set("phoenix.schema.isNamespaceMappingEnabled", "true"); // Can set Phoenix-specific settings, requires 'hbase.zookeeper.quorum' val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) // Load the columns 'ID' and 'COL1' from TABLE1 as a DataFrame val df = sqlContext.phoenixTableAsDataFrame( "TABLE1", Array("ID", "COL1"), conf = configuration ) df.show &#125;&#125; 输出结果 123456+---+----------+| ID| COL1|+---+----------+| 1|test_row_1|| 2|test_row_2|+---+----------+ 写入 Phoenix表创建以下 Phoenix 表进行测试 1CREATE TABLE OUTPUT_TEST_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER); 保存RDDspark2也可以用 saveToPhoenix方法是RDD [Product]上的隐式方法，或者是元组的RDD。数据类型必须与Phoenix支持的某种Java类型相对应。 1234567891011121314151617181920212223package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.phoenix.spark._// RDD写入object PhoenixRDDWriteTest &#123; def main(args: Array[String]): Unit = &#123; val sc = new SparkContext("local", "phoenix-test") val dataSet = List((1L, "1", 1), (2L, "2", 2), (3L, "3", 3)) sc .parallelize(dataSet) .saveToPhoenix( "OUTPUT_TEST_TABLE", Seq("ID","COL1","COL2"), zkUrl = Some("cdh01:2181") ) &#125;&#125; 查询 phoenix 表 123456780: jdbc:phoenix:&gt; select * from output_test_table;+-----+-------+-------+| ID | COL1 | COL2 |+-----+-------+-------+| 1 | 1 | 1 || 2 | 2 | 2 || 3 | 3 | 3 |+-----+-------+-------+ 保存DataFramessave是DataFrame上的方法允许传入数据源类型。您可以使用org.apache.phoenix.spark，还必须传入一个表和zkUrl参数来指定要将DataFrame保留到哪个表和服务器。列名称是从DataFrame的架构字段名称派生的，并且必须与Phoenix列名称匹配。 save方法还采用SaveMode选项，仅支持SaveMode.Overwrite。 12CREATE TABLE INPUT_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER);CREATE TABLE OUTPUT_TABLE (id BIGINT NOT NULL PRIMARY KEY, col1 VARCHAR, col2 INTEGER); 12upsert into input_table values(1,&apos;11&apos;,111);upsert into input_table values(2,&apos;22&apos;,222); 123456789101112131415161718192021222324252627package com.yibo.examplesimport org.apache.spark.SparkContextimport org.apache.spark.sql._import org.apache.phoenix.spark._object PhoenixSaveTest &#123; def main(args: Array[String]): Unit = &#123; // Load INPUT_TABLE val sc = new SparkContext("local", "phoenix-test") val sqlContext = new SQLContext(sc) val hbaseConnectionString = "cdh01:2181" val df = sqlContext.load("org.apache.phoenix.spark", Map("table" -&gt; "INPUT_TABLE", "zkUrl" -&gt; hbaseConnectionString)) // Save to OUTPUT_TABLE df.saveToPhoenix(Map("table" -&gt; "OUTPUT_TABLE", "zkUrl" -&gt; hbaseConnectionString)) //或者// df.write// .format("org.apache.phoenix.spark")// .mode("overwrite")// .option("table", "OUTPUT_TABLE")// .option("zkUrl", hbaseConnectionString)// .save() &#125;&#125; 查询 output_test 表 1234560: jdbc:phoenix:&gt; select * from output_table;+-----+-------+-------+| ID | COL1 | COL2 |+-----+-------+-------+| 1 | 11 | 111 |+-----+-------+-------+ 报错信息1Cannot initiate connection as SYSTEM:CATALOG is found but client does not have phoenix.schema.isNamespaceMappingEnabled enabled 在 src/main/resources 增加 hbase-site.xml 1234567&lt;?xml version="1.0"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/sql/DataFrame 2.x版本问题~(但是官网说2.x是可以用的啊，我试过 2.x下 dataset写是可以的 读会报 Dataset找不到 写会报 上面的错) 降成 1.x 12Error:(16, 25) value phoenixTableAsDataFrame is not a member of org.apache.spark.sql.SQLContext val df = sqlContext.phoenixTableAsDataFrame( 增加 import org.apache.phoenix.spark._即可~]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala之旅一-基础知识]]></title>
    <url>%2F2019%2F08%2F30%2FScala%E4%B9%8B%E6%97%85%E4%B8%80-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[官方文档 表达式表达式是可计算的语句。 11 + 1 可以使用 println 来输出表达式结果 1234println(1) // 1println(1 + 1) // 2println("Hello!") // Hello!println("Hello," + " world!") // Hello, world! 值您可以使用 val 关键字命名表达式的结果。 12val x = 1 + 1println(x) // 2 命名结果（例如x）称为值。引用值不会重新计算它。 无法重新分配值。 1x = 3 // 不能通过编译. 值的类型可以被推断出来，但您也可以显式声明类型，如下所示： 1val x: Int = 1 + 1 注意是如何声明类型的， Int 跟在标识符 x 之后，还需要一个 :。 变量变量就像值，除了你可以重新赋值。您可以使用 var 关键字定义变量。 123var x = 1 + 1x = 3 // 编译通过， 因为 "x" 是用 "var" 关键字声明的。println(x * x) // 9 与值一样，您可以根据需要明确说明类型： 1var x: Int = 1 + 1 块 Blocks 您可以通过用 {} 表达式来组合表达式。我们称之为 块。 块中最后一个表达式的结果也是整个块的结果。 1234println(&#123; val x = 1 + 1 x + 1&#125;) // 3 函数函数是带参数的表达式。 您可以定义一个返回给定整数加一的匿名函数（即无名称）： 1(x: Int) =&gt; x + 1 方法方法的外观和行为与函数非常相似，但它们之间存在一些关键差异。 方法使用 def 关键字定义。 def 之后是名称，参数列表，返回类型和正文。 12def add(x: Int, y: Int): Int = x + yprintln(add(1, 2)) // 3 注意返回值类型在参数列表后有 :Int 方法可以采用多个参数列表。 12def addThenMultiply(x: Int, y: Int)(multiplier: Int): Int = (x + y) * multiplierprintln(addThenMultiply(1, 2)(3)) // 9 或者根本没有参数列表。 12def name: String = System.getProperty("user.name")println("Hello, " + name + "!") 还有一些其他差异，但就目前而言，您可以将它们视为与函数类似的东西。 方法也可以有多行表达式。 1234def getSquareString(input: Double): String = &#123; val square = input * input square.toString&#125; 方法体中的最后一个表达式是方法的返回值。 （Scala确实有一个 return 关键字，但它很少使用。） 类您可以使用 class 关键字定义类，后跟其名称和构造函数参数。 1234class Greeter(prefix: String, suffix: String) &#123; def greet(name: String): Unit = println(prefix + name + suffix)&#125; 方法 greet 的返回类型是 Unit，它表示无返回值。它与 Java和 C 中的void 类似地使用。（不同之处在于，因为每个Scala表达式都必须具有某些值，所以实际上存在Unit类型的单例值，写作()。它不携带任何信息。） 您可以使用 new 关键字创建类的实例。 12val greeter = new Greeter("Hello, ", "!")greeter.greet("Scala developer") // Hello, Scala developer! 我们后面将深入介绍类。 Case 类Scala有一种特殊类型的类，称为“case”类。默认情况下，case类是不可变的，并按值进行比较。您可以使用 case class 关键字来定义。 1case class Point(x: Int, y: Int) 您可以在没有 new 关键字的情况下实例化 Case 类。 123val point = Point(1, 2)val anotherPoint = Point(1, 2)val yetAnotherPoint = Point(2, 2) 并且它们按值进行比较。 1234567891011if (point == anotherPoint) &#123; println(point + " and " + anotherPoint + " are the same.")&#125; else &#123; println(point + " and " + anotherPoint + " are different.")&#125; // Point(1,2) and Point(1,2) are the same.if (point == yetAnotherPoint) &#123; println(point + " and " + yetAnotherPoint + " are the same.")&#125; else &#123; println(point + " and " + yetAnotherPoint + " are different.")&#125; // Point(1,2) and Point(2,2) are different. 我们想介绍的 Case 类 还有很多，我们相信你会爱上它们！我们将在稍后深入介绍它们。 对象对象是它们自己定义的单个实例。你可以把它们想象成是它们自己类中的单例。 您可以使用 object 关键字定义对象。 1234567object IdFactory &#123; private var counter = 0 def create(): Int = &#123; counter += 1 counter &#125;&#125; 您可以通过引用其名称来访问该对象。 1234val newId: Int = IdFactory.create()println(newId) // 1val newerId: Int = IdFactory.create()println(newerId) // 2 我们将在以后深入介绍对象。 Traits 特征？ 特征是包含某些字段和方法的类型。可以组合多种特征。 您可以使用 trait 关键字定义特征。 123trait Greeter &#123; def greet(name: String): Unit&#125; 特征也可以有默认实现。 1234trait Greeter &#123; def greet(name: String): Unit = println("Hello, " + name + "!")&#125; 您可以使用 extends 关键字扩展 traits 并使用 override 关键字覆盖实现。 12345678910111213class DefaultGreeter extends Greeterclass CustomizableGreeter(prefix: String, postfix: String) extends Greeter &#123; override def greet(name: String): Unit = &#123; println(prefix + name + postfix) &#125;&#125;val greeter = new DefaultGreeter()greeter.greet("Scala developer") // Hello, Scala developer!val customGreeter = new CustomizableGreeter("How are you, ", "?")customGreeter.greet("Scala developer") // How are you, Scala developer? 在这里，DefaultGreeter 只扩展了一个特征，但它可以扩展多个特征。 我们稍后将深入介绍特征。 Main 方法Main方法是程序的入口点。 Java虚拟机需要将main方法命名为 main，并接收一个字符串数组参数。 使用对象，您可以按如下方式定义main方法： 1234object Main &#123; def main(args: Array[String]): Unit = println("Hello, Scala developer!")&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala-语法风格之命名规范]]></title>
    <url>%2F2019%2F08%2F30%2FScala-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%E4%B9%8B%E5%91%BD%E5%90%8D%E7%BA%A6%E5%AE%9A%2F</url>
    <content type="text"></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala-语法风格之缩进]]></title>
    <url>%2F2019%2F08%2F30%2FScala-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%E4%B9%8B%E7%BC%A9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[参考Scala官网 Scala style guide-Indentation 简述每个缩进级为 两个空格，而不是 Tab 制表符。 例如： 1234567// right!class Foo &#123; def twospaces = &#123; val x = 2 .. &#125;&#125; Scala语言鼓励使用大量嵌套作用域和逻辑块（函数值等）。 换行如果一个表达式过长（超过80个字符），这样看起来不易于理解，那么你可以换行并用两个空格缩进来表示 如下 123val result = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12 + 13 + 14 + 15 + 16 + 17 + 18 + 19 + 20 如果没有这种结尾方法，Scala会在一行的末尾推断出一个分号，它有时会包装，有时甚至不会发出警告而抛弃编译。 大量参数的方法当调用一个接受大量参数（在五个或更多范围内）的方法时，通常需要将方法调用包装到多行上。在这种情况下，将每个参数单独放在一行上，从当前缩进级别缩进两个空格： 12345foo( someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415) 这样，所有参数都会排成一行，如果稍后需要更改方法的名称，则无需重新对齐它们。 应该非常小心地避免超出行长度这种类型的调用。更具体地说，当每个参数必须缩进超过50个空格以实现对齐时，应该避免这种调用。在这种情况下，调用本身应该移动到下一行并缩进两个空格： 12345678910111213// right!val myLongFieldNameWithNoRealPoint = foo( someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415)// wrong!val myLongFieldNameWithNoRealPoint = foo(someVeryLongFieldName, andAnotherVeryLongFieldName, "this is a string", 3.1415)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习-读写Hbase数据]]></title>
    <url>%2F2019%2F08%2F30%2FSpark%E5%AD%A6%E4%B9%A0-%E8%AF%BB%E5%86%99Hbase%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[说明主要记录 Spark中读写Hbase 的操作记录 参考资料Spark入门：读写HBase数据 准备工作创建Hbase表12D:\BIGDATA\hbase-1.2.0\binλ hbase shell 查看所有表 123456hbase(main):001:0&gt; listTABLE0 row(s) in 0.1630 seconds=&gt; [] 创建 student 表 1hbase&gt; create &apos;student&apos;,&apos;info&apos; 可通过 describe 命令查看“ student” 表的基本信息： 1hbase&gt; describe &apos;student&apos; 录入示例数据123456put &apos;student&apos;,&apos;1&apos;,&apos;info:name&apos;,&apos;Xueqian&apos;put &apos;student&apos;,&apos;1&apos;,&apos;info:gender&apos;,&apos;F&apos;put &apos;student&apos;,&apos;1&apos;,&apos;info:age&apos;,&apos;23&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:name&apos;,&apos;Weiliang&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:gender&apos;,&apos;M&apos;put &apos;student&apos;,&apos;2&apos;,&apos;info:age&apos;,&apos;24&apos; 查看录入的数据 1234//如果每次只查看一行，就用下面命令hbase&gt; get &apos;student&apos;,&apos;1&apos;//如果每次查看全部数据，就用下面命令hbase&gt; scan &apos;student&apos; 配置Spark拷贝 hbase 的 jar 包到 D:\BIGDATA\spark\spark-1.6.2-bin-hadoop2.6\lib\下 需要拷贝的jar文件包括：所有hbase开头的jar文件、guava-12.0.1.jar、htrace-core-3.1.0-incubating.jar和protobuf-java-2.5.0.jar windows 上不知咋配~（下面我将 这些 jar作为 Libraries 来引用） 参考 博客 里配才是正确的吧~ 编写读取Hbase的应用程序一Libiaries配置如下引用 hbase 的 jar 如下引用 spark 的jar（这里我全引了~） 记得要 Add to modules 代码123456789101112131415161718192021222324252627282930313233package com.yibo.examplesimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.mapreduce.TableInputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark._object HbaseReader &#123; //读取hbase数据 def main(args: Array[String]) &#123; //序列化 System.setProperty("spark.serializer", "org.apache.spark.serializer.KryoSerializer") val conf = HBaseConfiguration.create() val sc = new SparkContext(new SparkConf().setMaster("local").setAppName("SparkOperateHBase")) //设置查询的表名 conf.set(TableInputFormat.INPUT_TABLE, "student") val stuRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = stuRDD.count() println("Students RDD Count:" + count) stuRDD.cache() //遍历输出 stuRDD.foreach(&#123; case (_, result) =&gt; val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes, "name".getBytes)) val gender = Bytes.toString(result.getValue("info".getBytes, "gender".getBytes)) val age = Bytes.toString(result.getValue("info".getBytes, "age".getBytes)) println("Row key:" + key + " Name:" + name + " Gender:" + gender + " Age:" + age) &#125;) &#125;&#125; 运行程序直接启动 com.yibo.examples.HbaseReader 类 输出结果提交程序一jar包到spark打包 Build -&gt; Build Arxx -&gt; Build 1spark-submit --class com.yibo.examples.HbaseReader --master yarn --deploy-mode client --driver-cores 1 --driver-memory 512M --num-executors 2 --executor-cores 2 --executor-memory 512M D:\zhf\Documents\projects\git\scala-spark-demo\out\artifacts\SparkOperateHBase__jar\scala-spark-demo.jar 输出结果有以下 123Students RDD Count:2Row key:1 Name:Xueqian Gender:F Age:23Row key:2 Name:Weiliang Gender:M Age:24 不打印 Row key 问题，增加 以下语句 1System.setProperty("spark.serializer", "org.apache.spark.serializer.KryoSerializer") 编写写入Hbase的应用在程序中增加 SparkWriteHBase 类 代码如下 12345678910111213141516171819202122232425262728293031323334package com.yibo.examplesimport org.apache.hadoop.hbase.client.&#123;Put, Result&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.mapreduce.Jobimport org.apache.spark._object SparkWriteHBase &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName("SparkWriteHBase").setMaster("local") val sc = new SparkContext(sparkConf) val tablename = "student" sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, tablename) val job = new Job(sc.hadoopConfiguration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) val indataRDD = sc.makeRDD(Array("3,Rongcheng,M,26", "4,Guanhua,M,27")) //构建两行记录 val rdd = indataRDD.map(_.split(',')).map &#123; arr =&gt; &#123; val put = new Put(Bytes.toBytes(arr(0))) //行健的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(arr(1))) //info:name列的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("gender"), Bytes.toBytes(arr(2))) //info:gender列的值 put.add(Bytes.toBytes("info"), Bytes.toBytes("age"), Bytes.toBytes(arr(3).toInt)) //info:age列的值 (new ImmutableBytesWritable, put) &#125; &#125; rdd.saveAsNewAPIHadoopDataset(job.getConfiguration()) &#125;&#125; 查询表数据 1hbase&gt; scan &apos;student&apos; 结果如下 1234567891011121314ROW COLUMN+CELL 1 column=info:age, timestamp=1479640712163, value=23 1 column=info:gender, timestamp=1479640704522, value=F 1 column=info:name, timestamp=1479640696132, value=Xueqian 2 column=info:age, timestamp=1479640752474, value=24 2 column=info:gender, timestamp=1479640745276, value=M 2 column=info:name, timestamp=1479640732763, value=Weiliang 3 column=info:age, timestamp=1479643273142, value=\x00\x00\x00\x1A 3 column=info:gender, timestamp=1479643273142, value=M 3 column=info:name, timestamp=1479643273142, value=Rongcheng 4 column=info:age, timestamp=1479643273142, value=\x00\x00\x00\x1B 4 column=info:gender, timestamp=1479643273142, value=M 4 column=info:name, timestamp=1479643273142, value=Guanhua 4 row(s) in 0.3240 seconds 一些问题提交任务 时报错1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration 原因为没有配置Spark 中引用 hbase 的jar，如上图是将 hbase的依赖打进 jar 中 运行任务时出错1java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge 缺少 metrics-core-2.2.0.jar 日志打印问题在 Libiaries 中将 sbt 的 logch 去掉，仅使用 spark 下的 slf4j ，然后将 SPARK_HOME/conf/log4j.properties复制到项目中的 src/main/resources 下 修改为 WARN ，就不会再打印一大堆日志了~ 12# Set everything to be logged to the consolelog4j.rootCategory=WARN, console]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装Hbase]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85Hbase%2F</url>
    <content type="text"><![CDATA[说明记录在 windows下安装 Hbase 参考Hbase与Hadoop版本对应 Hbase下载地址 在windows上安装hadoop2.6.4 安装 下载 Hbase ，注意版本对应，这里我下载 1.2.0 的 http://archive.apache.org/dist/hbase/1.2.0/hbase-1.2.0-bin.tar.gz 解压到目录下，如 D:\BIGDATA 独立模式配置这里windows仅支持 独立模式~ 修改在 D:\BIGDATA\hbase-1.2.0\conf\下 的 hbase-site.xml 文件 123456789101112131415161718192021&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hbase-1.2.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;D:/BIGDATA/hbase-1.2.0/zoo&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 增加统计支持 --&gt; &lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动Hbase进入 D:\BIGDATA\hbase-1.2.0\bin 目录下，执行 start-hbase.cmd，查看控制台输出，无异常则启动成功 检查进程执行 jps 命令，查看反馈中有个 HMaster 表示成功 123456789λ jps10772 Jps21428 NodeManager29288 ResourceManager6632 HMaster1105218828 DataNode22796 NameNode32236 NailgunRunner 访问web端管理界面http://localhost:16010/master-status 关闭hbase执行 stop-hbase.cmd]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>大数据</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala学习-使用IDE开发]]></title>
    <url>%2F2019%2F08%2F30%2FScala%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8IDE%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/lsshlsw/article/details/39668911 安装插件Ctrl+Alt+S 打开 settings 界面 点击 Plugins 切换到 Marketplace ，搜索 scala 安装下载，很慢~ 搜索 sbt 安装下载]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Scala编写Spark应用记]]></title>
    <url>%2F2019%2F08%2F30%2F%E7%94%A8Scala%E7%BC%96%E5%86%99Spark%E5%BA%94%E7%94%A8%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明学习 Scala 写 Spark 应用 IDEA配置需下载 插件 scala 新建项目新建一个 Scala sbt 项目 这里 Spark 版本为 1.6.0 ，所以用 2.10.x的 Scala 打包配置 点击 Ok 即可 打包]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-compose笔记]]></title>
    <url>%2F2019%2F08%2F30%2Fdocker-compose%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明记录 docker-compose 的一些笔记 一些网址官方介绍 compose安装 安装Linux 环境官方手册 下载1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 对二进制文件应用可执行权限1sudo chmod +x /usr/local/bin/docker-compose 如果在安装后 docker-compose 执行失败，可以创建链接到 /usr/bin 到其他路径 例如： 1sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 安装命令行自动完成bash 下面是 bash的 将完成脚本放在 /etc/bash_completion.d/中。 1sudo curl -L https://raw.githubusercontent.com/docker/compose/1.24.0/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose zsh 下面是 zsh 在 ~/.zshrc 下面增加 12plugins=(... docker docker-compose) 卸载如果是使用 crul 安装 1sudo rm /usr/local/bin/docker-compose 如果是使用 pip 安装 1pip uninstall docker-compose 简单说明Compose是一个用于定义和运行多容器Docker应用程序的工具。使用Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，您可以从配置中创建并启动所有服务。 使用Compose基本上是一个三步过程： 使用 Dockerfile 定义应用程序的环境，以便可以在任何地方进行复制。 在 docker-compose.yml 中定义构成应用程序的服务，以便它们可以在隔离环境中一起运行。 运行 docker-compose up 然后Compose会启动并运行整个应用程序。 docker-compose.yml 看起来像这样： 123456789101112131415version: &apos;3&apos;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redisvolumes: logvolume01: &#123;&#125; 详细配置说明]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>zsh</tag>
        <tag>docker-compose</tag>
        <tag>curl</tag>
        <tag>compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-nginx配置记录]]></title>
    <url>%2F2019%2F08%2F30%2FDocker-nginx%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[说明 Docker 的 nginx 镜像使用笔记 Docker Hub 网址https://hub.docker.com/_/nginx 简单的配置12FROM nginxCOPY static-html-directory /usr/share/nginx/html 自定义 nignx.conf如下配置即可 12FROM nginxCOPY nginx.conf /etc/nginx/nginx.conf]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装及配置Spark]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AESpark%2F</url>
    <content type="text"><![CDATA[说明记录在 windows 系统上 Spark 的环境配置 前置条件需安装 jdk1.8 需先在 windows 上搭建 hadoop ，见 在windows上安装hadoop2.6.4 下载地址 组件 下载链接 Scala https://www.scala-lang.org/download/all.html Spark https://archive.apache.org/dist/spark/ 安装Scala到 Scala下载地址 选择对应的版本下载，这里我下的 2.10.6 版本压缩包(下载界面往下拉可以找到) 版本对应的 scala版本可以在 https://spark.apache.org/docs/1.6.0/ 查到，将 1.6.0改成使用的版本即可 解压到某个目录 环境变量 path 增加 scala 的 bin 目录路径 如 D:\scala-2.10.6\bin 安装Spark 到 Spark下载地址 下载对应版本的压缩包，我下载 1.6.2 的 解压到某个目录 添加bin目录路径到 path 环境变量中 配置环境变量 变量名 SPARK_HOME D:\BIGDATA\spark\spark-1.6.2-bin-hadoop2.6 PATH 增加 %SPARK_HOME%\bin 验证在 cmd 中执行 spark-shell 有以下输出即为成功 12Spark context available as sc.SQL context available as sqlContext. 参考资料https://blog.csdn.net/u011513853/article/details/52865076 spark2.2scala 要下载 2.11.x spark下载地址 一些问题119/08/30 16:53:22 ERROR util.ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\zhf\AppData\Local\Temp\spark-35f55385-549a-47c7-adcc-3bc3daaebaf7 在 spark 的 log4j.properties 中配置 (不显示而已~) 在 %SPARK_HOME%\conf 下，如果没有，则复制模板，改名即可 12log4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在windows上安装hadoop2.6.4]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%85hadoop%2F</url>
    <content type="text"><![CDATA[版本 组件 版本 hadoop 2.6.4 jdk 1.8 下载地址hadoop winutils github JDK安装略 hadoop安装配置下载，解压到 D:\BIGDATA下 配置环境变量 变量名 值 HADOOP_HOME D:\BIGDATA\hadoop-2.6.4 HADOOP_CONF_DIR %HADOOP_HOME%/etc/hadoop PATH 增加 %HADOOP_HOME%\bin 配置 hadoop-env.cmd在 hadoop 目录下的 \etc\hadoop\hadoop-env.cmd 文件 修改为以下，注意路径中有空格要写成如下 Progra~1 set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_191 验证 1234C:\software\cmder_miniλ hadoopUsage: hadoop [--config confdir] COMMANDwhere COMMAND is one of: 配置 core-site.xml路径%HADOOP_HOME%/etc/hadoop/core-site.xml 内容如下 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 hdfs-site.xml 首先要创建 data 目录 在 HADOOP_HOME下创建data 目录，再创建 data/datanode data/namenode两个目录 在 hadoop 目录下的 /etc/hadoop/hdfs-site.xml 修改为以下内容 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hadoop-2.6.4/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///D:/BIGDATA/hadoop-2.6.4/data/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 yarn-site.xml在 hadoop 目录下的 /etc/hadoop/yarn-site.xml下 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 替换bin目录 下载 winutils github 到本地，找到对应的版本 将 HADOOP_HOME 的 bin 目录删除 将 winutils-master 里对应版本的 bin 目录复制到 HADOOP_HOME下 格式化执行以下命令(cmder 不行，要用cmd执行) 1hdfs namenode –format 有以下输出 1219/08/29 12:49:17 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1502620146-192.168.168.1-156705415741719/08/29 12:49:17 INFO common.Storage: Storage directory D:\BIGDATA\hadoop-2.6.4\data\namenode has been successfully formatted. 在 HADOOP_HOME下的 data/namenode 会生成一个目录 hadoop启动与关闭启动 进入 HADOOP_HOME\sbin 目录 双击 start-all.cmd 即可(或在 cmd 中调用) 关闭 进入 HADOOP_HOME\sbin 目录 双击 stop-all.cmd 即可(或在 cmd 中调用) 一些网址 说明 网址 NameNode http://localhost:50070 ResourceManage http://localhost:8088]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>windows</tag>
        <tag>hadoop</tag>
        <tag>yarn</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次pdf提取文本]]></title>
    <url>%2F2019%2F08%2F29%2F%E8%AE%B0%E4%B8%80%E6%AC%A1pdf%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[说明需要从 十几个pdf文件中提取 xml，本来想用 pdfbox 框架来直接转的，可是测试后发现转换出来的是乱码~，于是换个思路…… 思路 首页将需要提取的文件截图（可以写个脚本来做？有空看看）使用 ocr 来转换，我用的是 腾讯AI上的，然后写个程序去调 ocr 接口，将获取的结果写入xml中，然后再用 正则替换 来修正其中的一些格式问题。 优图OCR服务注意在 腾讯AI开放平台注册，然后创建应用，需要接入 通用OCR 能力，才能调api，不然会报 no auth 错误 点击 接入能力 选择你的应用即可 批量调用接口并生成xml项目github 正则替换记录我用的 vscode 软件来替换的 正则 替换文本 说明 ^\(|〈|&lt;\s+ &lt; &lt; 去空格 ( 改为 &lt; (&lt;[a-zA-Z]+)\) $1&gt; &lt;houseNumber) 改成 &lt;houseNumber&gt; 〉|\)$ &gt; 修正 &gt; \b’|’\s?|‘|”|’ “ &quot;修正 (=)\s(&quot;)\s $1$2 Code = &quot; IN 改成 Code = &quot;IN \s?(=)\s? $1 去掉 = 两边的空格 xsi:\s|xsi\s: xsi: xsi:type 修正 执行两次~ &lt;/\s|&lt;\s/ &lt;/ 修正 &lt;/ 去掉多余的空格 \s/&gt;|/\s&gt; /&gt; 修正 /&gt; 去掉多余的空格 ^&lt;!\s-+ &lt;!-- 修正 &lt;!-- (&lt;!-)(?!-) $1- &lt;!- 改成 &lt;!-- (?&lt;!-)(-&gt;)$ -$1 -&gt; 改成 --&gt; (\d)\s(\.) $1$2 2.16 .8改成 2.16.8 (\d\.)\s $1 2.16. 1 改成 2.16.1 \s(&quot;) $1 code=&quot;30954-2 &quot; 改成 code=&quot;30954-2&quot; (=&quot;)\s $1 typeCode=&quot; RCT&quot; 改成 typeCode=&quot;RCT&quot; (&lt;[a-zA-z]+)\) $1&gt; &lt;addr) 改成 &lt;addr&gt; (?&lt;=&quot;)(?=[a-zA-Z]+=) 空格 &quot;2&quot;unit= 改成 &quot;2&quot; unit= (=&quot;)\s $1 root=&quot; 2 改成 root=&quot;2 (?&lt;=&lt;/)([a-zA-Z]+)\s $1 &lt;/entryRelat ionship&gt;改成 &lt;/entryRelationship&gt; 应多次执行 (?&lt;=&lt;/?)([a-zA-Z]+$) $1&gt; &lt;/observation 改成 &lt;/observation&gt; (&lt;[a-zA-Z]+) (?!xsi:type)(?!/&gt;)(?![a-zA-Z:]+=) $1 &lt;entryRelat ionship 改成 &lt;entryRelationship 需要多次调用以防 出现 &lt;entry Relat ionship这种情况 (&lt;[a-zA-Z]+)\.\s* $1 &lt;ass. igned 改成 &lt;assigned (\s[a-zA-Z]+)\s([a-zA-Z]+=) $1$2 extens ion=&quot;PO改成 extension=&quot;PO (\.)\n $1 将以 . 结尾的换行符去掉 \n^(=) $1 将以 =开头的前一个换行符去掉 &quot;$\n &quot;空格 将以&quot; 结尾的换行符替换来空格]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>ocr</tag>
        <tag>pdf转换</tag>
        <tag>regex</tag>
        <tag>cda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[element-ui表格行内编辑使用]]></title>
    <url>%2F2019%2F08%2F25%2Felement-ui%E8%A1%A8%E6%A0%BC%E8%A1%8C%E5%86%85%E7%BC%96%E8%BE%91%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[说明通过使用 vue-element-extends 来拓展 element-ui 中的表格功能 参考vue-element-extends csdn博文 配置1npm install xe-utils vue-element-extends 在 main.js 里增加以下 12345import Vue from 'vue'import VueElementExtends from 'vue-element-extends'import 'vue-element-extends/lib/index.css'Vue.use(VueElementExtends) 官方示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;template&gt; &lt;div&gt; &lt;el-button @click=&quot;$refs.editable.insert()&quot;&gt;新增&lt;/el-button&gt; &lt;el-button @click=&quot;$refs.editable.removeSelecteds()&quot;&gt;删除选中&lt;/el-button&gt; &lt;elx-editable ref=&quot;editable&quot; :data.sync=&quot;tableData&quot;&gt; &lt;elx-editable-column type=&quot;selection&quot; width=&quot;55&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column type=&quot;index&quot; width=&quot;55&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;name&quot; label=&quot;只读&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;sex&quot; label=&quot;下拉&quot; :edit-render=&quot;&#123;name: &apos;ElSelect&apos;, options: sexList&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;num&quot; label=&quot;数值&quot; :edit-render=&quot;&#123;name: &apos;ElInputNumber&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;date&quot; label=&quot;日期&quot; :edit-render=&quot;&#123;name: &apos;ElDatePicker&apos;, props: &#123;type: &apos;date&apos;, format: &apos;yyyy-MM-dd&apos;&#125;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;flag&quot; label=&quot;开关&quot; :edit-render=&quot;&#123;name: &apos;ElSwitch&apos;, type: &apos;visible&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;elx-editable-column prop=&quot;remark&quot; label=&quot;文本&quot; :edit-render=&quot;&#123;name: &apos;ElInput&apos;&#125;&quot;&gt;&lt;/elx-editable-column&gt; &lt;/elx-editable&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data () &#123; return &#123; tableData: [&#123; date: 1551322088449, name: &apos;小徐&apos;, sex: &apos;1&apos;, num: &apos;26&apos;, flag: false, remark: &apos;备注&apos; &#125;], sexList: [ &#123; &apos;label&apos;: &apos;男&apos;, &apos;value&apos;: &apos;1&apos; &#125;, &#123; &apos;label&apos;: &apos;女&apos;, &apos;value&apos;: &apos;0&apos; &#125; ] &#125; &#125;&#125;&lt;/script&gt;]]></content>
      <categories>
        <category>element-ui</category>
      </categories>
      <tags>
        <tag>vue</tag>
        <tag>element-ui</tag>
        <tag>vue-element-extends</tag>
        <tag>elx-editable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用笔记]]></title>
    <url>%2F2019%2F08%2F24%2FHexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[说明记录一下搭建 hexo 博客中的一些笔记 Hexo 安装网上的教程很详细了，这里简单记一下 在 D:\Files 打开 git bash 12npm install -g hexo-clihexo init hexo-blog 12345zhf@DESKTOP-2EL8QVU MINGW64 /d/Files$ cd hexo-blog/zhf@DESKTOP-2EL8QVU MINGW64 /d/Files/hexo-blog$ npm install hexo s 启动 配置 git 12345678git initgit remote add origin https://github.com/zhouhongfa/hexo-blog.gitgit config user.email &apos;zhouhongfa1996@gmail.com&apos;git pull origin mastergit add .git commit -m &apos;init&apos;git push --set-upstream origin master next 主题配置先 fork next主题github仓库 ，(这样就能保存你的配置了，需要进入 next 目录来推，如果有更新，则用 pull request拉更新的内容，可参考 github上fork了别人的项目后，再同步更新别人的提交) 12cd hexo-bloggit clone https://github.com/zhouhongfa/hexo-theme-next.git themes/next 主题wiki github Pages部署到 github 创建一个 zhouhongfa.github.io 公开项目 _config.yml 配置如下 123deploy: type: git repo: https://github.com/zhouhongfa/zhouhongfa.github.io.git 同步到 github 上 12hexo cleanhexo deploy 头像配置主要配置 avatar 属性 123456# 配置头像路径url: /images/avatar.jpg# If true, the avatar would be dispalyed in circle.为true时会将头像显示为圆形的rounded: true# If true, the avatar would be rotated with the cursor.为true时，鼠标放在头像上会旋转~rotated: true 侧边栏配置社交栏邮箱正确配置如下 123social: GitHub: https://github.com/zhouhongfa || github E-Mail: mailto:zhouhongfa1996@gmail.com || envelope 侧边栏位置在 _config.yml 中配置 sidebar 属性 1position: right 默认展开所有目录在 _config.yml 中配置 toc 属性 1expand_all: true 搜索配置参考博客 记录一下 1npm install hexo-generator-searchdb --save 编辑博客配置文件：_config.yml 增加以下 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件：_config.yml 修改以下 123# Local searchlocal_search: enable: true 重新部署 点击爱心效果这里我用的 next 主题比较新，可能会不一样请参考 记录一下 在 themes/next/source/js 目录下创建 clicklove.js 文件 内容如下 1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 修改 themes\next\layout\_layout.swig 文件 在最后的&lt;/body&gt;下加上以下 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/clicklove.js&quot;&gt;&lt;/script&gt; 分类和标签参考博客 记得要在 主题配置文件中解开注释 大概在 158行 menu 属性下 如何设置read more?用以下语法即可 1&lt;!--more--&gt; 图片引用问题开启 _config.yml 下的 1post_asset_folder: true 然后将需要用到的图片放置到 文章同名目录下 引用语法如下 1&#123;% asset_img 1566638486203.png test%&#125; 然后首页和文章内都能正常访问啦 语言问题需要根据主题里的文件名进行配置 如 next 主题里是 zh-CN ，就在 _config.yml 配置为 zh-CN 是配置 hexo 的配置 大概在 11 行 我是如下配置的 123language: - zh-CN- en 评论功能这里用 Gitalk 来实现、 参考 额，注意配置里的是 repo name ，也就是配置仓库名字即可，而不是克隆的地址 看板娘设置参考 1git clone https://github.com/galnetwen/Live2D.git 复制 live2d 目录到 themes/next/source下 修改 themes/next/layout/_layout.swig 文件 在 head 标签增加以下 1&lt;link rel="stylesheet" href="/live2d/css/live2d.css" /&gt; 在 body 标签增加以下 12345&lt;div id="landlord"&gt; &lt;div class="message" style="opacity:0"&gt;&lt;/div&gt; &lt;canvas id="live2d" width="280" height="250" class="live2d"&gt;&lt;/canvas&gt; &lt;div class="hide-button"&gt;隐藏&lt;/div&gt;&lt;/div&gt; 增加以下脚本 12345678910&lt;!-- 看板娘 --&gt;&lt;script type="text/javascript"&gt; var message_Path = '/live2d/' var home_Path = 'https://zhouhongfa.github.io/' //此处修改为你的域名，必须带斜杠&lt;/script&gt;&lt;script type="text/javascript" src="/live2d/js/live2d.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="/live2d/js/message.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; loadlive2d("live2d", "/live2d/model/tia/model.json");&lt;/script&gt; 效果如下 引用其他文章举例 引用 Hello.md 1&#123;% post_link Hello %&#125; hexo 常用命令12345hexo clean：清除缓存文件和已生成的静态文件hexo g：生成静态文件hexo s：启动服务器。默认情况下，访问网址为： http://localhost:4000/hexo d：部署网站hexo v：显示 Hexo 版本 写作官方文档 你可以执行下列命令来创建一篇新文章或者新的页面。 1$ hexo new [layout] &lt;title&gt; 您可以在命令中指定文章的布局（layout），默认为 post，可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局。 Layout Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 草稿刚刚提到了 Hexo 的一种特殊布局：draft，这种布局在建立时会被保存到 source/_drafts 文件夹，您可通过 publish 命令将草稿移动到 source/_posts 文件夹，该命令的使用方式与 new 十分类似，您也可在命令中指定 layout 来指定布局。 1$ hexo publish [layout] &lt;title&gt; 草稿默认不会显示在页面中，您可在执行时加上 --draft 参数，或是把 render_drafts 参数设为 true 来预览草稿。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>next</tag>
        <tag>gitalk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的博客]]></title>
    <url>%2F2019%2F08%2F24%2Fintroduce%2F</url>
    <content type="text"><![CDATA[hexo 搭建 https://zhouhongfa.github.io]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
